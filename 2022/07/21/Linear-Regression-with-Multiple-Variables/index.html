<!DOCTYPE HTML>
<html>
<head>
  <meta charset="utf-8">
  
  <title>Linear Regression with Multiple Variables | Jiahao Peng</title>
  <meta name="author" content="me">
  
  <meta name="description" content="本篇笔记是大一时入门机器学习所写，听的是吴恩达的机器学习入门课程。由于课程是英文课程，所以笔记也是英文的。现在看来当时的写作水平十分生涩。">
  
  
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

  <meta property="og:title" content="Linear Regression with Multiple Variables"/>
  <meta property="og:site_name" content="Jiahao Peng"/>

  
    <meta property="og:image" content=""/>
  

  
    <link rel="alternative" href="/atom.xml" title="Jiahao Peng" type="application/atom+xml">
  
  
    <link href="/favicon.png" rel="icon">
  

  <!-- CSS -->
  <link rel="stylesheet" href="/css/themes/lumen.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/font-awesome.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/style.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/responsive.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/highlight.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/highlight-default.min.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/google-fonts.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/comment.css" media="screen" type="text/css">
  <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
	<script src="https://cdnjs.cloudflare.com/ajax/libs/es5-shim/4.5.9/es5-shim.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/es5-shim/4.5.7/es5-sham.min.js"></script>
  <![endif]-->

  <script src="/js/jquery-2.0.3.min.js"></script>
  
  
  <!-- analytics -->
  



<meta name="generator" content="Hexo 7.3.0"></head>

<body>
  <nav id="main-nav" class="navbar navbar-inverse navbar-default navbar-fixed-top" role="navigation">
    <div class="container">
      <button type="button" class="navbar-header navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
	<span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
       <a class="navbar-brand" href="/">Jiahao Peng</a>
      <div class="collapse navbar-collapse nav-menu">
		<ul class="nav navbar-nav">
		  
		  <li>
			<a href="/archives" title="All the articles.">
			  <i class="fa fa-archive"></i>Archives
			</a>
		  </li>
		  
		  <li>
			<a href="/categories" title="All the categories.">
			  <i class="fa fa-folder"></i>Categories
			</a>
		  </li>
		  
		  <li>
			<a href="/tags" title="All the tags.">
			  <i class="fa fa-tags"></i>Tags
			</a>
		  </li>
		  
		  <li>
			<a href="/about" title="About me.">
			  <i class="fa fa-user"></i>About
			</a>
		  </li>
		  
		</ul>
      </div>
    </div> <!-- container -->
</nav>
<div class="clearfix"></div>

  <div class="container">
    <div class="content">
      


	
		<div class="page-header page-header-inverse ">		
			<h1 class="title title-inverse "> Linear Regression with Multiple Variables</h1>
		</div>		
	






<div class="row post">
	<!-- cols -->
	
	<div id="top_meta"></div>
	<div class="col-md-9">
	

	<!-- content -->
	<div class="mypage">		
	  
		 <div class="alert alert-success description">
			<i class="fa fa-info-circle"></i> <p>本篇笔记是大一时入门机器学习所写，听的是吴恩达的机器学习入门课程。由于课程是英文课程，所以笔记也是英文的。现在看来当时的写作水平十分生涩。</p>
			
		 </div> <!-- alert -->
	  		

	  <h2 id="multiple-features">01 Multiple Features</h2>
<h3 id="introduction">1.1 Introduction</h3>
<p>In the previous sections, we’ve talked about linear regression with
one variate. What if we got more variates?</p>
<p>Multiple variates mean we will have multiple features. In the
previous section, we only have one feature and that is the size of the
house. Now we let’s discuss the multi-feature situation.</p>
<span id="more"></span>
<h3 id="multiple-featuresvariables">1.2 Multiple
Features(variables)</h3>
<p><img
src="/img/Linear-Regression-with-Multiple-Variables-01.png" /></p>
<p>In the above case, we have the data of the house’s bedrooms, floors
and age and hopefully to predict the possible price of the house. All of
this four numbers are called <span
class="math inline"><em>f</em><em>e</em><em>a</em><em>t</em><em>u</em><em>r</em><em>e</em><em>s</em></span>.</p>
<p>For there are four features, the input <span
class="math inline"><em>x</em><sup>(<em>i</em>)</sup></span> will be a
four-dimensional vector.</p>
<p>We use <span class="math inline"><em>n</em></span> to represent the
number of features here, <span
class="math inline"><em>x</em><sup>(<em>i</em>)</sup></span> as the
input of <span
class="math inline"><em>i</em><sup><em>t</em><em>h</em></sup></span>
training example and <span
class="math inline"><em>x</em><sub><em>j</em></sub><sup>(<em>i</em>)</sup></span>
as the value of feature <span class="math inline"><em>j</em></span> in
<span
class="math inline"><em>i</em><sup><em>t</em><em>h</em></sup></span>
training example.</p>
<p>Given that we have multiple features now, the previous hypothesis
function is no longer appropriate. Here is the rewritten one:</p>
<p><span
class="math display"><em>h</em><sub><em>θ</em></sub>(<em>x</em>) = <em>θ</em><sub>0</sub> + <em>θ</em><sub>1</sub><em>x</em><sub>1</sub> + <em>θ</em><sub>2</sub><em>x</em><sub>2</sub> +  ·  ·  ·  + <em>θ</em><sub><em>n</em></sub><em>x</em><sub><em>n</em></sub></span></p>
<p>The above function is clearly too complicated. May we simplify
it?</p>
<h3 id="simplication-of-h_thetax">1.3 Simplication of <span
class="math inline"><em>h</em><sub><em>θ</em></sub>(<em>x</em>)</span></h3>
<p>For convenience of notation, we define <span
class="math inline"><em>x</em><sub>0</sub> = 1</span>, which is the
equivalent to an additional feature. So previously we have <span
class="math inline"><em>n</em></span> features but now we’ve got <span
class="math inline"><em>n</em> + 1</span> whereas we are defining an
additional sort of zero feature vector that always takes on the value of
<span class="math inline">1</span>.</p>
<p>So now we have a (n+1)-dimensional indexed from <span
class="math inline">0</span> to <span
class="math inline"><em>n</em></span>.</p>
<p>And we are also going to think of our parameters as a vector. They
will be like below:</p>
<p><span class="math display">$$
X=\begin{bmatrix}
x_{0}\\
x_{1}\\
x_{2}\\
···\\
x_{n}\\
\end{bmatrix}
\;\;\;\;\;\;\;
\theta=\begin{bmatrix}
\theta_{0}\\
\theta_{1}\\
\theta_{2}\\
···\\
\theta_{n}\\
\end{bmatrix}
$$</span></p>
<p>So the hypothesis function can be rewritten like:</p>
<p><span
class="math display"><em>h</em><sub><em>θ</em></sub>(<em>x</em>) = <em>θ</em><sub>0</sub><em>x</em><sub>0</sub> + <em>θ</em><sub>1</sub><em>x</em><sub>1</sub> + <em>θ</em><sub>2</sub><em>x</em><sub>2</sub> +  ·  ·  ·  + <em>θ</em><sub><em>n</em></sub><em>x</em><sub><em>n</em></sub> (<em>x</em><sub>0</sub>=1)</span></p>
<p>If you are familiar with the vector multiplication, you will know it
is also equal to this:</p>
<p><span
class="math display"><em>h</em><sub><em>θ</em></sub>(<em>x</em>) = <em>θ</em><sup><em>T</em></sup><em>X</em></span></p>
<h2 id="feature-scaling">02 Feature Scaling</h2>
<h3 id="introduction-1">2.1 Introduction</h3>
<p>Imagine you have two features — <span
class="math inline"><em>x</em><sub>1</sub></span> and <span
class="math inline"><em>x</em><sub>2</sub></span>. The difference btween
this two numbers are really really large, for example, <span
class="math inline"><em>x</em><sub>1</sub></span> ranges from 0 to 2000
while <span class="math inline"><em>x</em><sub>2</sub></span> ranges
from 1 to 5. That way the contour of the cost function can take on this
sort of very skewed elliptical shape:</p>
<p><img
src="/img/Linear-Regression-with-Multiple-Variables-02.png" /></p>
<p>Thus your gradients may oscillate back and forth and end up taking a
long time before it can finally find its way to the global minimum:</p>
<p><img
src="/img/Linear-Regression-with-Multiple-Variables-03.png" /></p>
<p>In this setting, a useful thing to do is to scale the features. For
example, the feature <span
class="math inline"><em>x</em><sub>1</sub></span> is devided by 2000 and
feature <span class="math inline"><em>x</em><sub>2</sub></span> is
devided by 5, so that the contour will be much less skewed and your
gradients will find a direct way to the global minimun instead of
following a much more complicated trajectory.</p>
<p>This process is what we called <strong>Feature Scaling</strong>,
i.e. get every feature into approximately a <span
class="math inline"> − 1 ≤ <em>x</em><sub><em>i</em></sub> ≤ <em>i</em></span>
range.</p>
<h3 id="mean-normalization">2.2 Mean Normalization</h3>
<p>Replace <span
class="math inline"><em>x</em><sub><em>i</em></sub></span> with <span
class="math inline"><em>x</em><sub><em>i</em></sub> − <em>μ</em><sub><em>i</em></sub></span>，(<span
class="math inline"><em>μ</em></span> is the average) to make features
have approximately zero mean (Do not apply to <span
class="math inline"><em>x</em><sub>0</sub> = 1</span>)</p>
<p>For example, if you know the average size of a house is equal to
1000, you might use this formula set the feature <span
class="math inline"><em>x</em><sub>1</sub></span> to be size minus the
average value 1000 divided by 2000. Similarily, if every house has 1 to
5 bedrooms and on average a house has 2 bedrooms, you might use this
formula set the feature <span
class="math inline"><em>x</em><sub>2</sub></span> to be size minus the
average value 2 divided by 5.</p>
<p><img
src="/img/Linear-Regression-with-Multiple-Variables-04.png" /></p>
<h2 id="learning-rate">03 Learning Rate</h2>
<h3 id="debugging">3.1 Debugging</h3>
<p>Making sure gradient descent is working correctly.</p>
<h4 id="method-1">3.1.1 Method 1</h4>
<p><img
src="/img/Linear-Regression-with-Multiple-Variables-05.png" /></p>
<p>What we usually do is to plot the cost function <span
class="math inline"><em>J</em>(<em>θ</em>)</span> as gradient descent
runs. Hopefully <span class="math inline"><em>J</em>(<em>θ</em>)</span>
will decrease after every iteration of gradient descent and its plot
will flatten eventually, which means gradient descent has more or less
converged because your cost function isn’t going down much more. So
looking at this figure can help you judge whether or not gradient
descent has converged.</p>
<p><img
src="/img/Linear-Regression-with-Multiple-Variables-06.png" /></p>
<p>If the plot is increasing like above, that means you gradient descent
is not working normally. Sometimes it means you should be using a tinier
learning rate.</p>
<h4 id="method-2">3.1.2 Method 2</h4>
<p>It’s also possible to come up with automatic convergence test, namely
to have an algorithm to try to tell you if gradient descent has
converged.</p>
<p>Example automatic convergence test: Declare convergence if <span
class="math inline"><em>J</em>(<em>θ</em>)</span> decrease by less than
<span class="math inline">10<sup>−3</sup></span> in one iteration.</p>
<p>Here <span class="math inline">10<sup>−3</sup></span> is a threshold.
But this threshold is not fixed and sometimes it’s hard to choose the
appropriate threshold. So method 1 may be more practical.</p>
<h3 id="learning-rate-1">3.2 Learning Rate</h3>
<p>In the situation we’ve meantioned above, we said that if the value of
the cost function keep increasing, it’s often because the learning rate
is too big. Now let me explain why.</p>
<p>If your learning rate is too big, gradient descent may overshoot the
minimum over and over again and deviate from the minimum, so that you
will end up getting the higher value of the cost function.</p>
<p><img
src="/img/Linear-Regression-with-Multiple-Variables-07.png" /></p>
<p>If learning rate is too small, the convergence can be very slow.</p>
<h2 id="normal-equation">04 Normal Equation</h2>
<h3 id="introduction-2">4.1 Introduction</h3>
<p>In the previous sections, we’ve introduced an iterative algorithm
that takes many steps, multiple iterations of gradient descent to
converge to the global minimum.</p>
<p>In contrast, normal equation would give us a method to solve for
<span class="math inline"><em>θ</em></span> analytically. So rather than
run the iterative algorithm, we can instead just solve for the optimal
value for <span class="math inline"><em>θ</em></span> all at one go.</p>
<p>Mathematically, to work out the minimum of a multivariate funtion is
to take its partial deviratives and set them equal to zero, which can be
somewhat involved.</p>
<h3 id="explaination">4.2 Explaination</h3>
<p>Assume we have a dataset:</p>
<p><img
src="/img/Linear-Regression-with-Multiple-Variables-08.png" /></p>
<p>We take all the number <span
class="math inline"><em>x</em><sub>0</sub></span> to <span
class="math inline"><em>x</em><sub>4</sub></span> to construct a <span
class="math inline"><em>m</em> * (<em>n</em>+1)</span> matrix like:</p>
<p><span class="math display">$$
X=\begin{bmatrix}
1 &amp; 2104 &amp; 5 &amp; 1 &amp; 45\\
1 &amp; 1416 &amp; 3 &amp; 2 &amp; 40\\
1 &amp; 1534 &amp; 3 &amp; 2 &amp; 30\\
1 &amp; 852 &amp; 2 &amp; 1 &amp; 36\\
\end{bmatrix}
$$</span></p>
<p>And then do the similar thing to <span
class="math inline"><em>y</em></span> to construct a m-dimensional
vector like:</p>
<p><span class="math display">$$
y=\begin{bmatrix}
460\\
232\\
315\\
178\\
\end{bmatrix}
$$</span></p>
<p>And we can get the values of <span
class="math inline"><em>θ</em></span> by the formula:</p>
<p><span
class="math display"><em>θ</em> = (<em>X</em><sup><em>T</em></sup><em>X</em>)<sup>−1</sup><em>X</em><sup><em>T</em></sup><em>y</em></span></p>
<p>We use <span class="math inline"><em>x</em><sup>(1)</sup></span> to
represent the <span
class="math inline"><em>i</em><sup><em>t</em><em>h</em></sup></span>
features and <span
class="math inline"><em>y</em><sup>(<em>i</em>)</sup></span> to
represent the <span
class="math inline"><em>i</em><sup><em>t</em><em>h</em></sup></span>
price; then <span
class="math inline">(<em>x</em><sup>(<em>i</em>)</sup>,<em>y</em><sup>(<em>i</em>)</sup>)</span>
forms an example. <span
class="math inline"><em>x</em><sup>(<em>i</em>)</sup></span> will be
like:</p>
<p><span class="math display">$$
x^{(i)}=\begin{bmatrix}
x^{(i)}_0\\
x^{(i)}_1\\
x^{(i)}_2\\
...\\
x^{(i)}_i\\
\end{bmatrix}
$$</span></p>
<p>Here we are going to construct a matrix X called <span
class="math inline"><em>D</em><em>e</em><em>s</em><em>i</em><em>g</em><em>n</em> <em>M</em><em>a</em><em>t</em><em>r</em><em>i</em><em>x</em></span>:</p>
<p><span class="math display">$$
X_{m*(n+1)}=\begin{bmatrix}
(x^{(1)})^T\\
(x^{(2)})^T\\
(x^{(3)})^T\\
...\\
(x^{(m)})^T\\
\end{bmatrix}
$$</span></p>
<p>For instance, if <span
class="math inline"><em>x</em><sup>(<em>i</em>)</sup></span> is:</p>
<p><span class="math display">$$
x^{(i)}=\begin{bmatrix}
1\\
x^{(i)}_1\\
\end{bmatrix}
$$</span></p>
<p>Then the design matrix will be like:</p>
<p><span class="math display">$$
X=\begin{bmatrix}
(x^{(1)})^T\\
(x^{(2)})^T\\
(x^{(3)})^T\\
...\\
(x^{(m)})^T\\
\end{bmatrix}
=\begin{bmatrix}
1 &amp; x^{(1)}_1\\
1 &amp; x^{(2)}_1\\
1 &amp; x^{(3)}_1\\
... &amp; ...\\
1 &amp; x^{(m)}_1\\
\end{bmatrix}
$$</span></p>
<p>The by the formula above:</p>
<p><span
class="math display"><em>θ</em> = (<em>X</em><sup><em>T</em></sup><em>X</em>)<sup>−1</sup><em>X</em><sup><em>T</em></sup><em>y</em></span></p>
<p>We can obtain all the values of <span
class="math inline"><em>θ</em></span>.</p>
<h3 id="gradient-descent-v.s.-normal-equation">4.3 Gradient Descent v.s.
Normal Equation</h3>
<ul>
<li>Gradient Descent:
<ul>
<li>Needs to choose α</li>
<li>Needs many iterations</li>
<li>Works well even when <span class="math inline"><em>n</em></span> is
large</li>
</ul></li>
<li>Normal Equation
<ul>
<li>No need to choose α</li>
<li>Don’t need to iterate</li>
<li>Need to compute <span
class="math inline">(<em>X</em><sup><em>T</em></sup><em>X</em>)<sup>−1</sup></span></li>
<li>Slow if <span class="math inline"><em>n</em></span> is very
large</li>
</ul></li>
</ul>
	  
	</div>

	<!-- recommended posts -->
	

	<div>
  	<center>
	<div class="pagination">
<ul class="pagination">
	 
				
		<li class="prev"><a href="/2022/08/13/Logistic-Regression/" class="alignleft prev"><i class="fa fa-arrow-circle-o-left"></i>Prev</a></li>
  		

        <li><a href="/archives"><i class="fa fa-archive"></i>Archive</a></li>

		
          <li class="next"><a href="/2022/07/21/Introduction-to-Machine-Learning/" class="alignright next">Next<i class="fa fa-arrow-circle-o-right"></i></a></li>
        
	
</ul>
</div>

    </center>
	</div>

    <!-- share -->
    
        
    <div class="bdsharebuttonbox">
        <a href="#" class="bds_more" data-cmd="more"></a>
        <a href="#" class="bds_weixin" data-cmd="weixin" title="分享到微信"></a>
        <a href="#" class="bds_tsina" data-cmd="tsina" title="分享到新浪微博"></a>
        <a href="#" class="bds_fbook" data-cmd="fbook" title="分享到Facebook"></a>
        <a href="#" class="bds_twi" data-cmd="twi" title="分享到Twitter"></a>
        <a href="#" class="bds_linkedin" data-cmd="linkedin" title="分享到linkedin"></a>
        <a href="#" class="bds_evernotecn" data-cmd="evernotecn" title="分享到印象笔记"></a>
        <a href="#" class="bds_youdao" data-cmd="youdao" title="分享到有道云笔记"></a>
        <a href="#" class="bds_copy" data-cmd="copy" title="分享到复制网址"></a>
    </div>
    <script>
        window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"2","bdMiniList":false,"bdPic":"","bdStyle":"1","bdSize":"24"},"share":{}};
        with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];
    </script>


        

    
	
	<!-- comment -->
	
<section id="comment">
  <h2 class="title">Comments</h2>
  
</section>


	</div> <!-- col-md-9/col-md-12 -->
		
	
	<div id="side_meta">
		<div class="col-md-3" id="post_meta"> 

	<!-- date -->
	
	<div class="meta-widget">
	<i class="fa fa-clock-o"></i>
	2022-07-21 
	</div>
	

	<!-- categories -->
    
	<div class="meta-widget">
	<a data-toggle="collapse" data-target="#categorys"><i class="fa fa-folder"></i></a>	
    <ul id="categorys" class="tag_box list-unstyled collapse in">
          
  <li>
    <li><a href="/categories/Machine-Learning-Wu/">Machine Learning - Wu<span>3</span></a></li>
  </li>

    </ul>
	</div>
	

	<!-- tags -->
	
	<div class="meta-widget">
	<a data-toggle="collapse" data-target="#tags"><i class="fa fa-tags"></i></a>		  
    <ul id="tags" class="tag_box list-unstyled collapse in">	  
	    
  <li><a href="/tags/lecture-notes/">lecture notes<span>6</span></a></li>

    </ul>
	</div>
	

	<!-- toc -->
	<div class="meta-widget">
	
	   <a data-toggle="collapse" data-target="#toc"><i class="fa fa-bars"></i></a>
	   <div id="toc" class="toc collapse in">
			<ol class="toc-article"><li class="toc-article-item toc-article-level-2"><a class="toc-article-link" href="#multiple-features"><span class="toc-article-text">01 Multiple Features</span></a><ol class="toc-article-child"><li class="toc-article-item toc-article-level-3"><a class="toc-article-link" href="#introduction"><span class="toc-article-text">1.1 Introduction</span></a></li><li class="toc-article-item toc-article-level-3"><a class="toc-article-link" href="#multiple-featuresvariables"><span class="toc-article-text">1.2 Multiple
Features(variables)</span></a></li><li class="toc-article-item toc-article-level-3"><a class="toc-article-link" href="#simplication-of-h_thetax"><span class="toc-article-text">1.3 Simplication of hθ(x)</span></a></li></ol></li><li class="toc-article-item toc-article-level-2"><a class="toc-article-link" href="#feature-scaling"><span class="toc-article-text">02 Feature Scaling</span></a><ol class="toc-article-child"><li class="toc-article-item toc-article-level-3"><a class="toc-article-link" href="#introduction-1"><span class="toc-article-text">2.1 Introduction</span></a></li><li class="toc-article-item toc-article-level-3"><a class="toc-article-link" href="#mean-normalization"><span class="toc-article-text">2.2 Mean Normalization</span></a></li></ol></li><li class="toc-article-item toc-article-level-2"><a class="toc-article-link" href="#learning-rate"><span class="toc-article-text">03 Learning Rate</span></a><ol class="toc-article-child"><li class="toc-article-item toc-article-level-3"><a class="toc-article-link" href="#debugging"><span class="toc-article-text">3.1 Debugging</span></a><ol class="toc-article-child"><li class="toc-article-item toc-article-level-4"><a class="toc-article-link" href="#method-1"><span class="toc-article-text">3.1.1 Method 1</span></a></li><li class="toc-article-item toc-article-level-4"><a class="toc-article-link" href="#method-2"><span class="toc-article-text">3.1.2 Method 2</span></a></li></ol></li><li class="toc-article-item toc-article-level-3"><a class="toc-article-link" href="#learning-rate-1"><span class="toc-article-text">3.2 Learning Rate</span></a></li></ol></li><li class="toc-article-item toc-article-level-2"><a class="toc-article-link" href="#normal-equation"><span class="toc-article-text">04 Normal Equation</span></a><ol class="toc-article-child"><li class="toc-article-item toc-article-level-3"><a class="toc-article-link" href="#introduction-2"><span class="toc-article-text">4.1 Introduction</span></a></li><li class="toc-article-item toc-article-level-3"><a class="toc-article-link" href="#explaination"><span class="toc-article-text">4.2 Explaination</span></a></li><li class="toc-article-item toc-article-level-3"><a class="toc-article-link" href="#gradient-descent-v.s.-normal-equation"><span class="toc-article-text">4.3 Gradient Descent v.s.
Normal Equation</span></a></li></ol></li></ol>
		</div>
	
	</div>
	
    <hr>
	
</div><!-- col-md-3 -->

	</div>
		

</div><!-- row -->




    </div>
  </div>
  <div class="container-narrow">
    <footer> <!--<p>
  &copy; 2025 me
  
      with help from <a href="http://hexo.io/" target="_blank">Hexo</a> and <a href="http://getbootstrap.com/" target="_blank">Twitter Bootstrap</a>. Theme by <a target="_blank" rel="noopener" href="http://github.com/wzpan/hexo-theme-freemind/">Freemind</a>.    
</p>-->
 </footer>
  </div> <!-- container-narrow -->
  


  
<a id="gotop" href="#">   
  <span>▲</span> 
</a>

<script src="/js/jquery.imagesloaded.min.js"></script>
<script src="/js/gallery.js"></script>
<script src="/js/bootstrap.min.js"></script>
<script src="/js/main.js"></script>
<script src="/js/search.js"></script>


<link rel="stylesheet" href="/fancybox/jquery.fancybox.css" media="screen" type="text/css">
<script src="/fancybox/jquery.fancybox.pack.js"></script>
<script type="text/javascript">
(function($){
  $('.fancybox').fancybox();
})(jQuery);
</script>



   <script type="text/javascript">      
     var search_path = "search.xml";
	 if (search_path.length == 0) {
	 	search_path = "search.xml";
	 }
	 var path = "/" + search_path;
     searchFunc(path, 'local-search-input', 'local-search-result');
   </script>


<!-- syntax highlighting -->


<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</body>
	<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</html>