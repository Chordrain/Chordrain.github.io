<!DOCTYPE html>
<html lang=en>
<head>
    <meta charset="utf-8">
    
    <title>机器学习入门 吴恩达 | Jiahao Peng</title>
    
    
        <meta name="keywords" content="machine learning" />
    
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1" />
    <meta name="description" content="本篇笔记是大一时入门机器学习所写，听的是吴恩达的机器学习入门课程。由于课程是英文课程，所以笔记也是英文的，现在看来当时的写作水平十分生涩。由于当时的目标是入门机器学习，从神经网络开始之后的内容就没继续写了。">
<meta property="og:type" content="article">
<meta property="og:title" content="机器学习入门 吴恩达">
<meta property="og:url" content="https://example.com/2022/07/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8-%E5%90%B4%E6%81%A9%E8%BE%BE/index.html">
<meta property="og:site_name" content="Jiahao Peng">
<meta property="og:description" content="本篇笔记是大一时入门机器学习所写，听的是吴恩达的机器学习入门课程。由于课程是英文课程，所以笔记也是英文的，现在看来当时的写作水平十分生涩。由于当时的目标是入门机器学习，从神经网络开始之后的内容就没继续写了。">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://www.runoob.com/wp-content/uploads/2024/12/how-does-machine-learning-work.png">
<meta property="article:published_time" content="2022-07-21T08:26:16.000Z">
<meta property="article:modified_time" content="2025-04-12T04:50:57.875Z">
<meta property="article:author" content="me">
<meta property="article:tag" content="machine learning">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://www.runoob.com/wp-content/uploads/2024/12/how-does-machine-learning-work.png">
    

    

    
        <link rel="icon" href="/favicon.ico" />
    

    
<link rel="stylesheet" href="/libs/font-awesome/css/font-awesome.min.css">

    
<link rel="stylesheet" href="/libs/open-sans/styles.css">

    
<link rel="stylesheet" href="/libs/source-code-pro/styles.css">


    
<link rel="stylesheet" href="/css/style.css">

    
<script src="/libs/jquery/2.1.3/jquery.min.js"></script>

    
<script src="/libs/jquery/plugins/cookie/1.4.1/jquery.cookie.js"></script>

    
    
        
<link rel="stylesheet" href="/libs/lightgallery/css/lightgallery.min.css">

    
    
        
<link rel="stylesheet" href="/libs/justified-gallery/justifiedGallery.min.css">

    
    
    
    


    
<meta name="generator" content="Hexo 7.3.0"></head>

<body>
    <div id="container">
        <header id="header">
    <div id="header-main" class="header-inner">
        <div class="outer">
            <a href="/" id="logo">
                
                <span class="site-title">Jiahao Peng</span>
            </a>
            <nav id="main-nav">
                
                    <a class="main-nav-link" href="/">首页</a>
                
                    <a class="main-nav-link" href="/archives">归档</a>
                
                    <a class="main-nav-link" href="/categories">分类</a>
                
                    <a class="main-nav-link" href="/tags">标签</a>
                
            </nav>
            
                
                <nav id="sub-nav">
                    <div class="profile" id="profile-nav">
                        <a id="profile-anchor" href="javascript:;">
                            <img class="avatar" src="/images/logo.jpg" />
                            <i class="fa fa-caret-down"></i>
                        </a>
                    </div>
                </nav>
            
            <div id="search-form-wrap">

    <form class="search-form">
        <input type="text" class="ins-search-input search-form-input" placeholder="Search" />
        <button type="submit" class="search-form-submit"></button>
    </form>
    <div class="ins-search">
    <div class="ins-search-mask"></div>
    <div class="ins-search-container">
        <div class="ins-input-wrapper">
            <input type="text" class="ins-search-input" placeholder="Type something..." />
            <span class="ins-close ins-selectable"><i class="fa fa-times-circle"></i></span>
        </div>
        <div class="ins-section-wrapper">
            <div class="ins-section-container"></div>
        </div>
    </div>
</div>
<script>
(function (window) {
    var INSIGHT_CONFIG = {
        TRANSLATION: {
            POSTS: 'Posts',
            PAGES: 'Pages',
            CATEGORIES: 'Categories',
            TAGS: 'Tags',
            UNTITLED: '(Untitled)',
        },
        ROOT_URL: '/',
        CONTENT_URL: '/content.json',
    };
    window.INSIGHT_CONFIG = INSIGHT_CONFIG;
})(window);
</script>

<script src="/js/insight.js"></script>


</div>
        </div>
    </div>
    <div id="main-nav-mobile" class="header-sub header-inner">
        <table class="menu outer">
            <tr>
                
                    <td><a class="main-nav-link" href="/">首页</a></td>
                
                    <td><a class="main-nav-link" href="/archives">归档</a></td>
                
                    <td><a class="main-nav-link" href="/categories">分类</a></td>
                
                    <td><a class="main-nav-link" href="/tags">标签</a></td>
                
                <td>
                    
    <div class="search-form">
        <input type="text" class="ins-search-input search-form-input" placeholder="Search" />
    </div>

                </td>
            </tr>
        </table>
    </div>
</header>

        <div class="outer">
            
                

<aside id="profile">
    <div class="inner profile-inner">
        <div class="base-info profile-block">
            <img id="avatar" src="/images/logo.jpg" />
            <h2 id="name">Jiahao Peng</h2>
            <h3 id="title">Programmer with hair</h3>
            <span id="location"><i class="fa fa-map-marker"></i>Zhejiang, China</span>
            <a id="follow" target="_blank" href="https://github.com/Chordrain/Chordrain.github.io">FOLLOW</a>
        </div>
        <div class="article-info profile-block">
            <div class="article-info-block">
                13
                <span>posts</span>
            </div>
            <div class="article-info-block">
                22
                <span>tags</span>
            </div>
        </div>
        
    </div>
</aside>

            
            
                <aside id="sidebar">
   
        
    <div class="widget-wrap" id='categories'>
        <h3 class="widget-title">
            <span>categories</span>
            &nbsp;
            <a id='allExpand' href="#">
                <i class="fa fa-angle-double-down fa-2x"></i>
            </a>
        </h3>
        
        
        
         <ul class="unstyled" id="tree" > 
                    <li class="directory">
                        <a href="#" data-role="directory">
                            <i class="fa fa-folder"></i>
                            &nbsp;
                            Debugging
                        </a>
                         <ul class="unstyled" id="tree" >  <li class="file"><a href="/2025/04/05/A-WSL-Error-Encountered-When-installing-Docker/">A WSL Error Encountered When Installing Docker</a></li>  </ul> 
                    </li> 
                    
                    <li class="directory">
                        <a href="#" data-role="directory">
                            <i class="fa fa-folder"></i>
                            &nbsp;
                            【Lecture】Software Analysis Testing and Verification
                        </a>
                         <ul class="unstyled" id="tree" >  <li class="file"><a href="/2025/04/04/Introduction-to-Program-Analysis/">Introduction to Program Analysis</a></li>  </ul> 
                    </li> 
                    
                    <li class="directory">
                        <a href="#" data-role="directory">
                            <i class="fa fa-folder"></i>
                            &nbsp;
                            人生总结
                        </a>
                         <ul class="unstyled" id="tree" >  <li class="file"><a href="/2025/03/23/ECNU%E8%BD%AF%E5%AD%A6%E5%A4%8D%E8%AF%95%E6%80%BB%E7%BB%93/">ECNU软学复试总结</a></li>  </ul> 
                    </li> 
                    
                    <li class="directory">
                        <a href="#" data-role="directory">
                            <i class="fa fa-folder"></i>
                            &nbsp;
                            技术开发
                        </a>
                         <ul class="unstyled" id="tree" > 
                    <li class="directory">
                        <a href="#" data-role="directory">
                            <i class="fa fa-folder"></i>
                            &nbsp;
                            前端
                        </a>
                         <ul class="unstyled" id="tree" >  <li class="file"><a href="/2024/02/17/vue3/">Vue3入门</a></li>  </ul> 
                    </li> 
                    
                    <li class="directory">
                        <a href="#" data-role="directory">
                            <i class="fa fa-folder"></i>
                            &nbsp;
                            后端
                        </a>
                         <ul class="unstyled" id="tree" >  <li class="file"><a href="/2023/03/14/Flask/">Flask入门</a></li>  </ul> 
                    </li> 
                     </ul> 
                    </li> 
                    
                    <li class="directory">
                        <a href="#" data-role="directory">
                            <i class="fa fa-folder"></i>
                            &nbsp;
                            深度学习
                        </a>
                         <ul class="unstyled" id="tree" >  <li class="file"><a href="/2023/09/09/%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6-Self-attention/">自注意力机制 Self-attention</a></li>  <li class="file"><a href="/2023/10/24/Transformer/">Transformer</a></li>  <li class="file"><a href="/2024/04/03/%E5%9B%BE%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/">图注意力机制的原理</a></li>  </ul> 
                    </li> 
                    
                    <li class="directory open">
                        <a href="#" data-role="directory">
                            <i class="fa fa-folder-open"></i>
                            &nbsp;
                            计算机专业基础
                        </a>
                         <ul class="unstyled" id="tree" >  <li class="file active"><a href="/2022/07/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8-%E5%90%B4%E6%81%A9%E8%BE%BE/">机器学习入门 吴恩达</a></li>  <li class="file"><a href="/2023/12/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9B%BE%E5%BD%A2%E5%AD%A6/">计算机图形学</a></li>  <li class="file"><a href="/2023/12/11/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E6%8A%80%E6%9C%AF/">人工智能技术杂谈</a></li>  <li class="file"><a href="/2025/03/18/%E7%A6%BB%E6%95%A3%E6%95%B0%E5%AD%A6/">离散数学</a></li>  </ul> 
                    </li> 
                     <li class="file"><a href="/2021/04/05/Welcome/">✨Welcome!✨</a></li>  </ul> 
    </div>
    <script>
        $(document).ready(function() {
            var iconFolderOpenClass  = 'fa-folder-open';
            var iconFolderCloseClass = 'fa-folder';
            var iconAllExpandClass = 'fa-angle-double-down';
            var iconAllPackClass = 'fa-angle-double-up';
            // Handle directory-tree expansion:
            // 左键单独展开目录
            $(document).on('click', '#categories a[data-role="directory"]', function (event) {
                event.preventDefault();

                var icon = $(this).children('.fa');
                var expanded = icon.hasClass(iconFolderOpenClass);
                var subtree = $(this).siblings('ul');
                icon.removeClass(iconFolderOpenClass).removeClass(iconFolderCloseClass);
                if (expanded) {
                    if (typeof subtree != 'undefined') {
                        subtree.slideUp({ duration: 100 });
                    }
                    icon.addClass(iconFolderCloseClass);
                } else {
                    if (typeof subtree != 'undefined') {
                        subtree.slideDown({ duration: 100 });
                    }
                    icon.addClass(iconFolderOpenClass);
                }
            });
            // 右键展开下属所有目录
            $('#categories a[data-role="directory"]').bind("contextmenu", function(event){
                event.preventDefault();
                
                var icon = $(this).children('.fa');
                var expanded = icon.hasClass(iconFolderOpenClass);
                var listNode = $(this).siblings('ul');
                var subtrees = $.merge(listNode.find('li ul'), listNode);
                var icons = $.merge(listNode.find('.fa'), icon);
                icons.removeClass(iconFolderOpenClass).removeClass(iconFolderCloseClass);
                if(expanded) {
                    subtrees.slideUp({ duration: 100 });
                    icons.addClass(iconFolderCloseClass);
                } else {
                    subtrees.slideDown({ duration: 100 });
                    icons.addClass(iconFolderOpenClass);
                }
            })
            // 展开关闭所有目录按钮
            $(document).on('click', '#allExpand', function (event) {
                event.preventDefault();
                
                var icon = $(this).children('.fa');
                var expanded = icon.hasClass(iconAllExpandClass);
                icon.removeClass(iconAllExpandClass).removeClass(iconAllPackClass);
                if(expanded) {
                    $('#sidebar .fa.fa-folder').removeClass('fa-folder').addClass('fa-folder-open')
                    $('#categories li ul').slideDown({ duration: 100 });
                    icon.addClass(iconAllPackClass);
                } else {
                    $('#sidebar .fa.fa-folder-open').removeClass('fa-folder-open').addClass('fa-folder')
                    $('#categories li ul').slideUp({ duration: 100 });
                    icon.addClass(iconAllExpandClass);
                }
            });  
        });
    </script>

    
        
    <div class="widget-wrap">
        <h3 class="widget-title"><span>recent</span></h3>
        <div class="widget">
            <ul id="recent-post" class="no-thumbnail">
                
                    <li>
                        
                        <div class="item-inner">
                            <p class="item-category"><a class="article-category-link" href="/categories/Debugging/">Debugging</a></p>
                            <p class="item-title"><a href="/2025/04/05/A-WSL-Error-Encountered-When-installing-Docker/" class="title">A WSL Error Encountered When Installing Docker</a></p>
                            <p class="item-date"><time datetime="2025-04-05T14:30:36.000Z" itemprop="datePublished">2025-04-05</time></p>
                        </div>
                    </li>
                
                    <li>
                        
                        <div class="item-inner">
                            <p class="item-category"><a class="article-category-link" href="/categories/%E3%80%90Lecture%E3%80%91Software-Analysis-Testing-and-Verification/">【Lecture】Software Analysis Testing and Verification</a></p>
                            <p class="item-title"><a href="/2025/04/04/Introduction-to-Program-Analysis/" class="title">Introduction to Program Analysis</a></p>
                            <p class="item-date"><time datetime="2025-04-04T13:58:30.000Z" itemprop="datePublished">2025-04-04</time></p>
                        </div>
                    </li>
                
                    <li>
                        
                        <div class="item-inner">
                            <p class="item-category"><a class="article-category-link" href="/categories/%E4%BA%BA%E7%94%9F%E6%80%BB%E7%BB%93/">人生总结</a></p>
                            <p class="item-title"><a href="/2025/03/23/ECNU%E8%BD%AF%E5%AD%A6%E5%A4%8D%E8%AF%95%E6%80%BB%E7%BB%93/" class="title">ECNU软学复试总结</a></p>
                            <p class="item-date"><time datetime="2025-03-23T03:46:26.000Z" itemprop="datePublished">2025-03-23</time></p>
                        </div>
                    </li>
                
                    <li>
                        
                        <div class="item-inner">
                            <p class="item-category"><a class="article-category-link" href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E4%B8%93%E4%B8%9A%E5%9F%BA%E7%A1%80/">计算机专业基础</a></p>
                            <p class="item-title"><a href="/2025/03/18/%E7%A6%BB%E6%95%A3%E6%95%B0%E5%AD%A6/" class="title">离散数学</a></p>
                            <p class="item-date"><time datetime="2025-03-18T06:16:34.000Z" itemprop="datePublished">2025-03-18</time></p>
                        </div>
                    </li>
                
                    <li>
                        
                        <div class="item-inner">
                            <p class="item-category"><a class="article-category-link" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a></p>
                            <p class="item-title"><a href="/2024/04/03/%E5%9B%BE%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/" class="title">图注意力机制的原理</a></p>
                            <p class="item-date"><time datetime="2024-04-03T03:23:32.000Z" itemprop="datePublished">2024-04-03</time></p>
                        </div>
                    </li>
                
            </ul>
        </div>
    </div>

    
        
    <div class="widget-wrap">
        <h3 class="widget-title"><span>tag cloud</span></h3>
        <div class="widget tagcloud">
            <a href="/tags/a-star/" style="font-size: 10px;">a-star</a> <a href="/tags/ai/" style="font-size: 10px;">ai</a> <a href="/tags/attention/" style="font-size: 20px;">attention</a> <a href="/tags/deduction/" style="font-size: 10px;">deduction</a> <a href="/tags/discrete-math/" style="font-size: 10px;">discrete math</a> <a href="/tags/docker/" style="font-size: 10px;">docker</a> <a href="/tags/error/" style="font-size: 10px;">error</a> <a href="/tags/flask/" style="font-size: 10px;">flask</a> <a href="/tags/gnn/" style="font-size: 10px;">gnn</a> <a href="/tags/graphics/" style="font-size: 10px;">graphics</a> <a href="/tags/heuristic-search/" style="font-size: 10px;">heuristic search</a> <a href="/tags/hmm/" style="font-size: 10px;">hmm</a> <a href="/tags/machine-learning/" style="font-size: 20px;">machine learning</a> <a href="/tags/program-analysis/" style="font-size: 10px;">program analysis</a> <a href="/tags/svm/" style="font-size: 10px;">svm</a> <a href="/tags/transformer/" style="font-size: 10px;">transformer</a> <a href="/tags/vscode/" style="font-size: 10px;">vscode</a> <a href="/tags/vue3/" style="font-size: 10px;">vue3</a> <a href="/tags/web/" style="font-size: 20px;">web</a> <a href="/tags/%E5%89%8D%E7%AB%AF/" style="font-size: 10px;">前端</a> <a href="/tags/%E5%90%8E%E7%AB%AF/" style="font-size: 10px;">后端</a> <a href="/tags/%E7%94%9F%E6%B4%BB/" style="font-size: 10px;">生活</a>
        </div>
    </div>

    
    <div id="toTop" class="fa fa-angle-up"></div>
</aside>
            
            <section id="main"><article id="post-机器学习入门-吴恩达" class="article article-type-post" itemscope itemprop="blogPost">
    <div class="article-inner">
        
        
            <header class="article-header">
                
                    <div class="article-meta">
                        
    <div class="article-category">
    	<i class="fa fa-folder"></i>
        <a class="article-category-link" href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E4%B8%93%E4%B8%9A%E5%9F%BA%E7%A1%80/">计算机专业基础</a>
    </div>

                        
    <div class="article-tag">
        <i class="fa fa-tag"></i>
        <a class="tag-link-link" href="/tags/machine-learning/" rel="tag">machine learning</a>
    </div>

                        
    <div class="article-date">
        <i class="fa fa-calendar"></i>
        <a href="/2022/07/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8-%E5%90%B4%E6%81%A9%E8%BE%BE/">
            <time datetime="2022-07-21T08:26:16.000Z" itemprop="datePublished">2022-07-21</time>
        </a>
    </div>


                        
                        
                        
                    </div>
                
                
    
        <h1 class="article-title" itemprop="name">
            机器学习入门 吴恩达
        </h1>
    

            </header>
        
        
        <div class="article-entry" itemprop="articleBody">
        
        
            
                <div id="toc" class="toc-article">
                <strong class="toc-title">Catalogue</strong>
                    <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#introduction-to-machine-learning"><span class="toc-number">1.</span> <span class="toc-text">§ Introduction to Machine
Learning</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#definition"><span class="toc-number">1.1.</span> <span class="toc-text">01 Definition</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#two-types-of-learning"><span class="toc-number">1.2.</span> <span class="toc-text">02 Two Types of Learning</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#supervised-learning"><span class="toc-number">1.2.1.</span> <span class="toc-text">2.1 Supervised Learning</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#unsupervised-learning"><span class="toc-number">1.2.2.</span> <span class="toc-text">2.2 Unsupervised learning</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#linear-regression"><span class="toc-number">1.3.</span> <span class="toc-text">03 Linear Regression</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#cost-function"><span class="toc-number">1.4.</span> <span class="toc-text">04 Cost Function</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#gradient-descent"><span class="toc-number">1.5.</span> <span class="toc-text">05 Gradient Descent</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#gradient-descent-for-linear-regression"><span class="toc-number">1.6.</span> <span class="toc-text">06 Gradient Descent for
Linear Regression</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#linear-regression-with-multiple-variables"><span class="toc-number">2.</span> <span class="toc-text">§ Linear Regression
with Multiple Variables</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#multiple-features"><span class="toc-number">2.1.</span> <span class="toc-text">01 Multiple Features</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#introduction"><span class="toc-number">2.1.1.</span> <span class="toc-text">1.1 Introduction</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#multiple-featuresvariables"><span class="toc-number">2.1.2.</span> <span class="toc-text">1.2 Multiple
Features(variables)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#simplication-of-h_thetax"><span class="toc-number">2.1.3.</span> <span class="toc-text">1.3 Simplication of hθ(x)</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#feature-scaling"><span class="toc-number">2.2.</span> <span class="toc-text">02 Feature Scaling</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#introduction-1"><span class="toc-number">2.2.1.</span> <span class="toc-text">2.1 Introduction</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#mean-normalization"><span class="toc-number">2.2.2.</span> <span class="toc-text">2.2 Mean Normalization</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#learning-rate"><span class="toc-number">2.3.</span> <span class="toc-text">03 Learning Rate</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#debugging"><span class="toc-number">2.3.1.</span> <span class="toc-text">3.1 Debugging</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#method-1"><span class="toc-number">2.3.1.1.</span> <span class="toc-text">3.1.1 Method 1</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#method-2"><span class="toc-number">2.3.1.2.</span> <span class="toc-text">3.1.2 Method 2</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#learning-rate-1"><span class="toc-number">2.3.2.</span> <span class="toc-text">3.2 Learning Rate</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#normal-equation"><span class="toc-number">2.4.</span> <span class="toc-text">04 Normal Equation</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#introduction-2"><span class="toc-number">2.4.1.</span> <span class="toc-text">4.1 Introduction</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#explaination"><span class="toc-number">2.4.2.</span> <span class="toc-text">4.2 Explaination</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#gradient-descent-v.s.-normal-equation"><span class="toc-number">2.4.3.</span> <span class="toc-text">4.3 Gradient Descent v.s.
Normal Equation</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#logistic-regression"><span class="toc-number">3.</span> <span class="toc-text">§ Logistic Regression</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#introduction-3"><span class="toc-number">3.1.</span> <span class="toc-text">01 Introduction</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#hypotheses-representation"><span class="toc-number">3.2.</span> <span class="toc-text">02 Hypotheses Representation</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#logistic-regression-model"><span class="toc-number">3.2.1.</span> <span class="toc-text">2.1 Logistic Regression Model</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#interpretation-of-hypothesis-output"><span class="toc-number">3.2.2.</span> <span class="toc-text">2.2 Interpretation of
Hypothesis Output</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#decision-boundary"><span class="toc-number">3.3.</span> <span class="toc-text">03 Decision Boundary</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#recap-logistic-regression"><span class="toc-number">3.3.1.</span> <span class="toc-text">3.1 Recap: Logistic Regression</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#decision-boundary-1"><span class="toc-number">3.3.2.</span> <span class="toc-text">3.2 Decision Boundary</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#non-linear-decision-boundaries"><span class="toc-number">3.3.3.</span> <span class="toc-text">3.3 Non-linear decision
boundaries</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#cost-function-1"><span class="toc-number">3.4.</span> <span class="toc-text">04 Cost Function</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#why-the-former-wont-work"><span class="toc-number">3.4.1.</span> <span class="toc-text">4.1 Why the Former Won’t Work</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#logistic-regression-cost-function"><span class="toc-number">3.4.2.</span> <span class="toc-text">4.2 Logistic Regression Cost
Function</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#simplified-cost-function-and-gradient-descent"><span class="toc-number">3.5.</span> <span class="toc-text">05 Simplified
Cost Function and Gradient Descent</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#simplified-cost-function"><span class="toc-number">3.5.1.</span> <span class="toc-text">5.1 Simplified Cost Function</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#gradient-descent-1"><span class="toc-number">3.5.2.</span> <span class="toc-text">5.2 Gradient Descent</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#advance-optimization"><span class="toc-number">3.6.</span> <span class="toc-text">06 Advance Optimization</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#multi-class-classification-one-vs-all"><span class="toc-number">3.7.</span> <span class="toc-text">07 Multi-class
Classification: One vs all</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#multi-class-classification"><span class="toc-number">3.7.1.</span> <span class="toc-text">7.1 Multi-class Classification</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#one-vs-all"><span class="toc-number">3.7.2.</span> <span class="toc-text">7.2 One vs all</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#regularization"><span class="toc-number">4.</span> <span class="toc-text">§ Regularization</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#the-problem-of-overfitting"><span class="toc-number">4.1.</span> <span class="toc-text">01 The Problem of Overfitting</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#overfitting"><span class="toc-number">4.1.1.</span> <span class="toc-text">1.1 Overfitting</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#addressing-overfitting"><span class="toc-number">4.1.2.</span> <span class="toc-text">1.2 Addressing Overfitting</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#cost-function-2"><span class="toc-number">4.2.</span> <span class="toc-text">02 Cost Function</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#regularized-linear-regression"><span class="toc-number">4.3.</span> <span class="toc-text">03 Regularized Linear
Regression</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#gradient-descent-2"><span class="toc-number">4.3.1.</span> <span class="toc-text">3.1 Gradient Descent</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#normal-equation-1"><span class="toc-number">4.3.2.</span> <span class="toc-text">3.2 Normal Equation</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#regularized-logistic-regression"><span class="toc-number">4.4.</span> <span class="toc-text">04 Regularized Logistic
Regression</span></a></li></ol></li></ol>
                </div>
            
        
        
            <h2 id="introduction-to-machine-learning">§ Introduction to Machine
Learning</h2>
<h3 id="definition">01 Definition</h3>
<ul>
<li>Machine Learning: Field of study that gives computers the ability to
learn without being explicitly programmed. — Arthur Samuel</li>
<li>Well-posed Learning Problem: <span
class="math inline"><em>A</em></span> compouter program is said to learn
from experience <span class="math inline"><em>E</em></span> with respect
to some task <span class="math inline"><em>T</em></span> and some
performance measure <span class="math inline"><em>P</em></span>, if its
performance on <span class="math inline"><em>T</em></span>, as measured
by <span class="math inline"><em>P</em></span>, imporoves with
experience <span class="math inline"><em>E</em></span>. — Tom
Mitchell</li>
<li>Machine Learning algorithms:
<ul>
<li>Supervised learning</li>
<li>Unsupervised learning</li>
<li>Others: Reinforcement learning, recommender systems</li>
</ul></li>
</ul>
<span id="more"></span>
<h3 id="two-types-of-learning">02 Two Types of Learning</h3>
<p>There are two major types of learning —— supervised learning and
unsupervised learning.</p>
<h4 id="supervised-learning">2.1 Supervised Learning</h4>
<ul>
<li>Definition: The term supervised learning refers to the fact that we
give the algorithm a dataset in which the “right answer” are given.</li>
<li>Category
<ul>
<li>Regression: Predict continuous valued output</li>
<li>Classification: Give discrete valued outout</li>
</ul></li>
</ul>
<h4 id="unsupervised-learning">2.2 Unsupervised learning</h4>
<ul>
<li>Definition: Give the algorithm a bunch of data without any explicit
instruciton or imformation, but require it to categorize the data into
different clusters automatically.</li>
</ul>
<h3 id="linear-regression">03 Linear Regression</h3>
<ul>
<li>Definition
<ul>
<li>Regression analysis is a statistical analysis method to determine
the interdependent quantitative relationship between two or more
variables.</li>
<li>According to the number of variables involved, regression analysis
can be divided into univariate regression and multivariate regression
analysis.</li>
</ul></li>
</ul>
<p>The following example is a classic linear regression problem ——
Portland housing price prediction.</p>
<p><img src="/img/Introduction-to-Machine-Learning-01.png" /></p>
<p>The exact figures are examplified like:</p>
<p><img src="/img/Introduction-to-Machine-Learning-02.png" /></p>
<p>We are going to analyse the above data of housing prices from the
city of Portland, Oregan to predict the posibble price of a house
according to its size based on the analysis result. It’s a clear
supervised learning (regression) problem.</p>
<p>So the algorithm is aimed to find a hypothesis function through
quantities of data, and the function will map <span
class="math inline"><em>x</em></span> (size of house) to <span
class="math inline"><em>y</em></span> (estimated price) to get the
correct answer. The whole process can be illustrated by the graph
below.</p>
<p><img src="/img/Introduction-to-Machine-Learning-03.png" /></p>
<p>For there is only one variate (size of house) in this model, we call
it <strong>univariate</strong> linear regression.</p>
<h3 id="cost-function">04 Cost Function</h3>
<p>Let’s continue with the example of housing price prediction. Assume
the <span class="math inline"><em>m</em></span> equals 47, which
represents the number of training examples. And the hypothesis function
is as below (a univariate linear function): <span
class="math display"><em>h</em><sub><em>θ</em></sub>(<em>x</em>) = <em>θ</em><sub>0</sub> + <em>θ</em><sub>1</sub><em>x</em></span>
These <span class="math inline"><em>θ</em><sub>0</sub></span> and <span
class="math inline"><em>θ</em><sub>1</sub></span> are called the
parameters of the model.</p>
<p>So the idea is to choose the appropriate <span
class="math inline"><em>θ</em><sub>0</sub></span> and <span
class="math inline"><em>θ</em><sub>1</sub></span> so that <span
class="math inline"><em>h</em><sub><em>θ</em></sub>(<em>x</em>)</span>
can be approximated to the <span class="math inline"><em>y</em></span>
for our examples <span
class="math inline">(<em>x</em>,<em>y</em>)</span>, which is to say that
we want the difference between <span
class="math inline"><em>h</em>(<em>x</em>)</span> and <span
class="math inline"><em>y</em></span> to be small, i.e. minimize <span
class="math display">$$
J(\theta_0,\theta_1)=\frac{1}{2m}\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})^2
$$</span> The above function <span
class="math inline"><em>J</em>(<em>θ</em><sub>0</sub>,<em>θ</em><sub>1</sub>)</span>
is what we called Cost Function. And our goal is to minimize its result
by finding the best values for <span
class="math inline"><em>θ</em><sub>0</sub></span> and <span
class="math inline"><em>θ</em><sub>1</sub></span>.</p>
<p>The relation between the hypothesis function and the cost
function:</p>
<ul>
<li>The hypothesis function <span
class="math inline"><em>h</em><sub><em>θ</em></sub>(<em>x</em>)</span>
is a function of <span class="math inline"><em>x</em></span> for fixed
<span class="math inline"><em>θ</em></span>, while the cost function
<span
class="math inline"><em>J</em>(<em>θ</em><sub>0</sub>,<em>θ</em><sub>1</sub>)</span>
is of the parameter <span
class="math inline"><em>θ</em><sub>0</sub></span> and <span
class="math inline"><em>θ</em><sub>1</sub></span>.</li>
<li>By <span
class="math inline"><em>J</em>(<em>θ</em><sub>0</sub>,<em>θ</em><sub>1</sub>)</span>
we obtain the accurate <span class="math inline"><em>θ</em></span> we
want so that we can figure out <span
class="math inline"><em>h</em><sub><em>θ</em></sub>(<em>x</em>)</span>,
which is our final goal.</li>
</ul>
<h3 id="gradient-descent">05 Gradient Descent</h3>
<p>Let’s visualize the funtction <span
class="math inline"><em>J</em>(<em>θ</em><sub>0</sub>,<em>θ</em><sub>1</sub>)</span>
to better understand it:</p>
<p><img src="/img/Introduction-to-Machine-Learning-04.png" /></p>
<p>Imagine that the figure is a huge mountain and you are standing on
one point of it. Then think: which direction should I step in if I want
to physically walk down this mountain as quickly as possible? Once you
have determined the direction, you take your step and then stop to think
again: what direction should I take that step in next? Keep walking and
keep asking until you converge to this local minimum as the following
figure shows.</p>
<p><img src="/img/Introduction-to-Machine-Learning-05.png" /></p>
<p>The whole process is what we called <strong>Gradient
Descent</strong>.</p>
<p>The following algorithm gives the mathematical form of gradient
descent, which is not difficult to understand so I skip its
explaination.</p>
<p><img src="/img/Introduction-to-Machine-Learning-06.png" /></p>
<h3 id="gradient-descent-for-linear-regression">06 Gradient Descent for
Linear Regression</h3>
<p>What if we want to apply gradient descent to linear regression?</p>
<p><img src="/img/Introduction-to-Machine-Learning-07.png" /></p>
<p>It is pretty easy actually. All we need to do is to take the
derivatives of the <span
class="math inline"><em>θ</em><sub>0</sub></span> and <span
class="math inline"><em>θ</em><sub>1</sub></span> in the cost function
separately and substitute the derivatives into the gradient descent
algorithm. The algorithm below shows the gradient descent for linear
regression.</p>
<p><img src="/img/Introduction-to-Machine-Learning-08.png" /></p>
<p>When we implement the algorithm, the cost function is approaching the
optimum and the hypothesis function is getting more and more in line
with our dataset. The illustration below shows the visualization of the
process.</p>
<p><img src="/img/Introduction-to-Machine-Learning-09.png" /></p>
<p>If we accumulate the gradients of a small batch of data samples to
update the parameters at once, then we get <strong>Batch</strong>
Gradient Descent.</p>
<h2 id="linear-regression-with-multiple-variables">§ Linear Regression
with Multiple Variables</h2>
<h3 id="multiple-features">01 Multiple Features</h3>
<h4 id="introduction">1.1 Introduction</h4>
<p>In the previous sections, we’ve talked about linear regression with
one variate. What if we got more variates?</p>
<p>Multiple variates mean we will have multiple features. In the
previous section, we only have one feature and that is the size of the
house. Now we let’s discuss the multi-feature situation.</p>
<h4 id="multiple-featuresvariables">1.2 Multiple
Features(variables)</h4>
<p><img
src="/img/Linear-Regression-with-Multiple-Variables-01.png" /></p>
<p>In the above case, we have the data of the house’s bedrooms, floors
and age and hopefully to predict the possible price of the house. All of
this four numbers are called <span
class="math inline"><em>f</em><em>e</em><em>a</em><em>t</em><em>u</em><em>r</em><em>e</em><em>s</em></span>.</p>
<p>For there are four features, the input <span
class="math inline"><em>x</em><sup>(<em>i</em>)</sup></span> will be a
four-dimensional vector.</p>
<p>We use <span class="math inline"><em>n</em></span> to represent the
number of features here, <span
class="math inline"><em>x</em><sup>(<em>i</em>)</sup></span> as the
input of <span
class="math inline"><em>i</em><sup><em>t</em><em>h</em></sup></span>
training example and <span
class="math inline"><em>x</em><sub><em>j</em></sub><sup>(<em>i</em>)</sup></span>
as the value of feature <span class="math inline"><em>j</em></span> in
<span
class="math inline"><em>i</em><sup><em>t</em><em>h</em></sup></span>
training example.</p>
<p>Given that we have multiple features now, the previous hypothesis
function is no longer appropriate. Here is the rewritten one:</p>
<p><span
class="math display"><em>h</em><sub><em>θ</em></sub>(<em>x</em>) = <em>θ</em><sub>0</sub> + <em>θ</em><sub>1</sub><em>x</em><sub>1</sub> + <em>θ</em><sub>2</sub><em>x</em><sub>2</sub> +  ·  ·  ·  + <em>θ</em><sub><em>n</em></sub><em>x</em><sub><em>n</em></sub></span></p>
<p>The above function is clearly too complicated. May we simplify
it?</p>
<h4 id="simplication-of-h_thetax">1.3 Simplication of <span
class="math inline"><em>h</em><sub><em>θ</em></sub>(<em>x</em>)</span></h4>
<p>For convenience of notation, we define <span
class="math inline"><em>x</em><sub>0</sub> = 1</span>, which is the
equivalent to an additional feature. So previously we have <span
class="math inline"><em>n</em></span> features but now we’ve got <span
class="math inline"><em>n</em> + 1</span> whereas we are defining an
additional sort of zero feature vector that always takes on the value of
<span class="math inline">1</span>.</p>
<p>So now we have a (n+1)-dimensional indexed from <span
class="math inline">0</span> to <span
class="math inline"><em>n</em></span>.</p>
<p>And we are also going to think of our parameters as a vector. They
will be like below:</p>
<p><span class="math display">$$
X=\begin{bmatrix}
x_{0}\\
x_{1}\\
x_{2}\\
···\\
x_{n}\\
\end{bmatrix}
\;\;\;\;\;\;\;
\theta=\begin{bmatrix}
\theta_{0}\\
\theta_{1}\\
\theta_{2}\\
···\\
\theta_{n}\\
\end{bmatrix}
$$</span></p>
<p>So the hypothesis function can be rewritten like:</p>
<p><span
class="math display"><em>h</em><sub><em>θ</em></sub>(<em>x</em>) = <em>θ</em><sub>0</sub><em>x</em><sub>0</sub> + <em>θ</em><sub>1</sub><em>x</em><sub>1</sub> + <em>θ</em><sub>2</sub><em>x</em><sub>2</sub> +  ·  ·  ·  + <em>θ</em><sub><em>n</em></sub><em>x</em><sub><em>n</em></sub> (<em>x</em><sub>0</sub>=1)</span></p>
<p>If you are familiar with the vector multiplication, you will know it
is also equal to this:</p>
<p><span
class="math display"><em>h</em><sub><em>θ</em></sub>(<em>x</em>) = <em>θ</em><sup><em>T</em></sup><em>X</em></span></p>
<h3 id="feature-scaling">02 Feature Scaling</h3>
<h4 id="introduction-1">2.1 Introduction</h4>
<p>Imagine you have two features — <span
class="math inline"><em>x</em><sub>1</sub></span> and <span
class="math inline"><em>x</em><sub>2</sub></span>. The difference btween
this two numbers are really really large, for example, <span
class="math inline"><em>x</em><sub>1</sub></span> ranges from 0 to 2000
while <span class="math inline"><em>x</em><sub>2</sub></span> ranges
from 1 to 5. That way the contour of the cost function can take on this
sort of very skewed elliptical shape:</p>
<p><img
src="/img/Linear-Regression-with-Multiple-Variables-02.png" /></p>
<p>Thus your gradients may oscillate back and forth and end up taking a
long time before it can finally find its way to the global minimum:</p>
<p><img
src="/img/Linear-Regression-with-Multiple-Variables-03.png" /></p>
<p>In this setting, a useful thing to do is to scale the features. For
example, the feature <span
class="math inline"><em>x</em><sub>1</sub></span> is devided by 2000 and
feature <span class="math inline"><em>x</em><sub>2</sub></span> is
devided by 5, so that the contour will be much less skewed and your
gradients will find a direct way to the global minimun instead of
following a much more complicated trajectory.</p>
<p>This process is what we called <strong>Feature Scaling</strong>,
i.e. get every feature into approximately a <span
class="math inline"> − 1 ≤ <em>x</em><sub><em>i</em></sub> ≤ <em>i</em></span>
range.</p>
<h4 id="mean-normalization">2.2 Mean Normalization</h4>
<p>Replace <span
class="math inline"><em>x</em><sub><em>i</em></sub></span> with <span
class="math inline"><em>x</em><sub><em>i</em></sub> − <em>μ</em><sub><em>i</em></sub></span>，(<span
class="math inline"><em>μ</em></span> is the average) to make features
have approximately zero mean (Do not apply to <span
class="math inline"><em>x</em><sub>0</sub> = 1</span>)</p>
<p>For example, if you know the average size of a house is equal to
1000, you might use this formula set the feature <span
class="math inline"><em>x</em><sub>1</sub></span> to be size minus the
average value 1000 divided by 2000. Similarily, if every house has 1 to
5 bedrooms and on average a house has 2 bedrooms, you might use this
formula set the feature <span
class="math inline"><em>x</em><sub>2</sub></span> to be size minus the
average value 2 divided by 5.</p>
<p><img
src="/img/Linear-Regression-with-Multiple-Variables-04.png" /></p>
<h3 id="learning-rate">03 Learning Rate</h3>
<h4 id="debugging">3.1 Debugging</h4>
<p>Making sure gradient descent is working correctly.</p>
<h5 id="method-1">3.1.1 Method 1</h5>
<p><img
src="/img/Linear-Regression-with-Multiple-Variables-05.png" /></p>
<p>What we usually do is to plot the cost function <span
class="math inline"><em>J</em>(<em>θ</em>)</span> as gradient descent
runs. Hopefully <span class="math inline"><em>J</em>(<em>θ</em>)</span>
will decrease after every iteration of gradient descent and its plot
will flatten eventually, which means gradient descent has more or less
converged because your cost function isn’t going down much more. So
looking at this figure can help you judge whether or not gradient
descent has converged.</p>
<p><img
src="/img/Linear-Regression-with-Multiple-Variables-06.png" /></p>
<p>If the plot is increasing like above, that means you gradient descent
is not working normally. Sometimes it means you should be using a tinier
learning rate.</p>
<h5 id="method-2">3.1.2 Method 2</h5>
<p>It’s also possible to come up with automatic convergence test, namely
to have an algorithm to try to tell you if gradient descent has
converged.</p>
<p>Example automatic convergence test: Declare convergence if <span
class="math inline"><em>J</em>(<em>θ</em>)</span> decrease by less than
<span class="math inline">10<sup>−3</sup></span> in one iteration.</p>
<p>Here <span class="math inline">10<sup>−3</sup></span> is a threshold.
But this threshold is not fixed and sometimes it’s hard to choose the
appropriate threshold. So method 1 may be more practical.</p>
<h4 id="learning-rate-1">3.2 Learning Rate</h4>
<p>In the situation we’ve meantioned above, we said that if the value of
the cost function keep increasing, it’s often because the learning rate
is too big. Now let me explain why.</p>
<p>If your learning rate is too big, gradient descent may overshoot the
minimum over and over again and deviate from the minimum, so that you
will end up getting the higher value of the cost function.</p>
<p><img
src="/img/Linear-Regression-with-Multiple-Variables-07.png" /></p>
<p>If learning rate is too small, the convergence can be very slow.</p>
<h3 id="normal-equation">04 Normal Equation</h3>
<h4 id="introduction-2">4.1 Introduction</h4>
<p>In the previous sections, we’ve introduced an iterative algorithm
that takes many steps, multiple iterations of gradient descent to
converge to the global minimum.</p>
<p>In contrast, normal equation would give us a method to solve for
<span class="math inline"><em>θ</em></span> analytically. So rather than
run the iterative algorithm, we can instead just solve for the optimal
value for <span class="math inline"><em>θ</em></span> all at one go.</p>
<p>Mathematically, to work out the minimum of a multivariate funtion is
to take its partial deviratives and set them equal to zero, which can be
somewhat involved.</p>
<h4 id="explaination">4.2 Explaination</h4>
<p>Assume we have a dataset:</p>
<p><img
src="/img/Linear-Regression-with-Multiple-Variables-08.png" /></p>
<p>We take all the number <span
class="math inline"><em>x</em><sub>0</sub></span> to <span
class="math inline"><em>x</em><sub>4</sub></span> to construct a <span
class="math inline"><em>m</em> * (<em>n</em>+1)</span> matrix like:</p>
<p><span class="math display">$$
X=\begin{bmatrix}
1 &amp; 2104 &amp; 5 &amp; 1 &amp; 45\\
1 &amp; 1416 &amp; 3 &amp; 2 &amp; 40\\
1 &amp; 1534 &amp; 3 &amp; 2 &amp; 30\\
1 &amp; 852 &amp; 2 &amp; 1 &amp; 36\\
\end{bmatrix}
$$</span></p>
<p>And then do the similar thing to <span
class="math inline"><em>y</em></span> to construct a m-dimensional
vector like:</p>
<p><span class="math display">$$
y=\begin{bmatrix}
460\\
232\\
315\\
178\\
\end{bmatrix}
$$</span></p>
<p>And we can get the values of <span
class="math inline"><em>θ</em></span> by the formula:</p>
<p><span
class="math display"><em>θ</em> = (<em>X</em><sup><em>T</em></sup><em>X</em>)<sup>−1</sup><em>X</em><sup><em>T</em></sup><em>y</em></span></p>
<p>We use <span class="math inline"><em>x</em><sup>(1)</sup></span> to
represent the <span
class="math inline"><em>i</em><sup><em>t</em><em>h</em></sup></span>
features and <span
class="math inline"><em>y</em><sup>(<em>i</em>)</sup></span> to
represent the <span
class="math inline"><em>i</em><sup><em>t</em><em>h</em></sup></span>
price; then <span
class="math inline">(<em>x</em><sup>(<em>i</em>)</sup>,<em>y</em><sup>(<em>i</em>)</sup>)</span>
forms an example. <span
class="math inline"><em>x</em><sup>(<em>i</em>)</sup></span> will be
like:</p>
<p><span class="math display">$$
x^{(i)}=\begin{bmatrix}
x^{(i)}_0\\
x^{(i)}_1\\
x^{(i)}_2\\
...\\
x^{(i)}_i\\
\end{bmatrix}
$$</span></p>
<p>Here we are going to construct a matrix X called <span
class="math inline"><em>D</em><em>e</em><em>s</em><em>i</em><em>g</em><em>n</em> <em>M</em><em>a</em><em>t</em><em>r</em><em>i</em><em>x</em></span>:</p>
<p><span class="math display">$$
X_{m*(n+1)}=\begin{bmatrix}
(x^{(1)})^T\\
(x^{(2)})^T\\
(x^{(3)})^T\\
...\\
(x^{(m)})^T\\
\end{bmatrix}
$$</span></p>
<p>For instance, if <span
class="math inline"><em>x</em><sup>(<em>i</em>)</sup></span> is:</p>
<p><span class="math display">$$
x^{(i)}=\begin{bmatrix}
1\\
x^{(i)}_1\\
\end{bmatrix}
$$</span></p>
<p>Then the design matrix will be like:</p>
<p><span class="math display">$$
X=\begin{bmatrix}
(x^{(1)})^T\\
(x^{(2)})^T\\
(x^{(3)})^T\\
...\\
(x^{(m)})^T\\
\end{bmatrix}
=\begin{bmatrix}
1 &amp; x^{(1)}_1\\
1 &amp; x^{(2)}_1\\
1 &amp; x^{(3)}_1\\
... &amp; ...\\
1 &amp; x^{(m)}_1\\
\end{bmatrix}
$$</span></p>
<p>The by the formula above:</p>
<p><span
class="math display"><em>θ</em> = (<em>X</em><sup><em>T</em></sup><em>X</em>)<sup>−1</sup><em>X</em><sup><em>T</em></sup><em>y</em></span></p>
<p>We can obtain all the values of <span
class="math inline"><em>θ</em></span>.</p>
<h4 id="gradient-descent-v.s.-normal-equation">4.3 Gradient Descent v.s.
Normal Equation</h4>
<ul>
<li>Gradient Descent:
<ul>
<li>Needs to choose α</li>
<li>Needs many iterations</li>
<li>Works well even when <span class="math inline"><em>n</em></span> is
large</li>
</ul></li>
<li>Normal Equation
<ul>
<li>No need to choose α</li>
<li>Don’t need to iterate</li>
<li>Need to compute <span
class="math inline">(<em>X</em><sup><em>T</em></sup><em>X</em>)<sup>−1</sup></span></li>
<li>Slow if <span class="math inline"><em>n</em></span> is very
large</li>
</ul></li>
</ul>
<h2 id="logistic-regression">§ Logistic Regression</h2>
<h3 id="introduction-3">01 Introduction</h3>
<p>Linear Regression is a good tool to help solve the regression
problem, which predicts continuous valued output. However, it won’t be a
good idea if we apply it to the classification problem, which gives
discrete valued output. That’s because linear regression can be
susceptible to outlier data, rendering the result inaccurate.</p>
<p>So here we introduce a new method to cope with the classification
problem — <strong>Logistic Regression</strong>.</p>
<h3 id="hypotheses-representation">02 Hypotheses Representation</h3>
<h4 id="logistic-regression-model">2.1 Logistic Regression Model</h4>
<p>The value of the hypothesis function in classification problem should
be between 0 ~ 1: <span
class="math inline">0 ≤ <em>h</em><sub><em>θ</em></sub>(<em>x</em>) ≤ 1</span></p>
<p>As the previous sections have shown, <span
class="math inline"><em>h</em><sub><em>θ</em></sub>(<em>x</em>) = <em>θ</em><sup><em>T</em></sup><em>x</em></span>.
Now we are going to make the hypothesis equal <span
class="math inline"><em>g</em>(<em>θ</em><sup><em>T</em></sup><em>x</em>)</span>,
where we define the function <span class="math inline"><em>g</em></span>
as follows:</p>
<p><span class="math display">$$
g(z)=\frac{1}{1+e^{(-z)}}
$$</span></p>
<p>This function is what we called <span
class="math inline"><em>S</em><em>i</em><em>g</em><em>m</em><em>o</em><em>i</em><em>d</em> <em>F</em><em>u</em><em>n</em><em>t</em><em>i</em><em>o</em><em>n</em></span>
or <span
class="math inline"><em>L</em><em>o</em><em>g</em><em>i</em><em>s</em><em>t</em><em>i</em><em>c</em> <em>F</em><em>u</em><em>n</em><em>c</em><em>t</em><em>i</em><em>o</em><em>n</em></span>,
where <span
class="math inline"><em>L</em><em>o</em><em>g</em><em>i</em><em>s</em><em>t</em><em>i</em><em>c</em> <em>R</em><em>e</em><em>g</em><em>r</em><em>e</em><em>s</em><em>s</em><em>i</em><em>o</em><em>n</em></span>
gains its name.</p>
<p>Let’s plug the parameter back into it. Then there is an alternative
way writing out the form of our hypothesis:</p>
<p><span class="math display">$$
h_\theta(x)=\frac{1}{1+e^{-\theta^Tx}}
$$</span></p>
<p>Its curve looks like:</p>
<p><img src="/img/Logistic-Regression-01.png" /></p>
<p>As you can see, the sigmoid function approaches 1 as its parameter
<span class="math inline"><em>z</em></span> approaches infinity, and
approaches 0 as its parameter approaches infinitesimal.</p>
<h4 id="interpretation-of-hypothesis-output">2.2 Interpretation of
Hypothesis Output</h4>
<p><span class="math display">$$
h_\theta(x)=\text{estimated\;probability\;of\;}y=1\;\text{on\;input}\;x
$$</span></p>
Example: If $x=
=
<p>$, <span
class="math inline"><em>h</em><sub><em>θ</em></sub>(<em>x</em>) = 0.7</span></p>
<p>Tell patient that 70% chance of tumor being malignant.</p>
<h3 id="decision-boundary">03 Decision Boundary</h3>
<h4 id="recap-logistic-regression">3.1 Recap: Logistic Regression</h4>
<p><img src="/img/Logistic-Regression-02.png" /></p>
<p>Viewing the plot of our hypothesis function, we can easily find that
its value is greater than or equal to 0.5 whenever the parameter is
positive, while it’s less than 0.5 whenever the parameter is
negative.</p>
<h4 id="decision-boundary-1">3.2 Decision Boundary</h4>
<p>Suppose we have a dataset and hypothesis function like:</p>
<p><img src="/img/Logistic-Regression-03.png" /></p>
<p>Concretely, we define the parameters <span
class="math inline"><em>θ</em></span> to be:</p>
<p><span class="math display">$$
\theta=\begin{bmatrix}
-3\\
1\\
1\\
\end{bmatrix}
$$</span></p>
<p>Now let’s figure out where a hypothesis will end up predicting <span
class="math inline"><em>y</em> = 1</span> and where it will end up
predicting <span class="math inline"><em>y</em> = 0</span>.</p>
<p>Using the conclusion we’ve drawn in the recap, we know that <span
class="math inline"><em>y</em></span> is more likely equal to 1, that is
the probability that y equals 1 is greater than or equal to 0.5,
whenever <span
class="math inline"><em>θ</em><sup><em>T</em></sup><em>x</em></span> is
greater then zero:</p>
<p><span
class="math display">predict  <em>y</em> = 1  if   − 3 + <em>x</em><sub>1</sub> + <em>x</em><sub>2</sub> &gt;  = 0</span></p>
<p>That’s equivalent to <span
class="math inline"><em>x</em><sub>1</sub> + <em>x</em><sub>2</sub> ≥ 3</span>.
So if we draw a plot of <span
class="math inline"><em>x</em><sub>1</sub> + <em>x</em><sub>2</sub> = 3</span>,
it’s basically like:</p>
<p><img src="/img/Logistic-Regression-04.png" /></p>
<p>And we call that line <span
class="math inline"><em>D</em><em>e</em><em>c</em><em>i</em><em>s</em><em>i</em><em>o</em><em>n</em> <em>B</em><em>o</em><em>u</em><em>n</em><em>d</em><em>a</em><em>r</em><em>y</em></span>.
And the <span
class="math inline"><em>x</em><sub>1</sub>, <em>x</em><sub>2</sub></span>
above the line corrspond with the <span
class="math inline"><em>y</em></span> equal to 1, while in contrast, the
<span
class="math inline"><em>x</em><sub>1</sub>, <em>x</em><sub>2</sub></span>
fall below the line correspond with the <span
class="math inline"><em>y</em></span> euqal to 0.</p>
<p>Just to be clear, the decision boundary is a property of the
hypothesis including the parameters <span
class="math inline"><em>θ</em></span> rather than the dataset, which
means even if we change all the data, as long as the hypothesis remains
the same, the boundary remains the same.</p>
<h4 id="non-linear-decision-boundaries">3.3 Non-linear decision
boundaries</h4>
<p><img src="/img/Logistic-Regression-05.png" /></p>
<p>In the above case, we define the parameters <span
class="math inline"><em>θ</em></span> to be <span
class="math inline">[−1,0,0,1,1]</span>. And still we can draw a line to
separate thoses <span class="math inline"><em>x</em></span> but clearly
it’s more complicated than just a simple line. The decision boundary can
be much more complex especially when it comes to higher order polynomial
terms. But no matter how complex it is, it’s still a property of the
hypothesis instead of the training set.</p>
<h3 id="cost-function-1">04 Cost Function</h3>
<h4 id="why-the-former-wont-work">4.1 Why the Former Won’t Work</h4>
<p><img src="/img/Logistic-Regression-06.png" /></p>
<p>Back to when we were developing a linear regression model, we use the
following cost function:</p>
<p><span class="math display">$$
J(\theta)=\frac{1}{m}∑^m_{i=1}\frac{1}{2}(h_θ(x^{(i)})-y^{(i)})^2
$$</span></p>
<p>This time, we are going so change it a little bit. Let’s define</p>
<p><span class="math display">$$
Cost(h_θ(x^{(i)}),y^{(i)})=\frac{1}{2}(h_θ(x^{(i)})-y^{(i)})^2
$$</span></p>
<p>It seems like this cost function that works for linear regression can
still be applied to logistic regression, but however, it’s not quite
appropriate, because if we plug <span
class="math inline">$h_\theta(x)=\frac{1}{1+e^{-\theta^Tx}}$</span>in
the function, you will find that the function is not convex.</p>
<p><img src="/img/Logistic-Regression-07.jpg" /></p>
<p>Usually, when a function is convex, its plot is bowl-shaped, which is
easy to spot the global minimum. However when it’s non-convex, there
will be many local optimums, making it difficult for gradient descent to
converge to the global minimum.</p>
<p>So we are going to choose another function better suitable for
logistic regression.</p>
<h4 id="logistic-regression-cost-function">4.2 Logistic Regression Cost
Function</h4>
<p>So we give the cost function for logistic regression: <span
class="math display">$$
Cost(h_θ(x),y)=\begin{cases}
-log(h_θ(x)) &amp; if\;y=1\\
-log(1-h_θ(x)) &amp; if\;y=0\\
\end{cases}
$$</span></p>
<p>Its curve be like:</p>
<p><img src="/img/Logistic-Regression-08.png" /></p>
<p><img src="/img/Logistic-Regression-09.png" /></p>
<h3 id="simplified-cost-function-and-gradient-descent">05 Simplified
Cost Function and Gradient Descent</h3>
<h4 id="simplified-cost-function">5.1 Simplified Cost Function</h4>
<p><img src="/img/Logistic-Regression-10.png" /></p>
<p>Because <span class="math inline"><em>y</em></span> is either 0 or 1,
we’ll be able to come up with a simpler way to write the cost function.
Rather than write this function in separated two lines, we are going to
take these two lines and compress them into one equation, which will
make it convenient for us to write the cost function and derive gradient
descent.</p>
<p>Concretely, we can write out the cost function as follows:</p>
<p><span
class="math display"><em>C</em><em>o</em><em>s</em><em>t</em>(<em>h</em><sub><em>θ</em></sub>(<em>x</em>),<em>y</em>) =  − <em>y</em><em>l</em><em>o</em><em>g</em>(<em>h</em><sub><em>θ</em></sub>(<em>x</em>)) − (1−<em>y</em>)<em>l</em><em>o</em><em>g</em>(1−<em>h</em><sub><em>θ</em></sub>(<em>x</em>))</span></p>
<p>It’s easy to figure out why does this equation equal the original
one.</p>
<p>As we know, <span class="math inline"><em>y</em></span> is either 1
or 0.</p>
<p>If <span class="math inline"><em>y</em> = 1</span>, this equation
will be <span
class="math inline"> − <em>l</em><em>o</em><em>g</em>(<em>h</em><sub><em>θ</em></sub>(<em>x</em>))</span>.</p>
<p>If <span class="math inline"><em>y</em> = 0</span>, this equation
will be <span
class="math inline"> − <em>l</em><em>o</em><em>g</em>(1−<em>h</em><sub><em>θ</em></sub>(<em>x</em>))</span>.</p>
<p>This is exactly the same as the original equation.</p>
<p>The cost function:</p>
<p><span class="math display">$$
J(θ)=\frac{1}{m}∑^m_{i=1}Cost(h_θ(x^{(i)}),y^{(i)})\\=-\frac{1}{m}[∑^m_{(i=1)}y^{(i)}logh_θ(x^{(i)})+(1-y^{(i)})log(1-h_θ(x^{(i)}))]
$$</span></p>
<h4 id="gradient-descent-1">5.2 Gradient Descent</h4>
<p>Now we have got the new cost function. The next step is to fit
parameters <span class="math inline"><em>θ</em></span> and what we are
going to fit parameters <span class="math inline"><em>θ</em></span> is
try to find the parameters θ that minimizes <span
class="math inline"><em>J</em>(<em>θ</em>)</span>.</p>
<p><img src="/img/Logistic-Regression-11.png" /></p>
<h3 id="advance-optimization">06 Advance Optimization</h3>
<p>Given <span class="math inline"><em>θ</em></span>, we have code that
can compute <span class="math inline"><em>J</em>(<em>θ</em>)</span> and
<span class="math inline">$\frac{∂}{∂\theta_j}J{\theta}$</span> <span
class="math inline">(<em>f</em><em>o</em><em>r</em> <em>j</em>=0,1,...,<em>n</em>)</span>.</p>
<p>One is Gradient Descent, while there is other more advanced, more
sophisticated optimization algorithms to implement the computation, such
as <strong><em>Conjugate Gradient</em></strong>,
<strong><em>BFGS</em></strong>, <strong><em>L-BFGS</em></strong>.</p>
<p>Choosing the latter three algorithms has advantages as follows:</p>
<ul>
<li>No need to manually pick <span
class="math inline"><em>α</em></span></li>
<li>Often faster than gradient descent</li>
</ul>
<p>but also disadvantages as follows:</p>
<ul>
<li>More complex</li>
</ul>
<h3 id="multi-class-classification-one-vs-all">07 Multi-class
Classification: One vs all</h3>
<h4 id="multi-class-classification">7.1 Multi-class Classification</h4>
<ul>
<li>Email foldering/tagging: Work, Friends, Family, Hobby</li>
</ul>
<p>Here we have a classification problem with 4 classes, to which we
might assign the numbers (y=1,2,3,4).</p>
<p>Of course there can be more examples like:</p>
<ul>
<li>Medical diagrams: Not ill, Cold, Flue</li>
<li>Weather: Sunny, Cloudy, Rain, Snow</li>
<li>…</li>
</ul>
<p>And so in all of these examples, y can take on a small number of
discrete values. These are multi-class classification problems.</p>
<p><img src="/img/Logistic-Regression-12.png" /></p>
<p>We already know how to do binary classification. Using logistic
regression, we know how to plot a straight line to separate the positive
and negative classes. Using an idea called one-versus-all
classification, we can then take this and make it work for multi-class
classification as well.</p>
<h4 id="one-vs-all">7.2 One vs all</h4>
<p>Train a logistic regression classifier <span
class="math inline"><em>h</em><sub><em>θ</em></sub><sup>(<em>i</em>)</sup>(<em>x</em>)</span>
for each class <span class="math inline"><em>i</em></span> to predict
the probability that <span
class="math inline"><em>y</em> = <em>i</em></span>.</p>
<p>On a new input <span class="math inline"><em>x</em></span>, to make
prediction, pick the class <span class="math inline"><em>i</em></span>
that maximizes <span
class="math inline"><em>m</em><em>a</em><em>x</em><sub><em>i</em></sub><em>h</em><sub><em>θ</em></sub><sup>(<em>i</em>)</sup>(<em>x</em>)</span>.</p>
<p><img src="/img/Logistic-Regression-13.png" /></p>
<h2 id="regularization">§ Regularization</h2>
<h3 id="the-problem-of-overfitting">01 The Problem of Overfitting</h3>
<h4 id="overfitting">1.1 Overfitting</h4>
<p>What is overfitting? Let’s take some examples:</p>
<p><img src="/img/Regularization-01.png" /></p>
<p>In the first figure, the straight line does not fit the training data
very well. We call it “Underfitting” or “has a high bias”.</p>
<p>In the second plot, we could fit a quadratic functions to the data
and we could see that it fits the data pretty well.</p>
<p>At the other extreme would be if we were to fit a fourth order
polynomial to the data and with, we can actually fill a curve that
passes through all five of our training examples. But this is a very
wiggly curve and we don’t think that’s such a good model for predicting
housing prices.</p>
<p>We call this problem “Overfitting”, or say this algorithm has high
variance. </p>
<p>Another example:</p>
<p><img src="/img/Regularization-02.png" /></p>
<h4 id="addressing-overfitting">1.2 Addressing Overfitting</h4>
<p>Options:</p>
<ol type="1">
<li><p>Reduce number of features</p>
<p>- Manually select which features to keep.</p>
<p>- Model selection algorithm (later in course).</p></li>
<li><p>Regularization</p>
<p>- Keep all the features, but reduce magnitude/values of parameters
<span class="math inline"><em>θ</em><sub><em>j</em></sub></span>.</p>
<p>- Works well when we have a lot of features, each of which
contributes a bit to predicting <span
class="math inline"><em>y</em></span>.</p></li>
</ol>
<h3 id="cost-function-2">02 Cost Function</h3>
<p><img src="/img/Regularization-03.png" /></p>
<p>As you can see, the first curve fits the data very well but the
second is overfitting and not generalize well. How can we address
that?</p>
<p>Suppose we penalize and make <span
class="math inline"><em>θ</em><sub>3</sub></span>, <span
class="math inline"><em>θ</em><sub>4</sub></span> really small:</p>
<p><span class="math display">$$
min_\theta\frac{1}{2m}∑^m_{i=1}(h_\theta(x^{(i)})-y^{(i)})^2+n\theta^2_3+n\theta_4^2
$$</span></p>
<p>Our goal is to minimize the cost function and to do so, we must
minimize the <span class="math inline"><em>θ</em><sub>3</sub></span> and
<span class="math inline"><em>θ</em><sub>4</sub></span>, supposing the
<span class="math inline"><em>n</em><sub>1</sub></span> is pretty
big.</p>
<p>If we are to minimize the <span
class="math inline"><em>θ</em><sub>3</sub></span> and <span
class="math inline"><em>θ</em><sub>4</sub></span>, they will end up
close to 0. That’s as if we were getting rid of <span
class="math inline"><em>θ</em><sub>3</sub><em>x</em><sup>3</sup></span>
and <span
class="math inline"><em>θ</em><sub>4</sub><em>x</em><sup>4</sup></span>
these two terms. And if we get rid of these two terms, it’ll end up
still a quadratic function maybe plus tiny contributions from small
terms.</p>
<p>The idea of regularization is assigning small values to parameters
<span
class="math inline"><em>θ</em><sub>0</sub>, <em>θ</em><sub>1</sub>, ..., <em>θ</em><sub><em>n</em></sub></span>.
Thereupon we can obtain a simpler hypothesis less prone to
overfitting.</p>
<p>But the question is, sometimes it’s hard to distinguish which
features are less likely to be relevant. We don’t know which parameters
$ $ to pick to try to shrink.</p>
<p>So what we’re going to do is take our cost function and modify this
cost function by adding a new term to shrink all of our parameters.</p>
<p><span class="math display">$$
J(\theta)=\frac{1}{2m}∑^m_{i=1}(h_\theta(x^{(i)})-y^{(i)})^2+λ∑^n_{j=1}\theta^2_j
$$</span></p>
<p>Just to remind: m is the number of the training data; n is the number
of the features; the $$ is rgularization parameter.</p>
<p>The above function is equal to the following since λ is a
constant:</p>
<p><span class="math display">$$
J(\theta)=\frac{1}{2m}[∑^m_{i=1}(h_\theta(x^{(i)})-y^{(i)})^2+λ∑^n_{j=1}\theta^2_j]
$$</span></p>
<h3 id="regularized-linear-regression">03 Regularized Linear
Regression</h3>
<p><span class="math display">$$
J(\theta)=\frac{1}{2m}[∑^m_{i=1}(h_\theta(x^{(i)})-y^{(i)})^2+λ∑^n_{j=1}\theta^2_j]
$$</span></p>
<p>In the last section, we have introduced regularization to the cost
function and get a new cost function above. In this section, we are
going to generalize regularization to gradient descent and normal
equation.</p>
<h4 id="gradient-descent-2">3.1 Gradient Descent</h4>
<p>The original gradient descent is as follows:</p>
<p><img src="/img/Regularization-04.png" /></p>
<p>Here, as you can see, we separate the <span
class="math inline"><em>θ</em><sub>0</sub></span> from the overall
function since the objects we penalize only include <span
class="math inline"><em>θ</em><sub>1</sub>, <em>θ</em><sub>2</sub>, ..., <em>θ</em><sub>3</sub></span>.</p>
<p>To apply regularization to gradient descent is simple. We just need
to modify the cost function and take its derivate:</p>
<p><span class="math display">$$
\theta_j:=\theta_j(1-\alpha\frac{\lambda}{m})-\alpha\frac{1}{m}∑^m_{i=1}(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)}
$$</span></p>
<h4 id="normal-equation-1">3.2 Normal Equation</h4>
<p>The original idea to use normal equation to minimize the cost
function is through the equation below:</p>
<p><span
class="math display"><em>θ</em> = (<em>X</em><sup><em>T</em></sup><em>X</em>)<sup>−1</sup><em>X</em><sup><em>T</em></sup><em>y</em></span></p>
<p>Concretely, if we are to use regularization, then this formula is to
change as follows:</p>
<p><span class="math display">$$
\theta=(X^TX+\lambda\begin{bmatrix}
0 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\
0 &amp; 1 &amp; 0 &amp; 0 &amp; 0\\
0 &amp; 0 &amp; 1 &amp; 0 &amp; 0\\
0 &amp; 0 &amp; 0 &amp; ... &amp; 0\\
0 &amp; 0 &amp; 0 &amp; 0 &amp; 1
\end{bmatrix})^{-1}X^Ty
$$</span></p>
<p>The shape of the matrix is <span
class="math inline">(<em>n</em>+1) * (<em>n</em>+1)</span>. All of the
elements in the matrix is zero except for the elements on the diagonal
starting from line 2.</p>
<h3 id="regularized-logistic-regression">04 Regularized Logistic
Regression</h3>
<p>The gradient descent in logistic regression is to repeat the
sentences below until find all the parameters <span
class="math inline"><em>θ</em></span>: <span class="math display">$$
\begin{aligned}
&amp;J(θ)=-\frac{1}{m}[∑^m_{(i=1)}y^{(i)}logh_θ(x^{(i)})+(1-y^{(i)})log(1-h_θ(x^{(i)}))]\\
&amp;θ_0:=θ_0-α\frac{1}{m}∑^m_{i=1}(h_θ(x^{(i)})-y^{(i)})x_0^{(i)}\\
&amp;θ_j:=θ_j-α\frac{1}{m}∑^m_{i=1}(h_θ(x^{(i)})-y^{(i)})x_j^{(i)}\\
\end{aligned}
$$</span> Regularization for logistic regression: <span
class="math display">$$
\begin{aligned}
&amp;θ_0:=θ_0-α[\frac{1}{m}∑^m_{i=1}(h_θ(x^{(i)})-y^{(i)})x_0^{(i)}+\frac{λ}{m}θ_0]\\
&amp;θ_j:=θ_j-α[\frac{1}{m}∑^m_{i=1}(h_θ(x^{(i)})-y^{(i)})x_j^{(i)}+\frac{λ}{m}θ_j]
\end{aligned}
$$</span></p>

            </div>
        
        <footer class="article-footer">
        </footer>
    </div>
</article>


    
<nav id="article-nav">
    
        <a href="/2023/03/14/Flask/" id="article-nav-newer" class="article-nav-link-wrap">
            <strong class="article-nav-caption">Newer</strong>
            <div class="article-nav-title">
                
                    Flask入门
                
            </div>
        </a>
    
    
        <a href="/2021/04/05/Welcome/" id="article-nav-older" class="article-nav-link-wrap">
            <strong class="article-nav-caption">Older</strong>
            <div class="article-nav-title">✨Welcome!✨</div>
        </a>
    
</nav>





    
    




<!-- baidu url auto push script -->
<script type="text/javascript">
    !function(){var e=/([http|https]:\/\/[a-zA-Z0-9\_\.]+\.baidu\.com)/gi,r=window.location.href,o=document.referrer;if(!e.test(r)){var n="//api.share.baidu.com/s.gif";o?(n+="?r="+encodeURIComponent(document.referrer),r&&(n+="&l="+r)):r&&(n+="?l="+r);var t=new Image;t.src=n}}(window);
</script>     
</section>
        </div>
        

    
        
<script src="/libs/lightgallery/js/lightgallery.min.js"></script>

        
<script src="/libs/lightgallery/js/lg-thumbnail.min.js"></script>

        
<script src="/libs/lightgallery/js/lg-pager.min.js"></script>

        
<script src="/libs/lightgallery/js/lg-autoplay.min.js"></script>

        
<script src="/libs/lightgallery/js/lg-fullscreen.min.js"></script>

        
<script src="/libs/lightgallery/js/lg-zoom.min.js"></script>

        
<script src="/libs/lightgallery/js/lg-hash.min.js"></script>

        
<script src="/libs/lightgallery/js/lg-share.min.js"></script>

        
<script src="/libs/lightgallery/js/lg-video.min.js"></script>

    
    
        
<script src="/libs/justified-gallery/jquery.justifiedGallery.min.js"></script>

    
    
        <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true,
            TeX: {
                equationNumbers: {
                  autoNumber: 'AMS'
                }
            }
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script async src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    



<!-- Custom Scripts -->

<script src="/js/main.js"></script>


    </div>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</body>
</html>