<!DOCTYPE HTML>
<html>
<head>
  <meta charset="utf-8">
  
  <title>Regularization | Jiahao Peng</title>
  <meta name="author" content="me">
  
  <meta name="description" content="本篇笔记是大一时入门机器学习所写，听的是吴恩达的机器学习入门课程。由于课程是英文课程，所以笔记也是英文的。现在看来当时的写作水平十分生涩。">
  
  
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

  <meta property="og:title" content="Regularization"/>
  <meta property="og:site_name" content="Jiahao Peng"/>

  
    <meta property="og:image" content=""/>
  

  
    <link rel="alternative" href="/atom.xml" title="Jiahao Peng" type="application/atom+xml">
  
  
    <link href="/favicon.png" rel="icon">
  

  <!-- CSS -->
  <link rel="stylesheet" href="/css/themes/lumen.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/font-awesome.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/style.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/responsive.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/highlight.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/highlight-default.min.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/google-fonts.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/comment.css" media="screen" type="text/css">
  <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
	<script src="https://cdnjs.cloudflare.com/ajax/libs/es5-shim/4.5.9/es5-shim.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/es5-shim/4.5.7/es5-sham.min.js"></script>
  <![endif]-->

  <script src="/js/jquery-2.0.3.min.js"></script>
  
  
  <!-- analytics -->
  



<meta name="generator" content="Hexo 7.3.0"></head>

<body>
  <nav id="main-nav" class="navbar navbar-inverse navbar-default navbar-fixed-top" role="navigation">
    <div class="container">
      <button type="button" class="navbar-header navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
	<span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
       <a class="navbar-brand" href="/">Jiahao Peng</a>
      <div class="collapse navbar-collapse nav-menu">
		<ul class="nav navbar-nav">
		  
		  <li>
			<a href="/archives" title="All the articles.">
			  <i class="fa fa-archive"></i>Archives
			</a>
		  </li>
		  
		  <li>
			<a href="/categories" title="All the categories.">
			  <i class="fa fa-folder"></i>Categories
			</a>
		  </li>
		  
		  <li>
			<a href="/tags" title="All the tags.">
			  <i class="fa fa-tags"></i>Tags
			</a>
		  </li>
		  
		  <li>
			<a href="/about" title="About me.">
			  <i class="fa fa-user"></i>About
			</a>
		  </li>
		  
		</ul>
      </div>
    </div> <!-- container -->
</nav>
<div class="clearfix"></div>

  <div class="container">
    <div class="content">
      


	
		<div class="page-header page-header-inverse ">		
			<h1 class="title title-inverse "> Regularization</h1>
		</div>		
	






<div class="row post">
	<!-- cols -->
	
	<div id="top_meta"></div>
	<div class="col-md-9">
	

	<!-- content -->
	<div class="mypage">		
	  
		 <div class="alert alert-success description">
			<i class="fa fa-info-circle"></i> <p>本篇笔记是大一时入门机器学习所写，听的是吴恩达的机器学习入门课程。由于课程是英文课程，所以笔记也是英文的。现在看来当时的写作水平十分生涩。</p>
			
		 </div> <!-- alert -->
	  		

	  <h2 id="the-problem-of-overfitting">01 The Problem of Overfitting</h2>
<h3 id="overfitting">1.1 Overfitting</h3>
<p>What is overfitting? Let’s take some examples:</p>
<p><img src="/img/Regularization-01.png" /></p>
<p>In the first figure, the straight line does not fit the training data
very well. We call it “Underfitting” or “has a high bias”.</p>
<p>In the second plot, we could fit a quadratic functions to the data
and we could see that it fits the data pretty well.</p>
<span id="more"></span>
<p>At the other extreme would be if we were to fit a fourth order
polynomial to the data and with, we can actually fill a curve that
passes through all five of our training examples. But this is a very
wiggly curve and we don’t think that’s such a good model for predicting
housing prices.</p>
<p>We call this problem “Overfitting”, or say this algorithm has high
variance. </p>
<p>Another example:</p>
<p><img src="/img/Regularization-02.png" /></p>
<h3 id="addressing-overfitting">1.2 Addressing Overfitting</h3>
<p>Options:</p>
<ol type="1">
<li><p>Reduce number of features</p>
<p>- Manually select which features to keep.</p>
<p>- Model selection algorithm (later in course).</p></li>
<li><p>Regularization</p>
<p>- Keep all the features, but reduce magnitude/values of parameters
<span class="math inline"><em>θ</em><sub><em>j</em></sub></span>.</p>
<p>- Works well when we have a lot of features, each of which
contributes a bit to predicting <span
class="math inline"><em>y</em></span>.</p></li>
</ol>
<h2 id="cost-function">02 Cost Function</h2>
<p><img src="/img/Regularization-03.png" /></p>
<p>As you can see, the first curve fits the data very well but the
second is overfitting and not generalize well. How can we address
that?</p>
<p>Suppose we penalize and make <span
class="math inline"><em>θ</em><sub>3</sub></span>, <span
class="math inline"><em>θ</em><sub>4</sub></span> really small:</p>
<p><span class="math display">$$
min_\theta\frac{1}{2m}∑^m_{i=1}(h_\theta(x^{(i)})-y^{(i)})^2+n\theta^2_3+n\theta_4^2
$$</span></p>
<p>Our goal is to minimize the cost function and to do so, we must
minimize the <span class="math inline"><em>θ</em><sub>3</sub></span> and
<span class="math inline"><em>θ</em><sub>4</sub></span>, supposing the
<span class="math inline"><em>n</em><sub>1</sub></span> is pretty
big.</p>
<p>If we are to minimize the <span
class="math inline"><em>θ</em><sub>3</sub></span> and <span
class="math inline"><em>θ</em><sub>4</sub></span>, they will end up
close to 0. That’s as if we were getting rid of <span
class="math inline"><em>θ</em><sub>3</sub><em>x</em><sup>3</sup></span>
and <span
class="math inline"><em>θ</em><sub>4</sub><em>x</em><sup>4</sup></span>
these two terms. And if we get rid of these two terms, it’ll end up
still a quadratic function maybe plus tiny contributions from small
terms.</p>
<p>The idea of regularization is assigning small values to parameters
<span
class="math inline"><em>θ</em><sub>0</sub>, <em>θ</em><sub>1</sub>, ..., <em>θ</em><sub><em>n</em></sub></span>.
Thereupon we can obtain a simpler hypothesis less prone to
overfitting.</p>
<p>But the question is, sometimes it’s hard to distinguish which
features are less likely to be relevant. We don’t know which parameters
$ $ to pick to try to shrink.</p>
<p>So what we’re going to do is take our cost function and modify this
cost function by adding a new term to shrink all of our parameters.</p>
<p><span class="math display">$$
J(\theta)=\frac{1}{2m}∑^m_{i=1}(h_\theta(x^{(i)})-y^{(i)})^2+λ∑^n_{j=1}\theta^2_j
$$</span></p>
<p>Just to remind: m is the number of the training data; n is the number
of the features; the $$ is rgularization parameter.</p>
<p>The above function is equal to the following since λ is a
constant:</p>
<p><span class="math display">$$
J(\theta)=\frac{1}{2m}[∑^m_{i=1}(h_\theta(x^{(i)})-y^{(i)})^2+λ∑^n_{j=1}\theta^2_j]
$$</span></p>
<h2 id="regularized-linear-regression">03 Regularized Linear
Regression</h2>
<p><span class="math display">$$
J(\theta)=\frac{1}{2m}[∑^m_{i=1}(h_\theta(x^{(i)})-y^{(i)})^2+λ∑^n_{j=1}\theta^2_j]
$$</span></p>
<p>In the last section, we have introduced regularization to the cost
function and get a new cost function above. In this section, we are
going to generalize regularization to gradient descent and normal
equation.</p>
<h3 id="gradient-descent">3.1 Gradient Descent</h3>
<p>The original gradient descent is as follows:</p>
<p><img src="/img/Regularization-04.png" /></p>
<p>Here, as you can see, we separate the <span
class="math inline"><em>θ</em><sub>0</sub></span> from the overall
function since the objects we penalize only include <span
class="math inline"><em>θ</em><sub>1</sub>, <em>θ</em><sub>2</sub>, ..., <em>θ</em><sub>3</sub></span>.</p>
<p>To apply regularization to gradient descent is simple. We just need
to modify the cost function and take its derivate:</p>
<p><span class="math display">$$
\theta_j:=\theta_j(1-\alpha\frac{\lambda}{m})-\alpha\frac{1}{m}∑^m_{i=1}(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)}
$$</span></p>
<h3 id="normal-equation">3.2 Normal Equation</h3>
<p>The original idea to use normal equation to minimize the cost
function is through the equation below:</p>
<p><span
class="math display"><em>θ</em> = (<em>X</em><sup><em>T</em></sup><em>X</em>)<sup>−1</sup><em>X</em><sup><em>T</em></sup><em>y</em></span></p>
<p>Concretely, if we are to use regularization, then this formula is to
change as follows:</p>
<p><span class="math display">$$
\theta=(X^TX+\lambda\begin{bmatrix}
0 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\
0 &amp; 1 &amp; 0 &amp; 0 &amp; 0\\
0 &amp; 0 &amp; 1 &amp; 0 &amp; 0\\
0 &amp; 0 &amp; 0 &amp; ... &amp; 0\\
0 &amp; 0 &amp; 0 &amp; 0 &amp; 1
\end{bmatrix})^{-1}X^Ty
$$</span></p>
<p>The shape of the matrix is <span
class="math inline">(<em>n</em>+1) * (<em>n</em>+1)</span>. All of the
elements in the matrix is zero except for the elements on the diagonal
starting from line 2.</p>
<h2 id="regularized-logistic-regression">04 Regularized Logistic
Regression</h2>
<p>The gradient descent in logistic regression is to repeat the
sentences below until find all the parameters <span
class="math inline"><em>θ</em></span>: <span class="math display">$$
\begin{aligned}
&amp;J(θ)=-\frac{1}{m}[∑^m_{(i=1)}y^{(i)}logh_θ(x^{(i)})+(1-y^{(i)})log(1-h_θ(x^{(i)}))]\\
&amp;θ_0:=θ_0-α\frac{1}{m}∑^m_{i=1}(h_θ(x^{(i)})-y^{(i)})x_0^{(i)}\\
&amp;θ_j:=θ_j-α\frac{1}{m}∑^m_{i=1}(h_θ(x^{(i)})-y^{(i)})x_j^{(i)}\\
\end{aligned}
$$</span> Regularization for logistic regression: <span
class="math display">$$
\begin{aligned}
&amp;θ_0:=θ_0-α[\frac{1}{m}∑^m_{i=1}(h_θ(x^{(i)})-y^{(i)})x_0^{(i)}+\frac{λ}{m}θ_0]\\
&amp;θ_j:=θ_j-α[\frac{1}{m}∑^m_{i=1}(h_θ(x^{(i)})-y^{(i)})x_j^{(i)}+\frac{λ}{m}θ_j]
\end{aligned}
$$</span></p>
	  
	</div>

	<!-- recommended posts -->
	

	<div>
  	<center>
	<div class="pagination">
<ul class="pagination">
	 
				
		<li class="prev"><a href="/2023/09/09/%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6-Self-attention/" class="alignleft prev"><i class="fa fa-arrow-circle-o-left"></i>Prev</a></li>
  		

        <li><a href="/archives"><i class="fa fa-archive"></i>Archive</a></li>

		
          <li class="next"><a href="/2022/07/23/Logistic-Regression/" class="alignright next">Next<i class="fa fa-arrow-circle-o-right"></i></a></li>
        
	
</ul>
</div>

    </center>
	</div>

    <!-- share -->
    
        
    <div class="bdsharebuttonbox">
        <a href="#" class="bds_more" data-cmd="more"></a>
        <a href="#" class="bds_weixin" data-cmd="weixin" title="分享到微信"></a>
        <a href="#" class="bds_tsina" data-cmd="tsina" title="分享到新浪微博"></a>
        <a href="#" class="bds_fbook" data-cmd="fbook" title="分享到Facebook"></a>
        <a href="#" class="bds_twi" data-cmd="twi" title="分享到Twitter"></a>
        <a href="#" class="bds_linkedin" data-cmd="linkedin" title="分享到linkedin"></a>
        <a href="#" class="bds_evernotecn" data-cmd="evernotecn" title="分享到印象笔记"></a>
        <a href="#" class="bds_youdao" data-cmd="youdao" title="分享到有道云笔记"></a>
        <a href="#" class="bds_copy" data-cmd="copy" title="分享到复制网址"></a>
    </div>
    <script>
        window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"2","bdMiniList":false,"bdPic":"","bdStyle":"1","bdSize":"24"},"share":{}};
        with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];
    </script>


        

    
	
	<!-- comment -->
	
<section id="comment">
  <h2 class="title">Comments</h2>
  
</section>


	</div> <!-- col-md-9/col-md-12 -->
		
	
	<div id="side_meta">
		<div class="col-md-3" id="post_meta"> 

	<!-- date -->
	
	<div class="meta-widget">
	<i class="fa fa-clock-o"></i>
	2022-07-24 
	</div>
	

	<!-- categories -->
    
	<div class="meta-widget">
	<a data-toggle="collapse" data-target="#categorys"><i class="fa fa-folder"></i></a>	
    <ul id="categorys" class="tag_box list-unstyled collapse in">
          
  <li>
    <li><a href="/categories/Machine-Learning-Wu/">Machine Learning - Wu<span>4</span></a></li>
  </li>

    </ul>
	</div>
	

	<!-- tags -->
	
	<div class="meta-widget">
	<a data-toggle="collapse" data-target="#tags"><i class="fa fa-tags"></i></a>		  
    <ul id="tags" class="tag_box list-unstyled collapse in">	  
	    
  <li><a href="/tags/lecture-notes/">lecture notes<span>7</span></a></li>

    </ul>
	</div>
	

	<!-- toc -->
	<div class="meta-widget">
	
	   <a data-toggle="collapse" data-target="#toc"><i class="fa fa-bars"></i></a>
	   <div id="toc" class="toc collapse in">
			<ol class="toc-article"><li class="toc-article-item toc-article-level-2"><a class="toc-article-link" href="#the-problem-of-overfitting"><span class="toc-article-text">01 The Problem of Overfitting</span></a><ol class="toc-article-child"><li class="toc-article-item toc-article-level-3"><a class="toc-article-link" href="#overfitting"><span class="toc-article-text">1.1 Overfitting</span></a></li><li class="toc-article-item toc-article-level-3"><a class="toc-article-link" href="#addressing-overfitting"><span class="toc-article-text">1.2 Addressing Overfitting</span></a></li></ol></li><li class="toc-article-item toc-article-level-2"><a class="toc-article-link" href="#cost-function"><span class="toc-article-text">02 Cost Function</span></a></li><li class="toc-article-item toc-article-level-2"><a class="toc-article-link" href="#regularized-linear-regression"><span class="toc-article-text">03 Regularized Linear
Regression</span></a><ol class="toc-article-child"><li class="toc-article-item toc-article-level-3"><a class="toc-article-link" href="#gradient-descent"><span class="toc-article-text">3.1 Gradient Descent</span></a></li><li class="toc-article-item toc-article-level-3"><a class="toc-article-link" href="#normal-equation"><span class="toc-article-text">3.2 Normal Equation</span></a></li></ol></li><li class="toc-article-item toc-article-level-2"><a class="toc-article-link" href="#regularized-logistic-regression"><span class="toc-article-text">04 Regularized Logistic
Regression</span></a></li></ol>
		</div>
	
	</div>
	
    <hr>
	
</div><!-- col-md-3 -->

	</div>
		

</div><!-- row -->




    </div>
  </div>
  <div class="container-narrow">
    <footer> <!--<p>
  &copy; 2025 me
  
      with help from <a href="http://hexo.io/" target="_blank">Hexo</a> and <a href="http://getbootstrap.com/" target="_blank">Twitter Bootstrap</a>. Theme by <a target="_blank" rel="noopener" href="http://github.com/wzpan/hexo-theme-freemind/">Freemind</a>.    
</p>-->
 </footer>
  </div> <!-- container-narrow -->
  


  
<a id="gotop" href="#">   
  <span>▲</span> 
</a>

<script src="/js/jquery.imagesloaded.min.js"></script>
<script src="/js/gallery.js"></script>
<script src="/js/bootstrap.min.js"></script>
<script src="/js/main.js"></script>
<script src="/js/search.js"></script>


<link rel="stylesheet" href="/fancybox/jquery.fancybox.css" media="screen" type="text/css">
<script src="/fancybox/jquery.fancybox.pack.js"></script>
<script type="text/javascript">
(function($){
  $('.fancybox').fancybox();
})(jQuery);
</script>



   <script type="text/javascript">      
     var search_path = "search.xml";
	 if (search_path.length == 0) {
	 	search_path = "search.xml";
	 }
	 var path = "/" + search_path;
     searchFunc(path, 'local-search-input', 'local-search-result');
   </script>


<!-- syntax highlighting -->


<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</body>
	<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</html>