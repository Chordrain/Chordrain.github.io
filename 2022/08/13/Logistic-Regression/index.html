<!DOCTYPE HTML>
<html>
<head>
  <meta charset="utf-8">
  
  <title>Logistic Regression | Jiahao Peng</title>
  <meta name="author" content="me">
  
  <meta name="description" content="本篇笔记是大一时入门机器学习所写，听的是吴恩达的机器学习入门课程。由于课程是英文课程，所以笔记也是英文的。现在看来当时的写作水平十分生涩。">
  
  
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

  <meta property="og:title" content="Logistic Regression"/>
  <meta property="og:site_name" content="Jiahao Peng"/>

  
    <meta property="og:image" content=""/>
  

  
    <link rel="alternative" href="/atom.xml" title="Jiahao Peng" type="application/atom+xml">
  
  
    <link href="/favicon.png" rel="icon">
  

  <!-- CSS -->
  <link rel="stylesheet" href="/css/themes/lumen.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/font-awesome.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/style.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/responsive.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/highlight.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/highlight-default.min.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/google-fonts.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/comment.css" media="screen" type="text/css">
  <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
	<script src="https://cdnjs.cloudflare.com/ajax/libs/es5-shim/4.5.9/es5-shim.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/es5-shim/4.5.7/es5-sham.min.js"></script>
  <![endif]-->

  <script src="/js/jquery-2.0.3.min.js"></script>
  
  
  <!-- analytics -->
  



<meta name="generator" content="Hexo 7.3.0"></head>

<body>
  <nav id="main-nav" class="navbar navbar-inverse navbar-default navbar-fixed-top" role="navigation">
    <div class="container">
      <button type="button" class="navbar-header navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
	<span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
       <a class="navbar-brand" href="/">Jiahao Peng</a>
      <div class="collapse navbar-collapse nav-menu">
		<ul class="nav navbar-nav">
		  
		  <li>
			<a href="/archives" title="All the articles.">
			  <i class="fa fa-archive"></i>Archives
			</a>
		  </li>
		  
		  <li>
			<a href="/categories" title="All the categories.">
			  <i class="fa fa-folder"></i>Categories
			</a>
		  </li>
		  
		  <li>
			<a href="/tags" title="All the tags.">
			  <i class="fa fa-tags"></i>Tags
			</a>
		  </li>
		  
		  <li>
			<a href="/about" title="About me.">
			  <i class="fa fa-user"></i>About
			</a>
		  </li>
		  
		</ul>
      </div>
    </div> <!-- container -->
</nav>
<div class="clearfix"></div>

  <div class="container">
    <div class="content">
      


	
		<div class="page-header page-header-inverse ">		
			<h1 class="title title-inverse "> Logistic Regression</h1>
		</div>		
	






<div class="row post">
	<!-- cols -->
	
	<div id="top_meta"></div>
	<div class="col-md-9">
	

	<!-- content -->
	<div class="mypage">		
	  
		 <div class="alert alert-success description">
			<i class="fa fa-info-circle"></i> <p>本篇笔记是大一时入门机器学习所写，听的是吴恩达的机器学习入门课程。由于课程是英文课程，所以笔记也是英文的。现在看来当时的写作水平十分生涩。</p>
			
		 </div> <!-- alert -->
	  		

	  <h2 id="introduction">01 Introduction</h2>
<p>Linear Regression is a good tool to help solve the regression
problem, which predicts continuous valued output. However, it won’t be a
good idea if we apply it to the classification problem, which gives
discrete valued output. That’s because linear regression can be
susceptible to outlier data, rendering the result inaccurate.</p>
<p>So here we introduce a new method to cope with the classification
problem — <strong>Logistic Regression</strong>.</p>
<span id="more"></span>
<h2 id="hypotheses-representation">02 Hypotheses Representation</h2>
<h3 id="logistic-regression-model">2.1 Logistic Regression Model</h3>
<p>The value of the hypothesis function in classification problem should
be between 0 ~ 1: <span
class="math inline">0 ≤ <em>h</em><sub><em>θ</em></sub>(<em>x</em>) ≤ 1</span></p>
<p>As the previous sections have shown, <span
class="math inline"><em>h</em><sub><em>θ</em></sub>(<em>x</em>) = <em>θ</em><sup><em>T</em></sup><em>x</em></span>.
Now we are going to make the hypothesis equal <span
class="math inline"><em>g</em>(<em>θ</em><sup><em>T</em></sup><em>x</em>)</span>,
where we define the function <span class="math inline"><em>g</em></span>
as follows:</p>
<p><span class="math display">$$
g(z)=\frac{1}{1+e^{(-z)}}
$$</span></p>
<p>This function is what we called <span
class="math inline"><em>S</em><em>i</em><em>g</em><em>m</em><em>o</em><em>i</em><em>d</em> <em>F</em><em>u</em><em>n</em><em>t</em><em>i</em><em>o</em><em>n</em></span>
or <span
class="math inline"><em>L</em><em>o</em><em>g</em><em>i</em><em>s</em><em>t</em><em>i</em><em>c</em> <em>F</em><em>u</em><em>n</em><em>c</em><em>t</em><em>i</em><em>o</em><em>n</em></span>,
where <span
class="math inline"><em>L</em><em>o</em><em>g</em><em>i</em><em>s</em><em>t</em><em>i</em><em>c</em> <em>R</em><em>e</em><em>g</em><em>r</em><em>e</em><em>s</em><em>s</em><em>i</em><em>o</em><em>n</em></span>
gains its name.</p>
<p>Let’s plug the parameter back into it. Then there is an alternative
way writing out the form of our hypothesis:</p>
<p><span class="math display">$$
h_\theta(x)=\frac{1}{1+e^{-\theta^Tx}}
$$</span></p>
<p>Its curve looks like:</p>
<p><img src="/img/Logistic-Regression-01.png" /></p>
<p>As you can see, the sigmoid function approaches 1 as its parameter
<span class="math inline"><em>z</em></span> approaches infinity, and
approaches 0 as its parameter approaches infinitesimal.</p>
<h3 id="interpretation-of-hypothesis-output">2.2 Interpretation of
Hypothesis Output</h3>
<p><span class="math display">$$
h_\theta(x)=\text{estimated\;probability\;of\;}y=1\;\text{on\;input}\;x
$$</span></p>
Example: If $x=
=
<p>$, <span
class="math inline"><em>h</em><sub><em>θ</em></sub>(<em>x</em>) = 0.7</span></p>
<p>Tell patient that 70% chance of tumor being malignant.</p>
<h2 id="decision-boundary">03 Decision Boundary</h2>
<h3 id="recap-logistic-regression">3.1 Recap: Logistic Regression</h3>
<p><img src="/img/Logistic-Regression-02.png" /></p>
<p>Viewing the plot of our hypothesis function, we can easily find that
its value is greater than or equal to 0.5 whenever the parameter is
positive, while it’s less than 0.5 whenever the parameter is
negative.</p>
<h3 id="decision-boundary-1">3.2 Decision Boundary</h3>
<p>Suppose we have a dataset and hypothesis function like:</p>
<p><img src="/img/Logistic-Regression-03.png" /></p>
<p>Concretely, we define the parameters <span
class="math inline"><em>θ</em></span> to be:</p>
<p><span class="math display">$$
\theta=\begin{bmatrix}
-3\\
1\\
1\\
\end{bmatrix}
$$</span></p>
<p>Now let’s figure out where a hypothesis will end up predicting <span
class="math inline"><em>y</em> = 1</span> and where it will end up
predicting <span class="math inline"><em>y</em> = 0</span>.</p>
<p>Using the conclusion we’ve drawn in the recap, we know that <span
class="math inline"><em>y</em></span> is more likely equal to 1, that is
the probability that y equals 1 is greater than or equal to 0.5,
whenever <span
class="math inline"><em>θ</em><sup><em>T</em></sup><em>x</em></span> is
greater then zero:</p>
<p><span
class="math display">predict  <em>y</em> = 1  if   − 3 + <em>x</em><sub>1</sub> + <em>x</em><sub>2</sub> &gt;  = 0</span></p>
<p>That’s equivalent to <span
class="math inline"><em>x</em><sub>1</sub> + <em>x</em><sub>2</sub> ≥ 3</span>.
So if we draw a plot of <span
class="math inline"><em>x</em><sub>1</sub> + <em>x</em><sub>2</sub> = 3</span>,
it’s basically like:</p>
<p><img src="/img/Logistic-Regression-04.png" /></p>
<p>And we call that line <span
class="math inline"><em>D</em><em>e</em><em>c</em><em>i</em><em>s</em><em>i</em><em>o</em><em>n</em> <em>B</em><em>o</em><em>u</em><em>n</em><em>d</em><em>a</em><em>r</em><em>y</em></span>.
And the <span
class="math inline"><em>x</em><sub>1</sub>, <em>x</em><sub>2</sub></span>
above the line corrspond with the <span
class="math inline"><em>y</em></span> equal to 1, while in contrast, the
<span
class="math inline"><em>x</em><sub>1</sub>, <em>x</em><sub>2</sub></span>
fall below the line correspond with the <span
class="math inline"><em>y</em></span> euqal to 0.</p>
<p>Just to be clear, the decision boundary is a property of the
hypothesis including the parameters <span
class="math inline"><em>θ</em></span> rather than the dataset, which
means even if we change all the data, as long as the hypothesis remains
the same, the boundary remains the same.</p>
<h3 id="non-linear-decision-boundaries">3.3 Non-linear decision
boundaries</h3>
<p><img src="/img/Logistic-Regression-05.png" /></p>
<p>In the above case, we define the parameters <span
class="math inline"><em>θ</em></span> to be <span
class="math inline">[−1,0,0,1,1]</span>. And still we can draw a line to
separate thoses <span class="math inline"><em>x</em></span> but clearly
it’s more complicated than just a simple line. The decision boundary can
be much more complex especially when it comes to higher order polynomial
terms. But no matter how complex it is, it’s still a property of the
hypothesis instead of the training set.</p>
<h2 id="cost-function">04 Cost Function</h2>
<h3 id="why-the-former-wont-work">4.1 Why the Former Won’t Work</h3>
<p><img src="/img/Logistic-Regression-06.png" /></p>
<p>Back when we were developing a linear regression model, we use the
following cost function:</p>
<p><span class="math display">$$
J(\theta)=\frac{1}{m}∑^m_{i=1}\frac{1}{2}(h_θ(x^{(i)})-y^{(i)})^2
$$</span></p>
<p>This time, we are going so change it a little bit. Let’s define</p>
<p><span class="math display">$$
Cost(h_θ(x^{(i)}),y^{(i)})=\frac{1}{2}(h_θ(x^{(i)})-y^{(i)})^2
$$</span></p>
<p>It seems like this cost function that works for linear regression can
still be applied to logistic regression, but however, it’s not quite
appropriate, because if we plug <span
class="math inline">$h_\theta(x)=\frac{1}{1+e^{-\theta^Tx}}$</span>in
the function, you will find that the function is not convex.</p>
<p><img src="/img/Logistic-Regression-07.jpg" /></p>
<p>Usually, when a function is convex, its plot is bowl-shaped, which is
easy to spot the global minimum. However when it’s non-convex, there
will be many local optimums, making it difficult for gradient descent to
converge to the global minimum.</p>
<p>So we are going to choose another function better suitable for
logistic regression.</p>
<h3 id="logistic-regression-cost-function">4.2 Logistic Regression Cost
Function</h3>
<p>So we give the cost function for logistic regression: <span
class="math display">$$
Cost(h_θ(x),y)=\begin{cases}
-log(h_θ(x)) &amp; if\;y=1\\
-log(1-h_θ(x)) &amp; if\;y=0\\
\end{cases}
$$</span></p>
<p>Its curve be like:</p>
<p><img src="/img/Logistic-Regression-08.png" /></p>
<p><img src="/img/Logistic-Regression-09.png" /></p>
<h2 id="simplified-cost-function-and-gradient-descent">05 Simplified
Cost Function and Gradient Descent</h2>
<h3 id="simplified-cost-function">5.1 Simplified Cost Function</h3>
<p><img src="/img/Logistic-Regression-10.png" /></p>
<p>Because <span class="math inline"><em>y</em></span> is either 0 or 1,
we’ll be able to come up with a simpler way to write the cost function.
Rather than write this function in separated two lines, we are going to
take these two lines and compress them into one equation, which will
make it convenient for us to write the cost function and derive gradient
descent.</p>
<p>Concretely, we can write out the cost function as follows:</p>
<p><span
class="math display"><em>C</em><em>o</em><em>s</em><em>t</em>(<em>h</em><sub><em>θ</em></sub>(<em>x</em>),<em>y</em>) =  − <em>y</em><em>l</em><em>o</em><em>g</em>(<em>h</em><sub><em>θ</em></sub>(<em>x</em>)) − (1−<em>y</em>)<em>l</em><em>o</em><em>g</em>(1−<em>h</em><sub><em>θ</em></sub>(<em>x</em>))</span></p>
<p>It’s easy to figure out why does this equation equal the original
one.</p>
<p>As we know, <span class="math inline"><em>y</em></span> is either 1
or 0.</p>
<p>If <span class="math inline"><em>y</em> = 1</span>, this equation
will be <span
class="math inline"> − <em>l</em><em>o</em><em>g</em>(<em>h</em><sub><em>θ</em></sub>(<em>x</em>))</span>.</p>
<p>If <span class="math inline"><em>y</em> = 0</span>, this equation
will be <span
class="math inline"> − <em>l</em><em>o</em><em>g</em>(1−<em>h</em><sub><em>θ</em></sub>(<em>x</em>))</span>.</p>
<p>This is exactly the same as the original equation.</p>
<p>The cost function:</p>
<p><span class="math display">$$
J(θ)=\frac{1}{m}∑^m_{i=1}Cost(h_θ(x^{(i)}),y^{(i)})\\=-\frac{1}{m}[∑^m_{(i=1)}y^{(i)}logh_θ(x^{(i)})+(1-y^{(i)})log(1-h_θ(x^{(i)}))]
$$</span></p>
<h3 id="gradient-descent">5.2 Gradient Descent</h3>
<p>Now we have got the new cost function. The next step is to fit
parameters <span class="math inline"><em>θ</em></span> and what we are
going to fit parameters <span class="math inline"><em>θ</em></span> is
try to find the parameters θ that minimizes <span
class="math inline"><em>J</em>(<em>θ</em>)</span>.</p>
<p><img src="/img/Logistic-Regression-11.png" /></p>
<h2 id="advance-optimization">06 Advance Optimization</h2>
<p>Given <span class="math inline"><em>θ</em></span>, we have code that
can compute <span class="math inline"><em>J</em>(<em>θ</em>)</span> and
<span class="math inline">$\frac{∂}{∂\theta_j}J{\theta}$</span> <span
class="math inline">(<em>f</em><em>o</em><em>r</em> <em>j</em>=0,1,...,<em>n</em>)</span>.</p>
<p>One is Gradient Descent, while there is other more advanced, more
sophisticated optimization algorithms to implement the computation, such
as <strong><em>Conjugate Gradient</em></strong>,
<strong><em>BFGS</em></strong>, <strong><em>L-BFGS</em></strong>.</p>
<p>Choosing the latter three algorithms has advantages as follows:</p>
<ul>
<li>No need to manually pick <span
class="math inline"><em>α</em></span></li>
<li>Often faster than gradient descent</li>
</ul>
<p>but also disadvantages as follows:</p>
<ul>
<li>More complex</li>
</ul>
<h2 id="multi-class-classification-one-vs-all">07 Multi-class
Classification: One vs all</h2>
<h3 id="multi-class-classification">7.1 Multi-class Classification</h3>
<ul>
<li>Email foldering/tagging: Work, Friends, Family, Hobby</li>
</ul>
<p>Here we have a classification problem with 4 classes, to which we
might assign the numbers (y=1,2,3,4).</p>
<p>Of course there can be more examples like:</p>
<ul>
<li>Medical diagrams: Not ill, Cold, Flue</li>
<li>Weather: Sunny, Cloudy, Rain, Snow</li>
<li>…</li>
</ul>
<p>And so in all of these examples, y can take on a small number of
discrete values. These are multi-class classification problems.</p>
<p><img src="/img/Logistic-Regression-12.png" /></p>
<p>We already know how to do binary classification. Using logistic
regression, we know how to plot a straight line to separate the positive
and negative classes. Using an idea called one-versus-all
classification, we can then take this and make it work for multi-class
classification as well.</p>
<h3 id="one-vs-all">7.2 One vs all</h3>
<p>Train a logistic regression classifier <span
class="math inline"><em>h</em><sub><em>θ</em></sub><sup>(<em>i</em>)</sup>(<em>x</em>)</span>
for each class <span class="math inline"><em>i</em></span> to predict
the probability that <span
class="math inline"><em>y</em> = <em>i</em></span>.</p>
<p>On a new input <span class="math inline"><em>x</em></span>, to make
prediction, pick the class <span class="math inline"><em>i</em></span>
that maximizes <span
class="math inline"><em>m</em><em>a</em><em>x</em><sub><em>i</em></sub><em>h</em><sub><em>θ</em></sub><sup>(<em>i</em>)</sup>(<em>x</em>)</span>.</p>
<p><img src="/img/Logistic-Regression-13.png" /></p>
	  
	</div>

	<!-- recommended posts -->
	

	<div>
  	<center>
	<div class="pagination">
<ul class="pagination">
	 
				
		<li class="prev"><a href="/2023/09/09/%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6-Self-attention/" class="alignleft prev"><i class="fa fa-arrow-circle-o-left"></i>Prev</a></li>
  		

        <li><a href="/archives"><i class="fa fa-archive"></i>Archive</a></li>

		
          <li class="next"><a href="/2022/07/21/Linear-Regression-with-Multiple-Variables/" class="alignright next">Next<i class="fa fa-arrow-circle-o-right"></i></a></li>
        
	
</ul>
</div>

    </center>
	</div>

    <!-- share -->
    
        
    <div class="bdsharebuttonbox">
        <a href="#" class="bds_more" data-cmd="more"></a>
        <a href="#" class="bds_weixin" data-cmd="weixin" title="分享到微信"></a>
        <a href="#" class="bds_tsina" data-cmd="tsina" title="分享到新浪微博"></a>
        <a href="#" class="bds_fbook" data-cmd="fbook" title="分享到Facebook"></a>
        <a href="#" class="bds_twi" data-cmd="twi" title="分享到Twitter"></a>
        <a href="#" class="bds_linkedin" data-cmd="linkedin" title="分享到linkedin"></a>
        <a href="#" class="bds_evernotecn" data-cmd="evernotecn" title="分享到印象笔记"></a>
        <a href="#" class="bds_youdao" data-cmd="youdao" title="分享到有道云笔记"></a>
        <a href="#" class="bds_copy" data-cmd="copy" title="分享到复制网址"></a>
    </div>
    <script>
        window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"2","bdMiniList":false,"bdPic":"","bdStyle":"1","bdSize":"24"},"share":{}};
        with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];
    </script>


        

    
	
	<!-- comment -->
	
<section id="comment">
  <h2 class="title">Comments</h2>
  
</section>


	</div> <!-- col-md-9/col-md-12 -->
		
	
	<div id="side_meta">
		<div class="col-md-3" id="post_meta"> 

	<!-- date -->
	
	<div class="meta-widget">
	<i class="fa fa-clock-o"></i>
	2022-08-13 
	</div>
	

	<!-- categories -->
    
	<div class="meta-widget">
	<a data-toggle="collapse" data-target="#categorys"><i class="fa fa-folder"></i></a>	
    <ul id="categorys" class="tag_box list-unstyled collapse in">
          
  <li>
    <li><a href="/categories/Machine-Learning-Wu/">Machine Learning - Wu<span>3</span></a></li>
  </li>

    </ul>
	</div>
	

	<!-- tags -->
	
	<div class="meta-widget">
	<a data-toggle="collapse" data-target="#tags"><i class="fa fa-tags"></i></a>		  
    <ul id="tags" class="tag_box list-unstyled collapse in">	  
	    
  <li><a href="/tags/lecture-notes/">lecture notes<span>6</span></a></li>

    </ul>
	</div>
	

	<!-- toc -->
	<div class="meta-widget">
	
	   <a data-toggle="collapse" data-target="#toc"><i class="fa fa-bars"></i></a>
	   <div id="toc" class="toc collapse in">
			<ol class="toc-article"><li class="toc-article-item toc-article-level-2"><a class="toc-article-link" href="#introduction"><span class="toc-article-text">01 Introduction</span></a></li><li class="toc-article-item toc-article-level-2"><a class="toc-article-link" href="#hypotheses-representation"><span class="toc-article-text">02 Hypotheses Representation</span></a><ol class="toc-article-child"><li class="toc-article-item toc-article-level-3"><a class="toc-article-link" href="#logistic-regression-model"><span class="toc-article-text">2.1 Logistic Regression Model</span></a></li><li class="toc-article-item toc-article-level-3"><a class="toc-article-link" href="#interpretation-of-hypothesis-output"><span class="toc-article-text">2.2 Interpretation of
Hypothesis Output</span></a></li></ol></li><li class="toc-article-item toc-article-level-2"><a class="toc-article-link" href="#decision-boundary"><span class="toc-article-text">03 Decision Boundary</span></a><ol class="toc-article-child"><li class="toc-article-item toc-article-level-3"><a class="toc-article-link" href="#recap-logistic-regression"><span class="toc-article-text">3.1 Recap: Logistic Regression</span></a></li><li class="toc-article-item toc-article-level-3"><a class="toc-article-link" href="#decision-boundary-1"><span class="toc-article-text">3.2 Decision Boundary</span></a></li><li class="toc-article-item toc-article-level-3"><a class="toc-article-link" href="#non-linear-decision-boundaries"><span class="toc-article-text">3.3 Non-linear decision
boundaries</span></a></li></ol></li><li class="toc-article-item toc-article-level-2"><a class="toc-article-link" href="#cost-function"><span class="toc-article-text">04 Cost Function</span></a><ol class="toc-article-child"><li class="toc-article-item toc-article-level-3"><a class="toc-article-link" href="#why-the-former-wont-work"><span class="toc-article-text">4.1 Why the Former Won’t Work</span></a></li><li class="toc-article-item toc-article-level-3"><a class="toc-article-link" href="#logistic-regression-cost-function"><span class="toc-article-text">4.2 Logistic Regression Cost
Function</span></a></li></ol></li><li class="toc-article-item toc-article-level-2"><a class="toc-article-link" href="#simplified-cost-function-and-gradient-descent"><span class="toc-article-text">05 Simplified
Cost Function and Gradient Descent</span></a><ol class="toc-article-child"><li class="toc-article-item toc-article-level-3"><a class="toc-article-link" href="#simplified-cost-function"><span class="toc-article-text">5.1 Simplified Cost Function</span></a></li><li class="toc-article-item toc-article-level-3"><a class="toc-article-link" href="#gradient-descent"><span class="toc-article-text">5.2 Gradient Descent</span></a></li></ol></li><li class="toc-article-item toc-article-level-2"><a class="toc-article-link" href="#advance-optimization"><span class="toc-article-text">06 Advance Optimization</span></a></li><li class="toc-article-item toc-article-level-2"><a class="toc-article-link" href="#multi-class-classification-one-vs-all"><span class="toc-article-text">07 Multi-class
Classification: One vs all</span></a><ol class="toc-article-child"><li class="toc-article-item toc-article-level-3"><a class="toc-article-link" href="#multi-class-classification"><span class="toc-article-text">7.1 Multi-class Classification</span></a></li><li class="toc-article-item toc-article-level-3"><a class="toc-article-link" href="#one-vs-all"><span class="toc-article-text">7.2 One vs all</span></a></li></ol></li></ol>
		</div>
	
	</div>
	
    <hr>
	
</div><!-- col-md-3 -->

	</div>
		

</div><!-- row -->




    </div>
  </div>
  <div class="container-narrow">
    <footer> <!--<p>
  &copy; 2025 me
  
      with help from <a href="http://hexo.io/" target="_blank">Hexo</a> and <a href="http://getbootstrap.com/" target="_blank">Twitter Bootstrap</a>. Theme by <a target="_blank" rel="noopener" href="http://github.com/wzpan/hexo-theme-freemind/">Freemind</a>.    
</p>-->
 </footer>
  </div> <!-- container-narrow -->
  


  
<a id="gotop" href="#">   
  <span>▲</span> 
</a>

<script src="/js/jquery.imagesloaded.min.js"></script>
<script src="/js/gallery.js"></script>
<script src="/js/bootstrap.min.js"></script>
<script src="/js/main.js"></script>
<script src="/js/search.js"></script>


<link rel="stylesheet" href="/fancybox/jquery.fancybox.css" media="screen" type="text/css">
<script src="/fancybox/jquery.fancybox.pack.js"></script>
<script type="text/javascript">
(function($){
  $('.fancybox').fancybox();
})(jQuery);
</script>



   <script type="text/javascript">      
     var search_path = "search.xml";
	 if (search_path.length == 0) {
	 	search_path = "search.xml";
	 }
	 var path = "/" + search_path;
     searchFunc(path, 'local-search-input', 'local-search-result');
   </script>


<!-- syntax highlighting -->


<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</body>
	<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</html>