<!DOCTYPE HTML>
<html>
<head>
  <meta charset="utf-8">
  
  <title>图注意力机制的原理 | Jiahao Peng</title>
  <meta name="author" content="me">
  
  <meta name="description" content="该篇笔记总结了图注意力网络（Graph Attantion
Network）中的图注意力层的数学原理，参考的资料为 GAT 网络的原始论文.
假设输入是一组节点特征，表示为 $h=\{\vec{h_1},\vec{h_2},\cdots,\vec{h_N}\},\vec{h_i}\in\mathbb{R}^F$，其中
N 表示节点的个数，F 表示特征维数。GAL
将会输出一组新的节点特征，并且这组特征的特征维数并不一定与原特征相同，设
F′ 为 GAL
输出的特征维数，则新的节点特征可表示为 $h^{\prime}=\{\vec{h_1^{\prime}},\vec{h_2^{\prime}},\cdots,\vec{h_N^{\prime}}\},\vec{h_i^{\prime}}\in\mathbb{R}^{F^{\prime}}$。">
  
  
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

  <meta property="og:title" content="图注意力机制的原理"/>
  <meta property="og:site_name" content="Jiahao Peng"/>

  
    <meta property="og:image" content=""/>
  

  
    <link rel="alternative" href="/atom.xml" title="Jiahao Peng" type="application/atom+xml">
  
  
    <link href="/favicon.png" rel="icon">
  

  <!-- CSS -->
  <link rel="stylesheet" href="/css/themes/lumen.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/font-awesome.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/style.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/responsive.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/highlight.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/highlight-default.min.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/google-fonts.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/comment.css" media="screen" type="text/css">
  <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
	<script src="https://cdnjs.cloudflare.com/ajax/libs/es5-shim/4.5.9/es5-shim.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/es5-shim/4.5.7/es5-sham.min.js"></script>
  <![endif]-->

  <script src="/js/jquery-2.0.3.min.js"></script>
  
  
  <!-- analytics -->
  



<meta name="generator" content="Hexo 7.3.0"></head>

<body>
  <nav id="main-nav" class="navbar navbar-inverse navbar-default navbar-fixed-top" role="navigation">
    <div class="container">
      <button type="button" class="navbar-header navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
	<span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
       <a class="navbar-brand" href="/">Jiahao Peng</a>
      <div class="collapse navbar-collapse nav-menu">
		<ul class="nav navbar-nav">
		  
		  <li>
			<a href="/archives" title="All the articles.">
			  <i class="fa fa-archive"></i>Archives
			</a>
		  </li>
		  
		  <li>
			<a href="/categories" title="All the categories.">
			  <i class="fa fa-folder"></i>Categories
			</a>
		  </li>
		  
		  <li>
			<a href="/tags" title="All the tags.">
			  <i class="fa fa-tags"></i>Tags
			</a>
		  </li>
		  
		  <li>
			<a href="/about" title="About me.">
			  <i class="fa fa-user"></i>About
			</a>
		  </li>
		  
		</ul>
      </div>
    </div> <!-- container -->
</nav>
<div class="clearfix"></div>

  <div class="container">
    <div class="content">
      


	
		<div class="page-header page-header-inverse ">		
			<h1 class="title title-inverse "> 图注意力机制的原理</h1>
		</div>		
	






<div class="row post">
	<!-- cols -->
	
	<div id="top_meta"></div>
	<div class="col-md-9">
	

	<!-- content -->
	<div class="mypage">		
	  		

	  <p>该篇笔记总结了图注意力网络（Graph Attantion
Network）中的图注意力层的数学原理，参考的资料为 GAT 网络的<a
target="_blank" rel="noopener" href="https://arxiv.org/abs/1710.10903">原始论文</a>.</p>
<p>假设输入是一组节点特征，表示为 <span
class="math inline">$h=\{\vec{h_1},\vec{h_2},\cdots,\vec{h_N}\},\vec{h_i}\in\mathbb{R}^F$</span>，其中
<span class="math inline"><em>N</em></span> 表示节点的个数，<span
class="math inline"><em>F</em></span> 表示特征维数。GAL
将会输出一组新的节点特征，并且这组特征的特征维数并不一定与原特征相同，设
<span class="math inline"><em>F</em><sup>′</sup></span> 为 GAL
输出的特征维数，则新的节点特征可表示为 <span
class="math inline">$h^{\prime}=\{\vec{h_1^{\prime}},\vec{h_2^{\prime}},\cdots,\vec{h_N^{\prime}}\},\vec{h_i^{\prime}}\in\mathbb{R}^{F^{\prime}}$</span>。</p>
<span id="more"></span>
<p>为了把输入特征转换为高维特征，并且使高维特征具有足够高的表达原特征的能力，我们需要执行至少一个可学习的线性变换。考虑到这一点，我们将对每一个节点进行一次线性变换，即将每个节点的特征乘上一个<strong>共享</strong>的权重矩阵
<span
class="math inline"><em>W</em> ∈ ℝ<sup><em>F</em><sup>′</sup> × <em>F</em></sup></span>。然后我们会对每一个节点进行一次自注意力
(<em>self-attention</em>) 操作，我们用 <span
class="math inline"><em>a</em></span>
表示这一操作，该操作将会计算得出节点 <span
class="math inline"><em>i</em></span> 和节点 <span
class="math inline"><em>j</em></span> 之间的注意力系数 (<em>attention
coefficients</em>)： <span class="math display">$$
e_{ij}=a(W\vec{h_i},W\vec{h_j})
$$</span> 注意力系数 <span
class="math inline"><em>e</em><sub><em>i</em><em>j</em></sub></span>
表示了节点 <span class="math inline"><em>j</em></span> 的特征对于节点
<span class="math inline"><em>i</em></span>
的重要性。在自注意力机制最广泛应用的公式中，模型将会计算每个节点之间的注意力系数，导致结构信息被丢弃。我们通过执行掩码注意力机制
(<em>masked attention</em>)
来保留图的结构信息——我们只计算相邻节点之间的注意力系数。注意，这里的相邻节点指的是直接相邻
(first-order)
的节点，而不是连通的节点，并且一个节点本身也是其自己的相邻节点。为了能对注意力系数进行跨节点的比较，我们对它们进行归一化，用以衡量不同节点
<span class="math inline"><em>j</em> ∈ 𝒩<sub><em>i</em></sub></span>
对节点 <span class="math inline"><em>i</em></span> 的重要程度 (这里的
<span class="math inline">𝒩<sub><em>i</em></sub></span> 表示节点 <span
class="math inline"><em>i</em></span> 的邻居节点的编号)，归一化的过程由
softmax 函数实现： <span class="math display">$$
\alpha_{ij}=\operatorname{softmax}(e_{ij})=\frac{\operatorname{exp}(e_{ij})}{\sum_{k\in\mathcal{N}_i}\operatorname{exp}(e_{ik})}.
$$</span> 在我们的实验中，注意力机制 <span
class="math inline"><em>a</em></span>
是一个单层的前馈神经网络，以一个权重向量 <span
class="math inline">$\vec{\mathbf{a}}\in\mathbb{R}^{2F^\prime}$</span>​
作为参数，使用 LeakyReLU 函数实现非线性变换 (negative slope 设置为
0.2)。展开后，系数的计算可表示为： <span class="math display">$$
\alpha_{ij}=\frac{\operatorname{exp}(\operatorname{LeakyReLU}(\vec{\mathbf{a}}^{T}[\mathbf{W}\vec{h_i}||\mathbf{W}\vec{h_j}]))}{\sum_{k\in\mathcal{N}_i}\operatorname{exp}(\operatorname{LeakyReLU}(\vec{\mathbf{a}}^{T}[\mathbf{W}\vec{h_i}||\mathbf{W}\vec{h_k}]))}
$$</span> 其中，<span class="math inline">·<sup><em>T</em></sup></span>
表示转置操作，<span class="math inline">||</span> 是矩阵拼接操作。</p>
<p>一旦得到注意力系数，GAL
就会使用它们来计算出各个特征对应的线性组合，作为节点最终的输出特征，整个过程如下左图所示
(右图是多头注意力机制)：</p>
<p><img src="/img/GAT-01.png" /></p>
<p>输出特征 <span
class="math inline"><em>h⃗</em><sub><em>i</em></sub><sup>′</sup></span>
的计算公式如下： <span
class="math display"><em>h⃗</em><sub><em>i</em></sub><sup>′</sup> = <em>σ</em>(∑<sub><em>j</em> ∈ 𝒩<sub><em>i</em></sub></sub><em>α</em><sub><em>i</em><em>j</em></sub><strong>W</strong><em>h⃗</em><sub><em>j</em></sub>)</span>
其中，<span class="math inline"><em>σ</em></span>
代表一种非线性变换。</p>
<p>为了使整个自注意力过程更加稳定，我们引入了多头注意力 (<em>multi-head
attention</em>)。具体来说，<span class="math inline"><em>K</em></span>
个独立的注意力机制将分别执行上式的变换，随后，它们的结果将会被拼接起来，得到下面的输出特征表示：
<span class="math display">$$
\vec{h}_i^\prime=\overset{K}{\underset{k=1}{\|}}\sigma(\sum_{j\in\mathcal{N_i}}\alpha_{ij}^k\mathbf{W}^k\vec{h}_j)
$$</span>
特殊的，如果我们在输出层执行多头注意力，拼接过程将不再是显式的，我们会使用平均聚合
(<em>averaging</em>)，然后施加一个非线性变换来得到最终输出： <span
class="math display">$$
\vec{h}_i^\prime=\sigma(\frac{1}{K}\sum_{k=1}^{K}\sum_{j\in\mathcal{N_i}}\alpha_{ij}^k\mathbf{W}^k\vec{h}_j)
$$</span></p>
	  
	</div>

	<!-- recommended posts -->
	

	<div>
  	<center>
	<div class="pagination">
<ul class="pagination">
	 
				
		<li class="prev"><a href="/2025/03/18/%E7%A6%BB%E6%95%A3%E6%95%B0%E5%AD%A6/" class="alignleft prev"><i class="fa fa-arrow-circle-o-left"></i>Prev</a></li>
  		

        <li><a href="/archives"><i class="fa fa-archive"></i>Archive</a></li>

		
          <li class="next"><a href="/2023/12/18/%E9%9A%90%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E6%A8%A1%E5%9E%8B/" class="alignright next">Next<i class="fa fa-arrow-circle-o-right"></i></a></li>
        
	
</ul>
</div>

    </center>
	</div>

    <!-- share -->
    
        
    <div class="bdsharebuttonbox">
        <a href="#" class="bds_more" data-cmd="more"></a>
        <a href="#" class="bds_weixin" data-cmd="weixin" title="分享到微信"></a>
        <a href="#" class="bds_tsina" data-cmd="tsina" title="分享到新浪微博"></a>
        <a href="#" class="bds_fbook" data-cmd="fbook" title="分享到Facebook"></a>
        <a href="#" class="bds_twi" data-cmd="twi" title="分享到Twitter"></a>
        <a href="#" class="bds_linkedin" data-cmd="linkedin" title="分享到linkedin"></a>
        <a href="#" class="bds_evernotecn" data-cmd="evernotecn" title="分享到印象笔记"></a>
        <a href="#" class="bds_youdao" data-cmd="youdao" title="分享到有道云笔记"></a>
        <a href="#" class="bds_copy" data-cmd="copy" title="分享到复制网址"></a>
    </div>
    <script>
        window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"2","bdMiniList":false,"bdPic":"","bdStyle":"1","bdSize":"24"},"share":{}};
        with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];
    </script>


        

    
	
	<!-- comment -->
	
<section id="comment">
  <h2 class="title">Comments</h2>
  
</section>


	</div> <!-- col-md-9/col-md-12 -->
		
	
	<div id="side_meta">
		<div class="col-md-3" id="post_meta"> 

	<!-- date -->
	
	<div class="meta-widget">
	<i class="fa fa-clock-o"></i>
	2024-04-03 
	</div>
	

	<!-- categories -->
    
	<div class="meta-widget">
	<a data-toggle="collapse" data-target="#categorys"><i class="fa fa-folder"></i></a>	
    <ul id="categorys" class="tag_box list-unstyled collapse in">
          
  <li>
    <li><a href="/categories/deep-learning/">deep learning<span>3</span></a></li>
  </li>

    </ul>
	</div>
	

	<!-- tags -->
	
	<div class="meta-widget">
	<a data-toggle="collapse" data-target="#tags"><i class="fa fa-tags"></i></a>		  
    <ul id="tags" class="tag_box list-unstyled collapse in">	  
	    
  <li><a href="/tags/technique/">technique<span>7</span></a></li>

    </ul>
	</div>
	

	<!-- toc -->
	<div class="meta-widget">
	
	</div>
	
    <hr>
	
</div><!-- col-md-3 -->

	</div>
		

</div><!-- row -->




    </div>
  </div>
  <div class="container-narrow">
    <footer> <!--<p>
  &copy; 2025 me
  
      with help from <a href="http://hexo.io/" target="_blank">Hexo</a> and <a href="http://getbootstrap.com/" target="_blank">Twitter Bootstrap</a>. Theme by <a target="_blank" rel="noopener" href="http://github.com/wzpan/hexo-theme-freemind/">Freemind</a>.    
</p>-->
 </footer>
  </div> <!-- container-narrow -->
  


  
<a id="gotop" href="#">   
  <span>▲</span> 
</a>

<script src="/js/jquery.imagesloaded.min.js"></script>
<script src="/js/gallery.js"></script>
<script src="/js/bootstrap.min.js"></script>
<script src="/js/main.js"></script>
<script src="/js/search.js"></script>


<link rel="stylesheet" href="/fancybox/jquery.fancybox.css" media="screen" type="text/css">
<script src="/fancybox/jquery.fancybox.pack.js"></script>
<script type="text/javascript">
(function($){
  $('.fancybox').fancybox();
})(jQuery);
</script>



   <script type="text/javascript">      
     var search_path = "search.xml";
	 if (search_path.length == 0) {
	 	search_path = "search.xml";
	 }
	 var path = "/" + search_path;
     searchFunc(path, 'local-search-input', 'local-search-result');
   </script>


<!-- syntax highlighting -->


<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</body>
	<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</html>