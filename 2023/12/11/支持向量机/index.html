<!DOCTYPE HTML>
<html>
<head>
  <meta charset="utf-8">
  
  <title>支持向量机 | Jiahao Peng</title>
  <meta name="author" content="me">
  
  <meta name="description" content="支持向量机（support vector machines,
SVM）是一种二分类模型，它的基本模型是定义在特征空间上的间隔最大的线性分类器，间隔最大使它有别于感知机；SVM还包括核技巧，这使它成为实质上的非线性分类器。SVM的的学习策略就是间隔最大化，可形式化为一个求解凸二次规划的问题，也等价于正则化的合页损失函数的最小化问题。SVM的的学习算法就是求解凸二次规划的最优化算法。">
  
  
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

  <meta property="og:title" content="支持向量机"/>
  <meta property="og:site_name" content="Jiahao Peng"/>

  
    <meta property="og:image" content=""/>
  

  
    <link rel="alternative" href="/atom.xml" title="Jiahao Peng" type="application/atom+xml">
  
  
    <link href="/favicon.png" rel="icon">
  

  <!-- CSS -->
  <link rel="stylesheet" href="/css/themes/lumen.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/font-awesome.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/style.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/responsive.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/highlight.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/highlight-default.min.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/google-fonts.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/comment.css" media="screen" type="text/css">
  <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
	<script src="https://cdnjs.cloudflare.com/ajax/libs/es5-shim/4.5.9/es5-shim.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/es5-shim/4.5.7/es5-sham.min.js"></script>
  <![endif]-->

  <script src="/js/jquery-2.0.3.min.js"></script>
  
  
  <!-- analytics -->
  



<meta name="generator" content="Hexo 7.3.0"></head>

<body>
  <nav id="main-nav" class="navbar navbar-inverse navbar-default navbar-fixed-top" role="navigation">
    <div class="container">
      <button type="button" class="navbar-header navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
	<span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
       <a class="navbar-brand" href="/">Jiahao Peng</a>
      <div class="collapse navbar-collapse nav-menu">
		<ul class="nav navbar-nav">
		  
		  <li>
			<a href="/archives" title="All the articles.">
			  <i class="fa fa-archive"></i>Archives
			</a>
		  </li>
		  
		  <li>
			<a href="/categories" title="All the categories.">
			  <i class="fa fa-folder"></i>Categories
			</a>
		  </li>
		  
		  <li>
			<a href="/tags" title="All the tags.">
			  <i class="fa fa-tags"></i>Tags
			</a>
		  </li>
		  
		  <li>
			<a href="/about" title="About me.">
			  <i class="fa fa-user"></i>About
			</a>
		  </li>
		  
		</ul>
      </div>
    </div> <!-- container -->
</nav>
<div class="clearfix"></div>

  <div class="container">
    <div class="content">
      


	
		<div class="page-header page-header-inverse ">		
			<h1 class="title title-inverse "> 支持向量机</h1>
		</div>		
	






<div class="row post">
	<!-- cols -->
	
	<div id="top_meta"></div>
	<div class="col-md-9">
	

	<!-- content -->
	<div class="mypage">		
	  		

	  <p><strong>支持向量机（support vector machines,
SVM）</strong>是一种二分类模型，它的基本模型是定义在特征空间上的<strong>间隔最大的线性分类器</strong>，间隔最大使它有别于感知机；SVM还包括<strong>核技巧</strong>，这使它成为实质上的非线性分类器。SVM的的学习策略就是间隔最大化，可形式化为一个求解凸二次规划的问题，也等价于正则化的合页损失函数的最小化问题。SVM的的学习算法就是求解凸二次规划的最优化算法。</p>
<span id="more"></span>
<h2 id="线性模型">01 线性模型</h2>
<h3 id="算法思路">1.1 算法思路</h3>
<p>假设训练样本在空间中的分布如下左图分布，圆圈和星星分别代表两类不同类别的数据，那么我们能找出一条直线，将两者分割开，我们就称这样的训练样本集为一个<strong>线性可分（Linear
Separable）样本集</strong>，这样的模型就被称为<strong>线性模型</strong>；同理，倘若我们找不到这样一条直线，将两者完全分离开，如下右图所示，则称这样的训练样本集为一个<strong>线性不可分（Non-Linear
Separable）样本集</strong>，这样的模型就被称为<strong>非线性模型</strong>。</p>
<p><img src="/img/支持向量机-01.png" /></p>
<p><strong>支持向量机</strong>算法的思路大致是这样的：首先讨论如何在线性可分的训练样本集上找一条直线将样本分开，然后想办法将这样的方法推广到线性不可分的训练样本集上。所以，我们首先讨论第一部分：如何找到一条直线将线性可分训练样本集分开。</p>
<p>对于一个训练样本集，可以证明：<u>只要存在一条直线可以将样本集分开，就肯定存在无数条直线能将该样本集分开</u>（如下图所示）。既然如此，支持向量机提出的第一个问题就是：哪条直线是最好的？</p>
<p><img src="/img/支持向量机-02.png" /></p>
<p>通过直觉来判断，我们也可以感受出上图中的红色直线应该是最好的，问题是为什么？要解答这个问题，我们就必须定义一种性能指标（Performace
Measure），来评估每一条直线的好坏。</p>
<p>为了给出这个性能指标，支持向量机做的事情是，将上面的红线向左右两边平行移动，直到这条线碰到一个或几个样本点为止（如下图中两条虚线所示）：</p>
<p><img src="/img/支持向量机-03.png" /></p>
<p>然后，支持向量机给出了这个性能指标的定义，就是上图中两条虚线的距离（Gap），用
<span class="math inline"><em>d</em></span>
表示。而性能最好的那条线，就是能使 <span
class="math inline"><em>d</em></span>
最大的那条线。但是这样还不完善，要知道，能使 <span
class="math inline"><em>d</em></span>
最大的线也不唯一，将上图中的实线左右移动，作一条平行线，只要平行线不越过两条虚线所界定的范围，<span
class="math inline"><em>d</em></span>
就是不变的，所以还得给出另一个限制条件：直线必须位于两根平行线的正中间，也就是使上图中的实线与左右两根平行虚线的距离分别为
<span class="math inline">$\frac d2$</span>。</p>
<p><img src="/img/支持向量机-04.png" /></p>
<h3 id="数学描述">1.2 数学描述</h3>
<p>既然性能指标已经确定了，下一个问题就是如何描述这个优化过程了。在描述优化过程之前，我们还是先得给出一些定义，首先，我们将上面的
<span class="math inline"><em>d</em></span>
称为<strong>间隔（Margin）</strong>，将虚线穿过的向量称为<strong>支持向量（Support
Vectors）</strong>。通过上面对支持向量机算法的简单描述，我们可以发现，支持向量机找到的最优直线，只与支持向量有关，与其他向量无关，这就是为什么支持向量机也能用在小样本的数据上。</p>
<p>先给出线性模型的数学描述：</p>
<ol type="1">
<li><p>定义训练数据及标签为 <span
class="math inline">(<em>X</em><sub>1</sub>,<em>y</em><sub>1</sub>)、(<em>X</em><sub>2</sub>,<em>y</em><sub>2</sub>)...(<em>X</em><sub><em>n</em></sub>,<em>y</em><sub><em>n</em></sub>)</span>，其中，<span
class="math inline"><em>X</em></span>
是样本的特征，在上面给出的例子里，每个样本的特征是二维的，也就是说 <span
class="math inline">$X=\left[ \begin{array}{} x_1 \\ x_2
\end{array}\right]$</span> ，分别对应 x 轴和 y 轴；而 <span
class="math inline"><em>y</em></span>
是标签，在上面这个二分类问题里，标签只有两种，所以 <span
class="math inline"><em>y</em> ∈ { − 1, 1}</span>。</p></li>
<li><p>我们定义一个线性模型为 <span
class="math inline">(<em>W</em>,<em>b</em>)</span>，其中 <span
class="math inline"><em>W</em></span> 是一个向量，其维数与特征向量 <span
class="math inline"><em>X</em></span> 一致，<span
class="math inline"><em>b</em></span>
是一个常数，一个线性模型确定一个<strong>超平面（Hyperplane）</strong>，所谓超平面就是指划分空间的平面，超平面在二维空间里表现为我们上面所说的那条划分样本点的直线，而在更高的维度里就是一个平面，故称之为超平面。超平面由
<span class="math inline"><em>W</em></span> 和 <span
class="math inline"><em>b</em></span> 确定，其方程为 <span
class="math inline"><em>W</em><sup><em>T</em></sup><em>X</em> + <em>b</em> = 0</span>。机器学习的目标，就是通过所有样本的特征
<span class="math inline"><em>X</em></span> 来找到一个 <span
class="math inline"><em>W</em></span> 和 <span
class="math inline"><em>b</em></span>，使能够确定一个超平面能划分所有的样本点。</p></li>
<li><p>一个训练集线性可分是指：对于 <span
class="math inline">{(<em>X</em><sub><em>i</em></sub>,<em>y</em><sub><em>i</em></sub>)}<sub><em>i</em> = 1 ∼ <em>N</em></sub></span>，<span
class="math inline">$\exist(W,b)$</span>，使 <span
class="math inline">∀<em>i</em> = 1 ∼ <em>N</em></span>，有：</p>
<ol type="1">
<li>若 <span
class="math inline"><em>y</em><sub><em>i</em></sub> =  + 1</span>，则
<span
class="math inline"><em>W</em><sup><em>T</em></sup><em>X</em><sub><em>i</em></sub> + <em>b</em> ≥ 0</span>；</li>
<li>若 <span
class="math inline"><em>y</em><sub><em>i</em></sub> =  − 1</span>，则
<span
class="math inline"><em>W</em><sup><em>T</em></sup><em>X</em><sub><em>i</em></sub> + <em>b</em> &lt; 0</span>。</li>
</ol>
<p>当然，上述线性可分的定义是不唯一的，将 <span
class="math inline">≥</span> 和 <span class="math inline">&lt;</span>
换个位置也是一样。对于上面的定义，我们可以发现，凡是线性可分问题，一定存在
<span
class="math inline"><em>y</em><sub><em>i</em></sub>[<em>W</em><sup><em>T</em></sup><em>X</em><sub><em>i</em></sub>+<em>b</em>] ≥ 0</span>。</p></li>
</ol>
<p>接下来给出支持向量机优化问题的数学描述：</p>
<ol type="1">
<li>目标：最小化 <span
class="math inline">∥<em>W</em>∥<sup>2</sup></span></li>
<li>限制条件：<span
class="math inline"><em>y</em><sub><em>i</em></sub>[<em>W</em><sup><em>T</em></sup><em>X</em><sub><em>i</em></sub>+<em>b</em>] ≥ 1  (<em>i</em>=1∼<em>N</em>)</span></li>
</ol>
<p>对于上面这两个公式，相信很多人第一眼是懵的，因为按我们之前的描述，支持向量机算法就是去找一条使
<span class="math inline"><em>d</em></span>
最大且位于正中间位置的直线，怎么数学公式看起来跟这个过程完全没关系呢？</p>
<p>要搞清楚这两个公式，我们得先弄清楚两个事实：</p>
<ol type="1">
<li>事实一：<span
class="math inline"><em>W</em><sup><em>T</em></sup><em>X</em> + <em>b</em> = 0</span>
与 <span
class="math inline"><em>a</em><em>W</em><sup><em>T</em></sup><em>X</em> + <em>a</em><em>b</em> = 0  (<em>a</em>∈<em>R</em><sup>+</sup>)</span>
表示的是同一个平面。</li>
<li>事实二：向量 <span class="math inline"><em>X</em><sub>0</sub></span>
到超平面 <span
class="math inline"><em>W</em><sup><em>T</em></sup><em>X</em> + <em>b</em> = 0</span>
的距离是 <span class="math inline">$d=\frac{|W^TX_0+b|}{\lVert
W\rVert}$</span>。（不要慌，这其实就是高中学的点到平面的距离公式，以一维平面
<span
class="math inline"><em>w</em><sub>1</sub><em>x</em> + <em>w</em><sub>2</sub><em>y</em> + <em>b</em> = 0</span>，也就是直线为例，点
<span
class="math inline">(<em>x</em><sub>0</sub>,<em>y</em><sub>0</sub>)</span>
到这条直线的距离就是 <span
class="math inline">$d=\frac{|w_1x_0+w_2y_0+b|}{\sqrt{w_1^2+w_2^2}}$</span>，前面的那个公式只不是这个公式在高维情况下的推广）</li>
</ol>
<p>基于事实二，我们知道，支持向量机要做的事情，就是在 <span
class="math inline"><em>X</em><sub>0</sub></span> 是支持向量的情况下，使
<span class="math inline"><em>d</em></span> 最大。</p>
<p>基于事实一，我们知道，我们可以找到一个正实数 <span
class="math inline"><em>a</em></span> 来缩放 <span
class="math inline"><em>W</em></span> 和 <span
class="math inline"><em>b</em></span>，即 <span
class="math inline">(<em>W</em>,<em>b</em>) → (<em>a</em><em>W</em>,<em>a</em><em>b</em>)</span>，使
<span class="math inline"><em>d</em></span> 公式的分子 <span
class="math inline">|<em>W</em><sup><em>T</em></sup><em>X</em><sub>0</sub>+<em>b</em>| = 1</span>。这样的话，<span
class="math inline"><em>d</em></span> 的公式就变成了 <span
class="math inline">$d=\frac{1}{\lVert
W\rVert}$</span>。看到这个公式，就能明白为什么支持向量机的优化目标是最小化
<span class="math inline">∥<em>W</em>∥<sup>2</sup></span> 了，因为最小化
<span class="math inline">∥<em>W</em>∥<sup>2</sup></span> 就是最大化
<span class="math inline"><em>d</em></span>。</p>
<p>现在再来看限制条件，限制条件其实就是规定了，所有样本点到超平面的距离
<span
class="math inline"><em>W</em><sup><em>T</em></sup><em>X</em><sub><em>i</em></sub> + <em>b</em></span>，要么等于
<span class="math inline"><em>d</em></span>（支持向量），要么大于 <span
class="math inline"><em>d</em></span>（非支持向量）。
至于为什么要再乘上一个 <span
class="math inline"><em>y</em><sub><em>i</em></sub></span>，其实看<u>线性可分的定义</u>就知道了，乘上
<span class="math inline"><em>y</em><sub><em>i</em></sub></span>
是为了与线性可分的定义相统一。</p>
<blockquote>
<p>补充：</p>
<ul>
<li><p>为什么一定要使 <span
class="math inline">|<em>W</em><sup><em>T</em></sup><em>X</em><sub>0</sub>+<em>b</em>| = 1</span>，
<span
class="math inline">|<em>W</em><sup><em>T</em></sup><em>X</em><sub>0</sub>+<em>b</em>| = 2</span>
可不可以？可以，等于 1 还是等于 2 或是其他值都没有任何关系，这只取决于
<span class="math inline"><em>a</em></span> 的大小，而 <span
class="math inline"><em>a</em></span> 并不改变超平面。</p></li>
<li><p>对于任何线性可分样本集，一定能找到一个超平面分割所有样本点；反之，如果是线性不可分，那么将找不到任何一个能满足要求的
<span class="math inline"><em>W</em></span> 和 <span
class="math inline"><em>b</em></span>。</p></li>
<li><p>某些书上会将优化目标写成最小化 <span
class="math inline">$\frac12\lVert
W\rVert^2$</span>，这其实没有任何问题，加上 <span
class="math inline">$\frac12$</span> 只是为了求导方便。</p></li>
<li><p>支持向量机要解决的问题其实是一个凸优化问题，而且是一个二次规划问题，二次规划问题的特点是：</p>
<ul>
<li>目标函数（Objective Function）是二次项；</li>
<li>限制条件是一次项。</li>
</ul>
<p>对于凸优化问题，要么无解，要么只有一个解。凸优化问题是计算机领域研究最多的问题，因为凸优化问题要么无解，要么只要能找到一个解，那便是它唯一的解。所以只要证明一个问题是凸优化问题，那么我们只要找到一个局部极值，也便找到了它的全局极值，我们便可认定这个问题已经被解决了。</p>
<p>非凸问题的目标函数图像是一条包含很多局部极值的曲线，会使得机器很容易落入局部最优解的陷阱。支持向量机算法优美的地方就在于，它将求解目标化成了一个凸优化问题。</p></li>
</ul>
</blockquote>
<h2 id="非线性模型">02 非线性模型</h2>
<h3 id="优化目标">2.1 优化目标</h3>
<p>之前已经讨论过，非线性模型不是线性可分的，也就是说找不到一个 <span
class="math inline"><em>W</em></span> 和 <span
class="math inline"><em>b</em></span>，使之确定一个能完美分割所有样本点的超平面，即限制条件
<span
class="math inline"><em>y</em><sub><em>i</em></sub>[<em>W</em><sup><em>T</em></sup><em>X</em><sub><em>i</em></sub>+<em>b</em>] ≥ 1  (<em>i</em>=1∼<em>N</em>)</span>
是不可满足的，原本的优化目标是无解的。SVM
处理非线性模型的方法其实不难理解，就是在线性模型的基础上引入了一个<strong>松弛变量（Slack
Variable）</strong>，用 <span
class="math inline"><em>ξ</em> (<em>ξ</em>≥0)</span>
表示。新的优化目标如下：</p>
<ol type="1">
<li>目标：最小化 <span class="math inline">$\frac12\lVert
W\rVert^2+C\sum_{i=1}^N\xi_i$</span></li>
<li>限制条件：
<ol type="1">
<li><span
class="math inline"><em>y</em><sub><em>i</em></sub>[<em>W</em><sup><em>T</em></sup><em>X</em><sub><em>i</em></sub>+<em>b</em>] ≥ 1 − <em>ξ</em><sub><em>i</em></sub>  (<em>i</em>=1∼<em>N</em>)</span></li>
<li><span
class="math inline"><em>ξ</em><sub><em>i</em></sub> ≥ 0</span></li>
</ol></li>
</ol>
<p>可以发现，新的优化目标中，限制条件变成了 <span
class="math inline"><em>y</em><sub><em>i</em></sub>[<em>W</em><sup><em>T</em></sup><em>X</em><sub><em>i</em></sub>+<em>b</em>] ≥ 1 − <em>ξ</em><sub><em>i</em></sub>  (<em>i</em>=1∼<em>N</em>)</span>，只要这个
<span class="math inline"><em>ξ</em><sub><em>i</em></sub></span>
取得足够大，那么大于等于号右边就会无限小，那么限制条件就有了满足的可能；但同时，也不能允许
<span class="math inline"><em>ξ</em><sub><em>i</em></sub></span>
无限大，不然就没有意义了，所以新的最小化目标函数的末尾还要加上 <span
class="math inline"><em>ξ</em><sub><em>i</em></sub></span>。</p>
<p>接下来要明确，在上面的式子中，哪些是已知的，哪些是要求解的参数。显然，<span
class="math inline"><em>X</em></span>和<span
class="math inline"><em>y</em><sub><em>i</em></sub></span>是已知的，<span
class="math inline"><em>W</em></span>、<span
class="math inline"><em>b</em></span>以及<span
class="math inline"><em>ξ</em></span>是要求的，但是这里还有个<span
class="math inline"><em>C</em></span>，这个<span
class="math inline"><em>C</em></span>是什么？<span
class="math inline"><em>C</em></span>是一个由人事先设定的参数，这种参数一般称为<strong>超参数</strong>（<strong>Hyperparameter</strong>），作用是平衡<span
class="math inline">$\frac{1}{2}\lVert\ W\rVert^{2}$</span>和<span
class="math inline">$\sum_{i=1}^{N}\xi_{i}$</span>的权重。至于<span
class="math inline"><em>C</em></span>具体取多少是没有定论的，一般是凭经验，选定一个区间，然后一个一个尝试。SVM很方便的一点就是，它只有这一个参数需要人来设置，但是在神经网络里，要去一个一个尝试的参数可能有很多。</p>
<h3 id="高维映射">2.2 高维映射</h3>
<p>虽然通过引入松弛变量，我们将非线性问题转换为了一个线性可分问题，但是还是存在一个问题，那就是求解目标的本质没有变，最后仍然是找出一条直线，来分割样本点，也就是说，即使一个样本集用肉眼看就能看出其能被一条简单的曲线分割，SVM
还是会找一条直线来分割样本点，如下图所示：</p>
<p><img src="/img/支持向量机-05.png" /></p>
<p>这显然不是我们想要的。一些算法会很符合直觉地去找非直线来分割样本集，例如决策树是用矩形来分割，但是
SVM
的思想很精妙，它仍然是找直线，不过它不是在当前空间里去找，而是到高维空间里去找。它定义了一个<strong>高维映射</strong>
<span class="math inline"><em>ϕ</em>(<em>X</em>)</span>，通过 <span
class="math inline"><em>ϕ</em></span>，能将 <span
class="math inline"><em>X</em></span> 这个低维向量转化成一个高维向量
<span
class="math inline"><em>ϕ</em>(<em>X</em>)</span>。也就是说，也许在低维空间中，我们不容易去找一条直线能刚刚好分割所有样本点，那么我们就去高维空间中找，或许在高维空间中，我们就能找到样一条理想的直线了。</p>
<p>接下来我们用异或问题的例子，来具体解释这个过程为什么有效。异或问题是二维空间下最简单的非线性问题，其在二维空间中存在如下样本点分布：</p>
<p><img src="/img/支持向量机-06.png" /></p>
<p>我们先将图中四个样本点表示为 <span
class="math inline"><em>X</em><sub>1</sub></span>、<span
class="math inline"><em>X</em><sub>2</sub></span>、<span
class="math inline"><em>X</em><sub>3</sub></span> 和 <span
class="math inline"><em>X</em><sub>4</sub></span>，并且 <span
class="math inline"><em>X</em><sub>1</sub></span> 和 <span
class="math inline"><em>X</em><sub>2</sub></span> 属于一个类别 <span
class="math inline"><em>C</em><sub>1</sub></span>，<span
class="math inline"><em>X</em><sub>3</sub></span> 和 <span
class="math inline"><em>X</em><sub>4</sub></span> 属于一个类别 <span
class="math inline"><em>C</em><sub>2</sub></span>，有： <span
class="math display">$$
\begin{aligned}
&amp;X_1=\left[ \begin{array}{} 0 \\ 0 \end{array} \right]\quad
X_2=\left[ \begin{array}{} 1 \\ 1 \end{array} \right]\quad\in C_1\\
&amp;X_3=\left[ \begin{array}{} 1 \\ 0 \end{array} \right]\quad
X_4=\left[ \begin{array}{} 0 \\ 1 \end{array} \right]\quad\in C_2\\
\end{aligned}
$$</span> 定义高维映射函数为： <span class="math display">$$
\phi(X):\quad X=\left[ \begin{array}{} a \\ b \end{array}
\right]\overset{\phi}{\longrightarrow}\phi(X)=\left[ \begin{array}{} a^2
\\ b^2 \\ a \\ b \\ ab \end{array} \right]
$$</span> 则经过映射，四个样本点将变为： <span class="math display">$$
\begin{aligned}
&amp;\phi(X_1)=\left[ \begin{array}{} 0 \\ 0 \\ 0 \\ 0 \end{array}
\right]\quad \phi(X_2)=\left[ \begin{array}{} 1 \\ 1 \\ 1 \\ 1
\end{array} \right]\quad\in C_1\\
&amp;\phi(X_3)=\left[ \begin{array}{} 1 \\ 0 \\ 1 \\ 0 \\ 0 \end{array}
\right]\quad \phi(X_4)=\left[ \begin{array}{} 0 \\ 1 \\ 0 \\ 1 \\ 0
\end{array} \right]\quad\in C_2\\
\end{aligned}
$$</span> 现在，<span class="math inline"><em>X</em></span>
变成了五维向量，则 <span class="math inline"><em>W</em></span>
也要变成五维向量，<span class="math inline"><em>b</em></span>
仍然为常量，求解的目标就变成在五维空间中找一个超平面来分割四个样本点了。能做到分割的超平面不唯一，这里举一个例子：
<span class="math display">$$
W=\left[ \begin{array}{} -1 \\ -1 \\ -1 \\ -1 \\ 6 \end{array}
\right]\quad b=1
$$</span> 将样本点代入超平面的方程： <span class="math display">$$
\begin{aligned}
&amp;W^T\phi(X_1)+b=1&gt;0\\
&amp;W^T\phi(X_2)+b=3&gt;0\\
&amp;W^T\phi(X_3)+b=-1&lt;0\\
&amp;W^T\phi(X_4)+b=-1&lt;0\\
\end{aligned}
$$</span> 如上，该超平面刚刚好把 <span
class="math inline"><em>X</em><sub>1</sub></span>、<span
class="math inline"><em>X</em><sub>2</sub></span> 与 <span
class="math inline"><em>X</em><sub>3</sub></span>、<span
class="math inline"><em>X</em><sub>4</sub></span>
分开了。也就是说，在低维空间中线性不可分的样本集，可能在高维空间中就是线性可分的，这也就是我们要去升维的原因。关于这一点也有很多人研究过，它们的结论是，对于任何线性不可分的样本集，特征空间的维数越高，其被线性分割的概率也越大；若维数趋近无穷大，那么其被线性分割的概率将达到
1.</p>
<h3 id="核函数">2.3 核函数</h3>
<p>在引入了高维映射之后，优化式 1 就变成了 <span
class="math inline"><em>y</em><sub><em>i</em></sub>[<em>W</em><sup><em>T</em></sup><em>ϕ</em>(<em>X</em><sub><em>i</em></sub>)+<em>b</em>] ≥ 1 − <em>ξ</em><sub><em>i</em></sub>  (<em>i</em>=1∼<em>N</em>)</span>，虽然看起来只有
<span class="math inline"><em>X</em><sub><em>i</em></sub></span>
发生了变化，但不要忘记 <span class="math inline"><em>W</em></span>
也跟着一起升维了。那么现在面临的问题就是：<u>如何选取 <span
class="math inline"><em>ϕ</em></span></u> ？SVM 的回答是：无限维。</p>
<p>将特征空间增长到无限维，线性不可分问题就绝对可以变成线性可分。但是问题在于，当
<span class="math inline"><em>ϕ</em>(<em>X</em>)</span>
变成无限维，<span class="math inline"><em>W</em></span>
也要变成无限维，那这个问题就没有办法做了。这也是 SVM
巧妙的另一个地方，它在将特征空间映射到无限维的同时，又采用有限维的手段。</p>
<p>SVM 的意思是：我们可以不知道无限维映射 <span
class="math inline"><em>ϕ</em>(<em>X</em>)</span>
的显式表达，我们只要知道一个<strong>核函数（Kernel Function）</strong>：
<span
class="math display"><em>K</em>(<em>X</em><sub>1</sub>,<em>X</em><sub>2</sub>) = <em>ϕ</em>(<em>X</em><sub>1</sub>)<sup><em>T</em></sup><em>ϕ</em>(<em>X</em><sub>2</sub>)</span>
则优化式 1 仍然可解。<span
class="math inline"><em>K</em>(<em>X</em><sub>1</sub>,<em>X</em><sub>2</sub>)</span>
其实计算的是 <span
class="math inline"><em>ϕ</em>(<em>X</em><sub>1</sub>)</span> 和 <span
class="math inline"><em>ϕ</em>(<em>X</em><sub>2</sub>)</span>
的内积，虽然 <span
class="math inline"><em>ϕ</em>(<em>X</em><sub>1</sub>)</span> 和 <span
class="math inline"><em>ϕ</em>(<em>X</em><sub>2</sub>)</span>
是无限维的，但是两者仍然能进行内积计算，得到的结果 <span
class="math inline"><em>K</em>(<em>X</em><sub>1</sub>,<em>X</em><sub>2</sub>)</span>
是一个数。</p>
<p>核函数的要求是：能将函数的形式最终拆成 <span
class="math inline"><em>ϕ</em>(<em>X</em><sub>1</sub>)<sup><em>T</em></sup><em>ϕ</em>(<em>X</em><sub>2</sub>)</span>
的形式。常用的核函数有如下几个：</p>
<ol type="1">
<li>高斯核：<span class="math inline">$K(X_1,X_2)=e^{-\frac{\lVert
X_1-X_2\rVert^2}{2\sigma^2}}=\phi(X_1)^T\phi(X_2)$</span>，<span
class="math inline"><em>σ</em><sup>2</sup></span> 是方差。</li>
<li>多项式核：<span
class="math inline"><em>K</em>(<em>X</em><sub>1</sub>,<em>X</em><sub>2</sub>) = (<em>X</em><sub>1</sub><sup><em>T</em></sup><em>X</em><sub>2</sub>+1)<sup><em>d</em></sup> = <em>ϕ</em>(<em>X</em><sub>1</sub>)<sup><em>T</em></sup><em>ϕ</em>(<em>X</em><sub>2</sub>)</span>，<span
class="math inline"><em>d</em></span> 是多项式阶数。</li>
</ol>
<p>而能将核函数 <span
class="math inline"><em>K</em>(<em>X</em><sub>1</sub>,<em>X</em><sub>2</sub>)</span>
拆成 <span
class="math inline"><em>ϕ</em>(<em>X</em><sub>1</sub>)<sup><em>T</em></sup><em>ϕ</em>(<em>X</em><sub>2</sub>)</span>
的充要条件是：</p>
<ol type="1">
<li><span
class="math inline"><em>K</em>(<em>X</em><sub>1</sub>,<em>X</em><sub>2</sub>) = <em>K</em>(<em>X</em><sub>2</sub>,<em>X</em><sub>1</sub>)</span>，即交换性；</li>
<li><span
class="math inline">∀<em>C</em><sub><em>i</em></sub>, <em>X</em><sub><em>i</em></sub> (<em>i</em>=1∼<em>N</em>)</span>，有
<span
class="math inline">$\sum_{i=1}^{N}\sum_{j=1}^{N}C_iC_jK(X_i,X_j)\ge0$</span>，即半正定性，也就是说，我们选取的核函数，必须要对任意选定的常数
<span class="math inline"><em>C</em></span> 和向量 <span
class="math inline"><em>X</em></span> 都满足该式；</li>
</ol>
<h3 id="原问题到对偶问题">2.4 原问题到对偶问题</h3>
<p>现在我们有了核函数，那么我们要怎样利用核函数，来替代优化式 1 中的
<span class="math inline"><em>ϕ</em>(<em>X</em>)</span>
呢？在这之前，请先阅读<a
href="#7.3*%20补充：优化理论">优化理论</a>相关的内容。在稍微了解了优化理论中的原问题和对偶问题后，我们要做的，就是<u>把
SVM 的优化问题从原问题转换成对偶问题</u>。</p>
<p>首先，我们<u>把 SVM 的优化问题转换成原问题</u>：</p>
<p>对于目标函数，<span class="math inline">$\frac12\lVert
W\rVert^2+C\sum_{i=1}^N\xi_i$</span> 是一个<strong>凸函数</strong>。</p>
<p>对于限制条件，<span
class="math inline"><em>ξ</em><sub><em>i</em></sub> ≥ 0</span>
不满足原问题的限制条件形式，得先将大于等于号变成小于等于号，也就是变成
<span
class="math inline"><em>ξ</em><sub><em>i</em></sub> ≤ 0</span>，那么，目标函数就也得变换一下，变成
<span class="math inline">$\frac12\lVert
W\rVert^2-C\sum_{i=1}^N\xi_i$</span>；同样，另一个限制条件也得变换一下，变成
<span
class="math inline"><em>y</em><sub><em>i</em></sub>[<em>W</em><sup><em>T</em></sup><em>X</em><sub><em>i</em></sub>+<em>b</em>] ≥ 1 + <em>ξ</em><sub><em>i</em></sub>  (<em>i</em>=1∼<em>N</em>)</span>，但是这个不等式也不满足原问题的要求，必须将不等式右边变成
0，所以得到 <span
class="math inline">1 + <em>ξ</em><sub><em>i</em></sub> − <em>y</em><sub><em>i</em></sub>[<em>W</em><sup><em>T</em></sup><em>X</em><sub><em>i</em></sub>+<em>b</em>] ≤ 0  (<em>i</em>=1∼<em>N</em>)</span>。于是得到优化目标的原问题形式，新的优化目标：</p>
<ol type="1">
<li>目标：最小化 <span class="math inline">$\frac12\lVert
W\rVert^2-C\sum_{i=1}^N\xi_i$</span></li>
<li>限制条件：
<ol type="1">
<li><span
class="math inline">1 + <em>ξ</em><sub><em>i</em></sub> − <em>y</em><sub><em>i</em></sub>[<em>W</em><sup><em>T</em></sup><em>ϕ</em>(<em>X</em><sub><em>i</em></sub>)+<em>b</em>] ≤ 0  (<em>i</em>=1∼<em>N</em>)</span></li>
<li><span
class="math inline"><em>ξ</em><sub><em>i</em></sub> ≤ 0</span></li>
</ol></li>
</ol>
<p><u>将其转换为对偶问题</u>：</p>
<ol type="1">
<li><p>目标：最大化 <span
class="math inline">$\theta(\alpha,\beta)=\underset{(w,\xi_i,b)}{\inf}\{\frac12\lVert
W\rVert^2-C\sum_{i=1}^N\xi_i+\sum_{i=1}^{N}\alpha_i(1+\xi_i-y_i[W^T\phi(X_i)+b])+\sum_{i=1}^N\beta_i\xi_i\}$</span></p></li>
<li><p>限制条件：</p>
<ol type="1">
<li><span
class="math inline"><em>α</em><sub><em>i</em></sub> ≥ 0  (<em>i</em>=1∼<em>N</em>)</span></li>
<li><span
class="math inline"><em>β</em><sub><em>i</em></sub> ≥ 0  (<em>i</em>=1∼<em>N</em>)</span></li>
</ol></li>
</ol>
<p>解释一下这样变换的原因：</p>
<ol type="1">
<li>原问题中的 <span class="math inline"><em>w</em></span>
对应了原问题要求解的变量，有三个，分别是 <span
class="math inline"><em>W</em></span>、<span
class="math inline"><em>b</em></span> 和 <span
class="math inline"><em>ξ</em></span>，所以对偶问题中要遍历所有的 <span
class="math inline"><em>w</em></span>，到这里就变成了遍历所有的 <span
class="math inline"><em>W</em></span>、<span
class="math inline"><em>b</em></span> 和 <span
class="math inline"><em>ξ</em></span>。</li>
<li>根据对偶问题的定义，<span
class="math inline">$L(\omega,\alpha,\beta)=f(\omega)+\sum_{i=1}^{K}\alpha_ig_i(\omega)+\sum_{i=1}^M\beta_ih_i(\omega)$</span>，其中，<span
class="math inline">$f(w)=\frac12\lVert
W\rVert^2-C\sum_{i=1}^N\xi_i$</span>，这一点是没有疑问的，关键是下面，千万不要以为这里的
<span class="math inline"><em>α</em></span> 和 <span
class="math inline"><em>β</em></span> 分别对应了上面的 <span
class="math inline"><em>α</em><sub><em>i</em></sub></span> 和 <span
class="math inline"><em>β</em><sub><em>i</em></sub></span>，不是这样的，在对偶问题中，<span
class="math inline"><em>α</em></span>
管的是不等式条件，每个不等式条件要与 <span
class="math inline"><em>α</em></span> 相乘，<span
class="math inline"><em>β</em></span> 管的是等式条件，每个等式条件要与
<span class="math inline"><em>β</em></span> 相乘。但是在这里，SVM
原问题中的限制条件都是不等式，所以应该只有 <span
class="math inline"><em>α</em></span>，没有 <span
class="math inline"><em>β</em></span>，只是说为了方便表示，这里仍然沿用
<span class="math inline"><em>α</em><sub><em>i</em></sub></span> 和
<span
class="math inline"><em>β</em><sub><em>i</em></sub></span>，并且，由于
<span class="math inline"><em>α</em></span> 应该大于
0，所以到这里就变成了 <span
class="math inline"><em>α</em><sub><em>i</em></sub> ≥ 0  (<em>i</em>=1∼<em>N</em>)</span>，并且
<span
class="math inline"><em>β</em><sub><em>i</em></sub> ≥ 0  (<em>i</em>=1∼<em>N</em>)</span>。其他部分就是照抄的关系了。</li>
</ol>
<p>接下来我们就来求一下 <span
class="math inline"><em>L</em>(<em>W</em>,<em>ξ</em><sub><em>i</em></sub>,<em>b</em>,<em>α</em>)</span>
的最小值：</p>
<p>令偏导 <span class="math inline">$\frac{\partial L}{\partial
W}=0$</span>，<span class="math inline">$\frac{\partial L}{\partial
\xi_i}=0$</span>，<span class="math inline">$\frac{\partial L}{\partial
b}=0$</span>： <span class="math display">$$
\begin{aligned}
&amp;\frac{\partial L}{\partial W}=0\Rightarrow
W=\sum_{i=1}^N\alpha_iy_i\phi(X_i)&amp;①\\
&amp;\frac{\partial L}{\partial \xi_i}=0\Rightarrow
C=\beta_i+\alpha_i&amp;②\\
&amp;\frac{\partial L}{\partial b}=0\Rightarrow
\sum_{i=1}^N\alpha_iy_i=0&amp;③
\end{aligned}
$$</span> 接下来，我们要将上面得到的三个式子代回到 <span
class="math inline"><em>L</em>(<em>W</em>,<em>ξ</em><sub><em>i</em></sub>,<em>b</em>,<em>α</em>)</span>
中去，好消息是，将式 1 和式 3
代入之后，式子中的大部分项就能被消掉了，得到 <span
class="math inline">$\theta(\alpha,\beta)=\frac12\lVert
W\rVert^2+\sum_{i=1}^N\alpha_i-\sum_{i=1}^N\alpha_iy_iW^T\phi(X_i)$</span>，先来计算
<span class="math inline">$\frac12\lVert W\rVert^2$</span>： <span
class="math display">$$
\begin{aligned}
\frac12\lVert W\rVert^2&amp;=\frac12W^TW\\
&amp;=\frac12(\sum_{i=1}^N\alpha_iy_i\phi(X_i))^T(\sum_{j=1}^N\alpha_jy_j\phi(X_j))\\
&amp;=\frac12\sum_{i=1}^N\sum_{j=1}^N\alpha_i\alpha_jy_iy_j\phi(X_i)^T\phi(X_j)\\
&amp;=\frac12\sum_{i=1}^N\sum_{j=1}^N\alpha_i\alpha_jy_iy_jK(X_i,X_j)
\end{aligned}
$$</span> 一件惊喜的事情：上式的最终化简结果里出现了核函数！接下来化简
<span
class="math inline">$-\sum_{i=1}^N\alpha_iy_iW^T\phi(X_i)$</span>：
<span class="math display">$$
\begin{aligned}
-\sum_{i=1}^N\alpha_iy_iW^T\phi(X_i)&amp;=-\sum_{i=1}^N\alpha_iy_i(\sum_{j=1}^N\alpha_jy_j\phi(X_j))^T\phi(X_i)\\
&amp;=-\sum_{i=1}^N\sum_{j=1}^N\alpha_i\alpha_jy_iy_j\phi(X_j)^T\phi(X_i)\\
&amp;=-\sum_{i=1}^N\sum_{j=1}^N\alpha_i\alpha_jy_iy_jK(X_i,X_j)
\end{aligned}
$$</span> 所以，最后会得到： <span class="math display">$$
\theta(\alpha,\beta)=\sum_{i=1}^{N}\alpha_i-\frac12\sum_{i=1}^N\sum_{j=1}^N\alpha_i\alpha_jy_iy_jK(X_i,X_j)
$$</span> 经过这样一系列的推导，最终问题的形式会变成：</p>
<ol type="1">
<li><p>目标：最大化 <span
class="math inline">$\theta(\alpha)=\sum_{i=1}^{N}\alpha_i-\frac12\sum_{i=1}^N\sum_{j=1}^N\alpha_i\alpha_jy_iy_jK(X_i,X_j)$</span></p></li>
<li><p>限制条件：</p>
<ol type="1">
<li><span
class="math inline">0 ≤ <em>α</em><sub><em>i</em></sub> ≤ <em>C</em>  (<em>i</em>=1∼<em>N</em>)</span></li>
<li><span class="math inline">$\sum_{i=1}^N\alpha_iy_i=0$</span></li>
</ol></li>
</ol>
<p>解释一下限制条件：根据之前求的偏导我们得到了 <span
class="math inline"><em>β</em><sub><em>i</em></sub> + <em>α</em><sub><em>i</em></sub> = <em>C</em></span>，由于之前的限制条件规定了
<span class="math inline"><em>β</em><sub><em>i</em></sub> ≥ 0</span>
以及 <span
class="math inline"><em>α</em><sub><em>i</em></sub> ≥ 0</span>，所以我们可以直接把这两个条件合并成一个条件，即
<span
class="math inline">0 ≤ <em>α</em><sub><em>i</em></sub> ≤ <em>C</em>  (<em>i</em>=1∼<em>N</em>)</span>，那么为什么要合并呢？因为我们现在的目标函数中只剩下了
<span class="math inline"><em>α</em><sub><em>i</em></sub></span> 和
<span
class="math inline"><em>α</em><sub><em>j</em></sub></span>，已经不存在
<span class="math inline"><em>β</em></span>
了；而第二个限制条件则是直接照抄的令 <span
class="math inline">$\frac{\partial L}{\partial b}=0$</span>
得到的结果。</p>
<p>在这个对偶问题中，目标函数仍然是一个<strong>凸函数</strong>。并且，其中未知的参数只有
<span class="math inline"><em>α</em><sub><em>i</em></sub></span> 和
<span
class="math inline"><em>α</em><sub><em>j</em></sub></span>，核函数是已经确定的了。由于是一个凸优化问题，所以它应该是很容易求解的。有一种凸优化问题求解算法叫做
<strong>SMO
算法</strong>，在这里不再展开叙述，感兴趣的同学自行了解。总之，我们只需要知道，这个问题是有解的。</p>
<p>但是到这里还没结束，我们现在已经将 SVM
的优化问题从原问题转换成了对偶问题，将 <span
class="math inline"><em>ϕ</em>(<em>X</em><sub><em>i</em></sub>)</span>
用核函数进行了替换，但是还有一个问题：<u>对偶问题是求解 <span
class="math inline"><em>α</em><sub><em>i</em></sub></span> 和 <span
class="math inline"><em>α</em><sub><em>j</em></sub></span>，而我们要的是
<span class="math inline"><em>W</em></span> 和 <span
class="math inline"><em>b</em></span>，如何在这两者之间进行转换？</u></p>
<p>这里又体现了 SVM 的精妙之处，我们并不需要知道 <span
class="math inline"><em>W</em></span>
具体长什么样，根据我们之前求偏导的结果，我们知道 <span
class="math inline">$W=\sum_{i=1}^N\alpha_iy_i\phi(X_i)$</span>，同时我们也知道，最后分类的方法是，对于测试样本
<span class="math inline"><em>X</em></span>，若：</p>
<ol type="1">
<li><span
class="math inline"><em>W</em><sup><em>T</em></sup><em>ϕ</em>(<em>X</em>) + <em>b</em> ≥ 0</span>，则
<span class="math inline"><em>y</em> =  + 1</span></li>
<li><span
class="math inline"><em>W</em><sup><em>T</em></sup><em>ϕ</em>(<em>X</em>) + <em>b</em> &lt; 0</span>，则
<span class="math inline"><em>y</em> =  − 1</span></li>
</ol>
<p>我们将 <span
class="math inline">$W=\sum_{i=1}^N\alpha_iy_i\phi(X_i)$</span>
代入到不等式左边的式子中，就会得到： <span class="math display">$$
\begin{aligned}
W^T\phi(X)+b&amp;=\sum_{i=1}^N[\alpha_iy_i\phi(X_i)]^T\phi(X)+b\\
&amp;=\sum_{i=1}^N\alpha_iy_i\phi(X_i)^T\phi(X)+b\\
&amp;=\sum_{i=1}^N\alpha_iy_iK(X_i,X)+b
\end{aligned}
$$</span> 所以说，我们并不需要知道 <span
class="math inline"><em>W</em></span>
的具体值，我们只需要有核函数，就能对样本进行分类。现在的关键问题是：<u><span
class="math inline"><em>b</em></span> 是多少</u>？<span
class="math inline"><em>b</em></span>
的求解并不简单，需要用到优化理论中的 <strong>KKT 条件</strong>。</p>
<p>根据 KKT 条件，当原问题和对偶问题满足强对偶定理时，<span
class="math inline">∀<em>i</em> = 1 ∼ <em>K</em></span>，要么 <span
class="math inline"><em>β</em><sub><em>i</em></sub><sup>*</sup> = 0</span>，要么
<span
class="math inline"><em>h</em><sub><em>i</em></sub>(<em>ω</em><sup>*</sup>) = 0</span>；要么
<span
class="math inline"><em>α</em><sub><em>i</em></sub><sup>*</sup> = 0</span>，要么
<span
class="math inline"><em>g</em><sub><em>i</em></sub>(<em>ω</em><sup>*</sup>) = 0</span>，而在这个问题中，<span
class="math inline"><em>g</em>(<em>W</em>) = 1 + <em>ξ</em><sub><em>i</em></sub> − <em>y</em><sub><em>i</em></sub>[<em>W</em><sup><em>T</em></sup><em>ϕ</em>(<em>X</em><sub><em>i</em></sub>)+<em>b</em>]</span>，所以，要么
<span
class="math inline"><em>α</em><sub><em>i</em></sub> = 0</span>，要么
<span
class="math inline"><em>g</em>(<em>W</em>) = 1 + <em>ξ</em><sub><em>i</em></sub> − <em>y</em><sub><em>i</em></sub>[<em>W</em><sup><em>T</em></sup><em>ϕ</em>(<em>X</em><sub><em>i</em></sub>)+<em>b</em>] = 0</span>.
现在，我们取一个 <span
class="math inline"><em>α</em><sub><em>i</em></sub></span>，使之 <span
class="math inline">0 &lt; <em>α</em><sub><em>i</em></sub> &lt; <em>C</em></span>（这是肯定能满足的，原因见限制条件），则根据
KKT 条件，肯定有 <span
class="math inline">1 + <em>ξ</em><sub><em>i</em></sub> − <em>y</em><sub><em>i</em></sub>[<em>W</em><sup><em>T</em></sup><em>ϕ</em>(<em>X</em><sub><em>i</em></sub>)+<em>b</em>] = 0</span>。又因为
<span
class="math inline"><em>β</em><sub><em>i</em></sub> + <em>α</em><sub><em>i</em></sub> = <em>C</em></span>，根据
KKT 条件，所以 <span
class="math inline"><em>β</em><sub><em>i</em></sub> ≠ 0</span>，<span
class="math inline"><em>h</em>(<em>W</em>) = <em>ξ</em><sub><em>i</em></sub> = 0</span>.
将 <span class="math inline"><em>ξ</em><sub><em>i</em></sub> = 0</span>
代入前式，就有： <span class="math display">$$
\begin{aligned}
&amp;1-y_i[W^T\phi(X_i)+b]=0\\
&amp;\Downarrow\text{to rearrange the terms}\\
&amp;b=\frac{1-y_iW^T\phi(X_i)}{y_i}\\
&amp;\Downarrow\text{to substitute
}W^T\phi(X)=\sum_{i=1}^N\alpha_iy_iK(X_i,X)\text{ into it}\\
&amp;b=\frac{1-y_i\sum_{i=1}^N\alpha_iy_iK(X_i,X)}{y_i}
\end{aligned}
$$</span> 于是，就连 <span class="math inline"><em>b</em></span>
我们也知道了。在现实中，我们一般会遍历所有 <span
class="math inline"><em>α</em><sub><em>i</em></sub> ∉ {0, <em>C</em>}</span>（在上面的讨论中我们只取了一个
<span class="math inline"><em>α</em></span>），然后计算 <span
class="math inline"><em>b</em></span>，最后取 <span
class="math inline"><em>b</em></span>
的平均值，这样能使结果更加精确。</p>
<h3 id="算法流程总结">2.5 算法流程总结</h3>
<h4 id="训练流程">训练流程</h4>
<ol type="1">
<li>输入：<span
class="math inline">{(<em>X</em><sub><em>i</em></sub>,<em>y</em><sub><em>i</em></sub>)}  <em>i</em> = 1 ∼ <em>N</em></span></li>
<li>求解优化问题（SMO 算法）：
<ol type="1">
<li>最大化 <span
class="math inline">$\theta(\alpha)=\sum_{i=1}^{N}\alpha_i-\frac12\sum_{i=1}^N\sum_{j=1}^N\alpha_i\alpha_jy_iy_jK(X_i,X_j)$</span></li>
<li>限制条件：
<ol type="1">
<li><span
class="math inline">0 ≤ <em>α</em><sub><em>i</em></sub> ≤ <em>C</em>  (<em>i</em>=1∼<em>N</em>)</span></li>
<li><span class="math inline">$\sum_{i=1}^N\alpha_iy_i=0$</span></li>
</ol></li>
</ol></li>
<li>通过上一步计算出来的 <span
class="math inline"><em>α</em><sub><em>i</em></sub></span> 来计算 <span
class="math inline"><em>b</em></span>：<span
class="math inline">$b=\frac{1-y_i\sum_{i=1}^N\alpha_iy_iK(X_i,X)}{y_i}$</span></li>
</ol>
<h4 id="测试流程">测试流程</h4>
<ol type="1">
<li>输入测试样本 <span class="math inline"><em>X</em></span></li>
<li>分类：
<ol type="1">
<li>若 <span
class="math inline">$\sum_{i=1}^N\alpha_iy_iK(X_i,X)+b\ge0$</span>，则
<span class="math inline"><em>y</em> =  + 1</span></li>
<li>若 <span
class="math inline">$\sum_{i=1}^N\alpha_iy_iK(X_i,X)+b\lt0$</span>，则
<span class="math inline"><em>y</em> =  − 1</span></li>
</ol></li>
</ol>
<blockquote>
<p>可以发现，最终训练流程和测试流程中完全不需要用到无限维的 <span
class="math inline"><em>ϕ</em>(<em>X</em>)</span>，只需要使用核函数就行了。这也就是
SVM 用有限维手段来处理无限维问题的方法。</p>
</blockquote>
<h2 id="补充优化理论">03* 补充：优化理论</h2>
<p>在优化领域中，在优化理论中，<strong>原问题（Prime
Problem）</strong>和<strong>对偶问题（Dual
Problem）</strong>是一对相关的数学问题。</p>
<p>原问题的定义如下：</p>
<ol type="1">
<li>目标：最小化 <span
class="math inline"><em>f</em>(<em>ω</em>)</span></li>
<li>限制条件：
<ol type="1">
<li><span
class="math inline"><em>g</em><sub><em>i</em></sub>(<em>ω</em>) ≤ 0  (<em>i</em>=1∼<em>K</em>)</span></li>
<li><span
class="math inline"><em>h</em><sub><em>i</em></sub>(<em>ω</em>) = 0  (<em>i</em>=1∼<em>M</em>)</span></li>
</ol></li>
</ol>
<p>原问题是非常普适化的，虽然上面展示的是最小化问题，但只需要在 <span
class="math inline"><em>f</em>(<em>ω</em>)</span>
前加一个负号，立马就变成了最大化问题；同样地，在 <span
class="math inline"><em>g</em><sub><em>i</em></sub>(<em>ω</em>) ≤ 0</span>
中加一个负号，也就变成了 <span
class="math inline"> − <em>g</em><sub><em>i</em></sub>(<em>ω</em>) ≥ 0</span>；而在式
2 的左边减去一个常数 <span
class="math inline"><em>C</em></span>，就立马变成了 <span
class="math inline"><em>h</em><sub><em>i</em></sub>(<em>ω</em>) − <em>C</em> = 0</span>，这样就可以把等式右边的
0 变成任意常数 <span class="math inline"><em>C</em></span>。</p>
<p>对偶问题是从原问题派生出来的一个新问题，对偶问题首先定义了一个函数：
<span class="math display">$$
\begin{aligned}
L(\omega,\alpha,\beta)&amp;=f(\omega)+\sum_{i=1}^{K}\alpha_ig_i(\omega)+\sum_{i=1}^M\beta_ih_i(\omega)\quad&amp;①\\
&amp;=f(\omega)+\alpha^Tg(\omega)+\beta^Th(\omega)\quad&amp;②
\end{aligned}
$$</span> 上式中，<span class="math inline"><em>α</em></span> 和 <span
class="math inline"><em>β</em></span> 是两个和 <span
class="math inline"><em>ω</em></span>
维数一样的向量，并且分别乘上了不等式的限制条件和等式的限制条件。式 ①
是该式的代数形式，式 ②
是该式的矩阵形式。有了这个函数，我们就可以给出对偶问题的定义了：</p>
<ol type="1">
<li>目标：最大化 <span
class="math inline">$\theta(\alpha,\beta)=\underset{\omega}{\inf}\{L(\omega,\alpha,\beta)\}$</span></li>
<li>限制条件：<span
class="math inline"><em>α</em><sub><em>i</em></sub> ≥ 0  (<em>i</em>=1∼<em>K</em>)</span></li>
</ol>
<p>解释一下这里的目标函数，<span class="math inline">inf </span>
就是求最小值的意思，下面的 <span class="math inline"><em>ω</em></span>
是指，遍历所有每个 <span class="math inline"><em>ω</em></span> 对应的
<span
class="math inline"><em>L</em>(<em>ω</em>,<em>α</em>,<em>β</em>)</span>
，所以 <span
class="math inline">$\underset{\omega}{\inf}\{L(\omega,\alpha,\beta)\}$</span>
就是指，求所有 <span class="math inline"><em>ω</em></span> 对应的 <span
class="math inline"><em>L</em>(<em>ω</em>,<em>α</em>,<em>β</em>)</span>
中，<span
class="math inline"><em>L</em>(<em>ω</em>,<em>α</em>,<em>β</em>)</span>
最小的取值。而通过 <span
class="math inline"><em>θ</em>(<em>α</em>,<em>β</em>)</span>
可以看出，<span class="math inline"><em>α</em></span> 和 <span
class="math inline"><em>β</em></span> 是固定的，也就是说，我们每确定一组
<span class="math inline"><em>α</em></span> 和 <span
class="math inline"><em>β</em></span>，就去求一次 <span
class="math inline"><em>L</em>(<em>ω</em>,<em>α</em>,<em>β</em>)</span>
的最小值，所以 <span class="math inline"><em>θ</em></span> 是只和 <span
class="math inline"><em>α</em></span> 和 <span
class="math inline"><em>β</em></span> 有关的函数。而我们的目标又是最大化
<span
class="math inline"><em>θ</em>(<em>α</em>,<em>β</em>)</span>，所以，实质上我们就是要使
<span
class="math inline"><em>L</em>(<em>ω</em>,<em>α</em>,<em>β</em>)</span>
的最小值最大化。而对偶问题的限制条件很简单，只要求每个 <span
class="math inline"><em>α</em><sub><em>i</em></sub></span> 大于 0
即可。</p>
<p>接下来我们就来介绍一下原问题和对偶问题的关系，有一条定理是这样的：</p>
<blockquote>
<p>如果 <span class="math inline"><em>ω</em><sup>*</sup></span>
是原问题的解，而 <span class="math inline"><em>α</em><sup>*</sup></span>
和 <span class="math inline"><em>β</em><sup>*</sup></span>
是对偶问题的解，则有 <span
class="math inline"><em>f</em>(<em>ω</em><sup>*</sup>) ≥ <em>θ</em>(<em>α</em><sup>*</sup>,<em>β</em><sup>*</sup>)</span>。</p>
</blockquote>
<p>这条定理的证明如下：</p>
<p>由于 <span class="math inline"><em>α</em><sup>*</sup></span> 和 <span
class="math inline"><em>β</em><sup>*</sup></span>
是对偶问题的解，则下式肯定成立： <span class="math display">$$
\theta(\alpha^*,\beta^*)=\underset{\omega}{\inf}\{L(\omega,\alpha^*,\beta^*\}\le
L(\omega^*,\alpha^*,\beta^*)
$$</span> 这里的 <span class="math inline"><em>ω</em><sup>*</sup></span>
是指一个具体的 <span class="math inline"><em>ω</em></span> 的值。根据
<span
class="math inline"><em>L</em>(<em>ω</em>,<em>α</em>,<em>β</em>)</span>
的定义，展开不等式右边的式子有： <span class="math display">$$
L(\omega^*,\alpha^*,\beta^*)=f(\omega^*)+\sum_{i=1}^{K}\alpha_i^*g_i(\omega^*)+\sum_{i=1}^M\beta_i^*h_i(\omega^*)
$$</span> 既然 <span class="math inline"><em>ω</em><sup>*</sup></span>
是原问题的解，那么 <span
class="math inline"><em>ω</em><sup>*</sup></span>
必然满足原问题的两个限制条件，也就是说 <span
class="math inline"><em>g</em><sub><em>i</em></sub>(<em>ω</em><sup>*</sup>) ≤ 0</span>，<span
class="math inline"><em>h</em><sub><em>i</em></sub>(<em>ω</em><sup>*</sup>) = 0</span>；另外，既然
<span class="math inline"><em>α</em><sup>*</sup></span>
是对偶问题的解，那么 <span
class="math inline"><em>α</em><sup>*</sup></span> 也必然满足 <span
class="math inline"><em>α</em><sup>*</sup> ≥ 0</span>。进一步，既然
<span
class="math inline"><em>h</em><sub><em>i</em></sub>(<em>ω</em><sup>*</sup>) = 0</span>，那么上式中
<span
class="math inline">$\sum_{i=1}^M\beta_i^*h_i(\omega^*)=0$</span>；既然
<span
class="math inline"><em>g</em><sub><em>i</em></sub>(<em>ω</em><sup>*</sup>) ≤ 0</span>，<span
class="math inline"><em>α</em><sup>*</sup> ≥ 0</span>，那么上式中 <span
class="math inline">$\sum_{i=1}^{K}\alpha_i^*g_i(\omega^*)\le0$</span>，所以存在：
<span class="math display">$$
\theta(\alpha^*,\beta^*)=\underset{\omega}{\inf}\{L(\omega,\alpha^*,\beta^*\}\le
L(\omega^*,\alpha^*,\beta^*)\le f(\omega^*)
$$</span> 证毕。</p>
<p>遂定义： <span
class="math display"><em>G</em> = <em>f</em>(<em>ω</em><sup>*</sup>) − <em>θ</em>(<em>α</em><sup>*</sup>,<em>β</em><sup>*</sup>) ≥ 0</span>
<span class="math inline"><em>G</em></span>
叫做原问题与对偶问题的<strong>间距（Duality
Gap）</strong>。对应某些特定的优化问题，可以证明 <span
class="math inline"><em>G</em> = 0</span>.
这里不再证明，直接给出问题的结论——<strong>强对偶定理</strong>：</p>
<blockquote>
<p>若 <span class="math inline"><em>f</em>(<em>ω</em>)</span>
为凸函数，且 <span
class="math inline"><em>g</em>(<em>ω</em>) = <em>A</em><em>ω</em> + <em>b</em></span>（线性函数），<span
class="math inline"><em>h</em>(<em>ω</em>) = <em>C</em><em>W</em> + <em>d</em></span>（一组线性函数），则此优化问题的原问题和对偶问题的间距是
0，即 <span
class="math inline"><em>f</em>(<em>ω</em><sup>*</sup>) = <em>θ</em>(<em>α</em><sup>*</sup>,<em>β</em><sup>*</sup>)</span>。</p>
</blockquote>
<p>问题是，强对偶定理的前提成立意味着什么？假设现在原问题和对偶问题满足强对偶定理，即
<span
class="math inline"><em>f</em>(<em>ω</em><sup>*</sup>) = <em>θ</em>(<em>α</em><sup>*</sup>,<em>β</em><sup>*</sup>)</span>
成立，那么就有 <span
class="math inline">$f(\omega^*)=\theta(\alpha^*,\beta^*)=\underset{\omega}{\inf}\{L(\omega,\alpha^*,\beta^*\}$</span>，也就是说，<u>原问题的解
<span
class="math inline"><em>ω</em><sup>*</sup></span>，刚刚就是对偶问题在
<span class="math inline"><em>α</em><sup>*</sup></span> 和 <span
class="math inline"><em>β</em><sup>*</sup></span>
确定时，取到最小值的那个点</u>。</p>
<p>更加精妙的是，当 <span class="math inline"><em>G</em> = 0</span>
成立时，<span
class="math inline">$\sum_{i=1}^{K}\alpha_i^*g_i(\omega^*)+\sum_{i=1}^M\beta_i^*h_i(\omega^*)=0$</span>，其中，<span
class="math inline">$\sum_{i=1}^M\beta_i^*h_i(\omega^*)$</span> 等于 0
不用再说了，但 <span
class="math inline">$\sum_{i=1}^{K}\alpha_i^*g_i(\omega^*)=0$</span>
意味着，<u><span
class="math inline">∀<em>i</em> = 1 ∼ <em>K</em></span>，要么 <span
class="math inline"><em>α</em><sub><em>i</em></sub><sup>*</sup> = 0</span>，要么
<span
class="math inline"><em>g</em><sub><em>i</em></sub>(<em>ω</em><sup>*</sup>) = 0</span></u>。这个条件叫做
<strong>KKT 条件</strong>。</p>
	  
	</div>

	<!-- recommended posts -->
	

	<div>
  	<center>
	<div class="pagination">
<ul class="pagination">
	 
				
		<li class="prev"><a href="/2023/12/11/%E6%A0%B7%E6%9D%A1%E8%A1%A8%E7%A4%BA/" class="alignleft prev"><i class="fa fa-arrow-circle-o-left"></i>Prev</a></li>
  		

        <li><a href="/archives"><i class="fa fa-archive"></i>Archive</a></li>

		
          <li class="next"><a href="/2023/12/11/%E4%BA%8C%E7%BB%B4%E5%8F%98%E6%8D%A2/" class="alignright next">Next<i class="fa fa-arrow-circle-o-right"></i></a></li>
        
	
</ul>
</div>

    </center>
	</div>

    <!-- share -->
    
        
    <div class="bdsharebuttonbox">
        <a href="#" class="bds_more" data-cmd="more"></a>
        <a href="#" class="bds_weixin" data-cmd="weixin" title="分享到微信"></a>
        <a href="#" class="bds_tsina" data-cmd="tsina" title="分享到新浪微博"></a>
        <a href="#" class="bds_fbook" data-cmd="fbook" title="分享到Facebook"></a>
        <a href="#" class="bds_twi" data-cmd="twi" title="分享到Twitter"></a>
        <a href="#" class="bds_linkedin" data-cmd="linkedin" title="分享到linkedin"></a>
        <a href="#" class="bds_evernotecn" data-cmd="evernotecn" title="分享到印象笔记"></a>
        <a href="#" class="bds_youdao" data-cmd="youdao" title="分享到有道云笔记"></a>
        <a href="#" class="bds_copy" data-cmd="copy" title="分享到复制网址"></a>
    </div>
    <script>
        window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"2","bdMiniList":false,"bdPic":"","bdStyle":"1","bdSize":"24"},"share":{}};
        with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];
    </script>


        

    
	
	<!-- comment -->
	
<section id="comment">
  <h2 class="title">Comments</h2>
  
</section>


	</div> <!-- col-md-9/col-md-12 -->
		
	
	<div id="side_meta">
		<div class="col-md-3" id="post_meta"> 

	<!-- date -->
	
	<div class="meta-widget">
	<i class="fa fa-clock-o"></i>
	2023-12-11 
	</div>
	

	<!-- categories -->
    
	<div class="meta-widget">
	<a data-toggle="collapse" data-target="#categorys"><i class="fa fa-folder"></i></a>	
    <ul id="categorys" class="tag_box list-unstyled collapse in">
          
  <li>
    <li><a href="/categories/Machine-Learning/">Machine Learning<span>2</span></a></li>
  </li>

    </ul>
	</div>
	

	<!-- tags -->
	
	<div class="meta-widget">
	<a data-toggle="collapse" data-target="#tags"><i class="fa fa-tags"></i></a>		  
    <ul id="tags" class="tag_box list-unstyled collapse in">	  
	    
  <li><a href="/tags/machine-learning/">machine learning<span>6</span></a></li>

    </ul>
	</div>
	

	<!-- toc -->
	<div class="meta-widget">
	
	   <a data-toggle="collapse" data-target="#toc"><i class="fa fa-bars"></i></a>
	   <div id="toc" class="toc collapse in">
			<ol class="toc-article"><li class="toc-article-item toc-article-level-2"><a class="toc-article-link" href="#%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B"><span class="toc-article-text">01 线性模型</span></a><ol class="toc-article-child"><li class="toc-article-item toc-article-level-3"><a class="toc-article-link" href="#%E7%AE%97%E6%B3%95%E6%80%9D%E8%B7%AF"><span class="toc-article-text">1.1 算法思路</span></a></li><li class="toc-article-item toc-article-level-3"><a class="toc-article-link" href="#%E6%95%B0%E5%AD%A6%E6%8F%8F%E8%BF%B0"><span class="toc-article-text">1.2 数学描述</span></a></li></ol></li><li class="toc-article-item toc-article-level-2"><a class="toc-article-link" href="#%E9%9D%9E%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B"><span class="toc-article-text">02 非线性模型</span></a><ol class="toc-article-child"><li class="toc-article-item toc-article-level-3"><a class="toc-article-link" href="#%E4%BC%98%E5%8C%96%E7%9B%AE%E6%A0%87"><span class="toc-article-text">2.1 优化目标</span></a></li><li class="toc-article-item toc-article-level-3"><a class="toc-article-link" href="#%E9%AB%98%E7%BB%B4%E6%98%A0%E5%B0%84"><span class="toc-article-text">2.2 高维映射</span></a></li><li class="toc-article-item toc-article-level-3"><a class="toc-article-link" href="#%E6%A0%B8%E5%87%BD%E6%95%B0"><span class="toc-article-text">2.3 核函数</span></a></li><li class="toc-article-item toc-article-level-3"><a class="toc-article-link" href="#%E5%8E%9F%E9%97%AE%E9%A2%98%E5%88%B0%E5%AF%B9%E5%81%B6%E9%97%AE%E9%A2%98"><span class="toc-article-text">2.4 原问题到对偶问题</span></a></li><li class="toc-article-item toc-article-level-3"><a class="toc-article-link" href="#%E7%AE%97%E6%B3%95%E6%B5%81%E7%A8%8B%E6%80%BB%E7%BB%93"><span class="toc-article-text">2.5 算法流程总结</span></a><ol class="toc-article-child"><li class="toc-article-item toc-article-level-4"><a class="toc-article-link" href="#%E8%AE%AD%E7%BB%83%E6%B5%81%E7%A8%8B"><span class="toc-article-text">训练流程</span></a></li><li class="toc-article-item toc-article-level-4"><a class="toc-article-link" href="#%E6%B5%8B%E8%AF%95%E6%B5%81%E7%A8%8B"><span class="toc-article-text">测试流程</span></a></li></ol></li></ol></li><li class="toc-article-item toc-article-level-2"><a class="toc-article-link" href="#%E8%A1%A5%E5%85%85%E4%BC%98%E5%8C%96%E7%90%86%E8%AE%BA"><span class="toc-article-text">03* 补充：优化理论</span></a></li></ol>
		</div>
	
	</div>
	
    <hr>
	
</div><!-- col-md-3 -->

	</div>
		

</div><!-- row -->




    </div>
  </div>
  <div class="container-narrow">
    <footer> <!--<p>
  &copy; 2025 me
  
      with help from <a href="http://hexo.io/" target="_blank">Hexo</a> and <a href="http://getbootstrap.com/" target="_blank">Twitter Bootstrap</a>. Theme by <a target="_blank" rel="noopener" href="http://github.com/wzpan/hexo-theme-freemind/">Freemind</a>.    
</p>-->
 </footer>
  </div> <!-- container-narrow -->
  


  
<a id="gotop" href="#">   
  <span>▲</span> 
</a>

<script src="/js/jquery.imagesloaded.min.js"></script>
<script src="/js/gallery.js"></script>
<script src="/js/bootstrap.min.js"></script>
<script src="/js/main.js"></script>
<script src="/js/search.js"></script>


<link rel="stylesheet" href="/fancybox/jquery.fancybox.css" media="screen" type="text/css">
<script src="/fancybox/jquery.fancybox.pack.js"></script>
<script type="text/javascript">
(function($){
  $('.fancybox').fancybox();
})(jQuery);
</script>



   <script type="text/javascript">      
     var search_path = "search.xml";
	 if (search_path.length == 0) {
	 	search_path = "search.xml";
	 }
	 var path = "/" + search_path;
     searchFunc(path, 'local-search-input', 'local-search-result');
   </script>


<!-- syntax highlighting -->


<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</body>
	<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</html>