{"meta":{"title":"Jiahao Peng","subtitle":"Hello & Welcome!","description":"","author":"me","url":"https://example.com","root":"/"},"pages":[{"title":"About","date":"2025-04-08T09:00:38.895Z","updated":"2025-04-08T09:00:38.895Z","comments":true,"path":"about/index.html","permalink":"https://example.com/about/index.html","excerpt":"","text":"I graduated from HUZU with a bachelor's degree majoring in computer science and technology, and was admitted to ECNU as an academic master's student of software engineering in 2025. Although I didn't do well in the college entrance exam, I found a suitable study method in college and got into graduate school, thanks to personal effort and the teachers at ECNU who didn't have any prejudices because of my undergraduate background. My mentor's main research area is software testing, so most of the articles in this blog in the future will probably be centred around this area. I will be very happy if something I write helps you! Feel free to let me know if there are any errors in my posts by sending an email to hibiya163@163.com."},{"title":"Tags","date":"2025-04-07T17:07:06.202Z","updated":"2025-04-07T17:07:06.202Z","comments":true,"path":"tags/index.html","permalink":"https://example.com/tags/index.html","excerpt":"","text":""},{"title":"Categories","date":"2025-04-07T17:06:51.838Z","updated":"2025-04-07T17:06:51.838Z","comments":true,"path":"categories/index.html","permalink":"https://example.com/categories/index.html","excerpt":"","text":""}],"posts":[{"title":"A WSL Error Encountered When Installing Docker","slug":"A-WSL-Error-Encountered-When-installing-Docker","date":"2025-04-05T14:30:36.000Z","updated":"2025-04-12T11:00:17.356Z","comments":true,"path":"2025/04/05/A-WSL-Error-Encountered-When-installing-Docker/","permalink":"https://example.com/2025/04/05/A-WSL-Error-Encountered-When-installing-Docker/","excerpt":"This document explains the problems the author had installing Docker and how to solve them. You can find and download the installation package for Docker here. After installing Docker, an Unexpected WSL error was encountered, and the process was terminated. After conducting some online research, I discovered that this error message was indicating that I needed to enable the Hyper-V, Windows Subsystem for Linux and Virtual Machine Platform functions on my system. I opened the configuration panel and found that the latter two functions had been properly enabled, but the option for the first one was missing. It took me some time to find the solution.","text":"This document explains the problems the author had installing Docker and how to solve them. You can find and download the installation package for Docker here. After installing Docker, an Unexpected WSL error was encountered, and the process was terminated. After conducting some online research, I discovered that this error message was indicating that I needed to enable the Hyper-V, Windows Subsystem for Linux and Virtual Machine Platform functions on my system. I opened the configuration panel and found that the latter two functions had been properly enabled, but the option for the first one was missing. It took me some time to find the solution. To address the issue, it was necessary to create and run a file named Hyper-V.bat as administrator, which contains the following content: 12345pushd &quot;%~dp0&quot;dir /b %SystemRoot%\\servicing\\Packages\\*Hyper-V*.mum &gt;hyper-v.txtfor /f %%i in (&#x27;findstr /i . hyper-v.txt 2^&gt;nul&#x27;) do dism /online /norestart /add-package:&quot;%SystemRoot%\\servicing\\Packages\\%%i&quot;del hyper-v.txtDism /online /enable-feature /featurename:Microsoft-Hyper-V-All /LimitAccess /ALL The program will then require you to restart your machine, at which point you will see that Hyper-V is properly enabled. But Docker still cannot run normally. After that, you need to install WSL2. To do this, run the following command: 123wsl --install # may require VPNwsl --set-default-version 2 # set the default wsl version to wsl2wsl --update # may not be necessary Finally, Docker can run without any problems after all this work. Off topic: When using the VSCode extension Remote Containers to set up containers, you might encounter the message “the container does not meet all the requirements of the VS Code Server”. This happens because VSCode has increased the minimum requirements for remote server build toolchain since version 1.86. To solve the problem, just downgrade your VSCode to a version below 1.86. You can download version 1.85.2 here. Besides, it is necessary to downgrade your extensions as well.","categories":[{"name":"Debugging","slug":"Debugging","permalink":"https://example.com/categories/Debugging/"}],"tags":[{"name":"docker","slug":"docker","permalink":"https://example.com/tags/docker/"},{"name":"error","slug":"error","permalink":"https://example.com/tags/error/"},{"name":"vscode","slug":"vscode","permalink":"https://example.com/tags/vscode/"}]},{"title":"Introduction to Program Analysis","slug":"Introduction-to-Program-Analysis","date":"2025-04-04T13:58:30.000Z","updated":"2025-04-09T14:36:11.032Z","comments":true,"path":"2025/04/04/Introduction-to-Program-Analysis/","permalink":"https://example.com/2025/04/04/Introduction-to-Program-Analysis/","excerpt":"01 What Is Program Analysis Program analysis is to discover useful facts about programs. You probably have known some manual or automated testing tools like: Manual testing or semi-automated testing: JUnit, Selenium, etc. Manual “analysis” of programs: Code inspection, debugging, etc. The focus of this course is automated program analysis.","text":"01 What Is Program Analysis Program analysis is to discover useful facts about programs. You probably have known some manual or automated testing tools like: Manual testing or semi-automated testing: JUnit, Selenium, etc. Manual “analysis” of programs: Code inspection, debugging, etc. The focus of this course is automated program analysis. Program analysis can be broadly classified into three kinds: Static (compile-time) Infer facts by inspecting source or binary code Typically: Consider all inputs Overapproximate possible behavior E.g. compilers, lint-like tools Dynamic (execution-time) Infer facts by monitoring program executions Typically: Consider current input Underapproximate possible behavior E.g. automated testing tools, profilers Hybrid (combining dynamic and static) 02 Terminology The following is a snippet of JavaScript code. 1234567var r = Math.random(); //value in [0,1)var out = &quot;yes&quot;;if(r &lt; 0.5) out = &quot;no&quot;;if(r == 1) out = &quot;maybe&quot;;console.log(out); Q: What are the possible outputs? 2.1 Overapproximation v.s. Underapproximation Judging from the static code, it seems that there are three possible outputs: “yes”, “no” or “maybe”. (Overapproximation) If we consider the case of only one execution like r=0.7, its output is “yes”. (Underapproximation) However, both responses are erroneous. The first option yields the implausible output “maybe”, while the second excludes the feasible output “no”. These erroneous responses serve as quintessential illustrations of over- and under-approximation, respectively. Overapproximation: Consider all paths Underapproximation: Execute the program once 2.2 Soundness &amp; Completeness It is easy for us humans to give the right answer —— “yes” or “no”. We think these answers are sound and complete. “Soundness” means it contains all the possible outputs we want (might give false positives). “Completeness” means it excludes all the impossible outputs we do not want (might give false negtives). When we put these two ideas together, we get a definition that includes exactly all possible outputs. 2.3 False Positives &amp; False Negatives The definitions of false positives and false negatives: False positives: impossible outputs that are indicated possible False negatives: possible outputs that are indicated impossible Let P be Program, i be Input, P(i) be Behavior. The following graph shows the relations between the above ideas. 2.4 Precision &amp; Recall Differentiate precision and recall: Precision: how many retrieved items are relevant Recall: how many relevant items are retrieved Take the overapproximated answer aforementioned for instance, the precision and the recall are: $$ \\mathrm{precision}=\\frac{2}{3}=0.67 $$ $$ \\mathrm{recall}=\\frac{1}{2}=0.5 $$ 2.5 Program Invariants Program Invariants are logical assertions whose certain conditions or properties remain true throughout the execution of a program. These invariants are key to program correctness. They help verify that the program behaves as expected and play an important role in software development. See the below code snippet: 12345678int p(int x) &#123; return x * x; &#125;void main() &#123; int z; if (getc() == &#x27;a&#x27;) z = p(6) + 6; else z = p(-7) - 7;&#125; Q: An invariant at the end of the program is (z == c) for some constant c. What is c? Clearly, the z will yield 42 regardless of any inputs. Therefore, (z == 42) is definitely an invariant, while (z == 30) is definitely not an invariant. Using the invariant to avert disaster: 1234567891011int p(int x) &#123; return x * x; &#125;void main() &#123; int z; if (getc() == &#x27;a&#x27;) z = p(6) + 6; else z = p(-7) - 7; if (z != 42) &#123; disaster(); // disaster averted &#125;&#125; 03 Others 3.1 Undecidability of Program Properties Q: Can program analysis be sound and complete? A: Not if we want it to terminate! Questions like “is a program point reachable on some input?” are undecidable. Designing a program analysis is an art —— tradeoffs dictated by consumer. 3.2 Why Take This Course? Learn methods to improve software quality, reliability, security, performance, etc. Become a better software developer/tester Build specialized tools for software analysis, testing and verification Finding Jobs &amp; Do research 3.3 Who Needs Program Analysis? Three primary consumers of program analysis: Compilers Software Quality Tools (Primary focus of this course) Integrated Development Environments (IDEs) 3.3.1 Compilers Program analysis serves as the bridge between high-level languages and architectures. For example, we use program analysis to generate efficient code. Before: 123456789int p(int x) &#123; return x * x; &#125;void main(int arg) &#123; int z; if (arg != 0) z = p(6) + 6; else z = p(-7) - 7; print (z);&#125; After: 1234int p(int x) &#123; return x * x; &#125;void main() &#123; print (42);&#125; 3.3.2 Software Quality Tools Software quality tools are tools for testing, debugging, and verification. Software quality tools use program analysis for: Finding programming errors Proving program invariants Generating test cases Localizing causes of errors … Some software quality tools: Static Program Analysis Suspicious error patterns: Lint, SpotBugs, Coverity Memory leak detection: Facebook Infer Checking API usage rules: Microsoft SLAM Verifying invariants: ESC/Java Dynamic Program Analysis Array bound checking: Purify Datarace detection: Eraser Memory leak detection: Valgrind Finding likely invariants: Daikon 3.3.3 Integrated Development Environments Examples: Eclipse and VS Code Use program analysis to help programmers: Understand programs Refactor programs Restructuring a program without changing its behavior Useful in dealing with large, complex programs 04 Quiz Dynamic vs. Static Analysis: Dynamic Static Cost Proportional to program’s execution time Proportional to program’s size Effectiveness Unsound (may miss errors) Incomplete (may report spurious errors) Unsoundness yields false negatives; incompleteness yields false positives.","categories":[{"name":"【Lecture】Software Analysis Testing and Verification","slug":"【Lecture】Software-Analysis-Testing-and-Verification","permalink":"https://example.com/categories/%E3%80%90Lecture%E3%80%91Software-Analysis-Testing-and-Verification/"}],"tags":[{"name":"program analysis","slug":"program-analysis","permalink":"https://example.com/tags/program-analysis/"}]},{"title":"ECNU软学复试总结","slug":"ECNU软学复试总结","date":"2025-03-23T03:46:26.000Z","updated":"2025-04-12T06:29:45.906Z","comments":true,"path":"2025/03/23/ECNU软学复试总结/","permalink":"https://example.com/2025/03/23/ECNU%E8%BD%AF%E5%AD%A6%E5%A4%8D%E8%AF%95%E6%80%BB%E7%BB%93/","excerpt":"现在是25年3月23日晚上，研究生复试正式结束！趁着大脑里还留有复试的余温（或者说余震？），复盘一下整个复试历程。 1 复试结束后的感想 我的考研经历用八个字概括就是“剑走偏锋，有惊无险”。我在24年上半年还在忙项目和比赛，正式的考研复习其实拖到6月份才开始，所以满打满算我只准备了半年。因为时间很仓促，所以我用的是速成法，i.e. 快速过完基础和强化阶段的知识点，然后开始集中式刷题。24年的暑假大概是我学习强度最高的一段时间，拼了命地赶数学和408的进度，最后成功在暑假期间结束了数学和408的一轮复习（还是建议各位能早点开始就早点开始，数学+408+英一按理说压力是比较大的，我能这么做的基础是我英语好且专业课基础好）。中间我刷了近十年的英二真题，觉得太简单了就换了英一，挑战ECNU软件工程学硕。我选择ECNU软学的原因：","text":"现在是25年3月23日晚上，研究生复试正式结束！趁着大脑里还留有复试的余温（或者说余震？），复盘一下整个复试历程。 1 复试结束后的感想 我的考研经历用八个字概括就是“剑走偏锋，有惊无险”。我在24年上半年还在忙项目和比赛，正式的考研复习其实拖到6月份才开始，所以满打满算我只准备了半年。因为时间很仓促，所以我用的是速成法，i.e. 快速过完基础和强化阶段的知识点，然后开始集中式刷题。24年的暑假大概是我学习强度最高的一段时间，拼了命地赶数学和408的进度，最后成功在暑假期间结束了数学和408的一轮复习（还是建议各位能早点开始就早点开始，数学+408+英一按理说压力是比较大的，我能这么做的基础是我英语好且专业课基础好）。中间我刷了近十年的英二真题，觉得太简单了就换了英一，挑战ECNU软件工程学硕。我选择ECNU软学的原因： 学科实力，ECNU软件工程学科评估为A，全国排名第四； 考试科目为12408，可以充分发挥我英语的优势； 学硕的机试占比相比专硕低，ECNU的机试是出了名的难，我并没有打过ACM，所以打算避开这点； 我的项目经历和竞赛成绩还算丰富，四六级分数也较高，复试并非完全没有优势； ECNU不歧视本科出身。 暑假里我学得还是比较认真的，但是之后就开始走下坡路了……自从大四上开学之后，我每天都是睡到自然醒（大概早上十点多这个样子），吃完中饭就去学习，学到晚上十点回寝，将近一点钟入眠。说实话这个考研过程完全算不上刻苦（朋友说我有一种该死的松弛感），这种情况下我就会说重要的是效率而不是时间☝️。 初试考完后感觉数学还行，408很糟糕，实际上刚刚好反一下，数学考得很一般，408反而还不错（感觉这辈子都学不会数学了……）。好在今年英一是近十年最难的一次，但我考得蛮高的，英语弥补了我数学上的弱势，充分证明了跳车英一的选择是多么明智。本来我觉得自己是没机会的，工作都找好了，结果我以去年的录取平均分进入了复试名单，具体情况是招15进18排10，算是一个比较危险的名次。于是我火速辞职回来准备复试。 ECNU软学的复试压力是比较大的，初复试比为6:4，复试分为机试笔试面试。笔试占复试的大头，而且考的是四本计算机经典大黑书，每本都有八百页。其中，我本科学习的离散数学是阉割版，不全面，需要恶补，软工更是整本书没学过，而且学院还不划考试范围，让我在准备复试时焦头烂额，最后我只能找来了一些ECNU本科生的授课课件和期末考卷，然后对照着进行复习。复试之后的感受是： 【由于保密协议，该部分内容将在五月后更新】 最终我的复试成绩排名5/18，还算不错，估计是面试分比较高。我的初试成绩排10/18，按6:4加上复试成绩后就变成了6/18，总共录取15人，也算达到中上水平。 下面是具体的复试细节。 2 分数构成 学硕复试的分数构成较为复杂，主要分为笔试、专业知识考察、英语面试，其中： 笔试卷面分满分100分，考察科目为算法、OS、软工和离散，每个科目25分 专业知识考察总分=40%×机试分+60%×面试分，其中：机试共3道题，每题100分共300分，按样例给分，最后会根据排名进行赋分，满分100分；专业面试共100分，包括PPT个人介绍和专业知识提问。 英语面满分100分，包括文献翻译和专业知识提问，与专业面一起进行。 最终复试分=笔试*2+专业知识考察*2+英语面=笔试*2+(40%×机试分+60%×面试分)*2+英语面 3 笔试 【由于保密协议，该部分内容将在五月后更新】 4 机试 【由于保密协议，该部分内容将在五月后更新】 5 面试 【由于保密协议，该部分内容将在五月后更新】 6 写在最后 回顾一下考研的日子，说不辛苦实在有点“忘本”。考研难不难其实都取决于最终结果，考不上的自然说难。我从一个学院本一战考上985，完成了一个二本生的逆袭，虽说心里明白985研究生比不过985/211本科生，但我仍想在这份平庸的人生里找到一个为自己骄傲的理由，而这就是我考研的动力。 我考研的初衷很纯粹，就只是想去更好的平台看看。大学四年，我没有一刻感觉自己真的在上大学，老师上课没有动力，学生听课没有动力，作业、实验、实践活动都是充数。当然这不能怪老师，毕竟确实认真学习的人太少，费太多心思在教学上没有性价比可言。我希望去更好的平台看看，看那里的老师是怎么上课的，看更加优秀的人是怎么学习、怎么思考、怎么为人处世的，这是我考研的初衷。 感谢我的朋友们陪我一起去上海复试。复试结束的那个晚上，我们一起在陆家嘴的岸边欣赏黄浦江对岸的璀璨建筑，那是我那段时间里最放松、最惬意的时光。 陆家嘴黄浦江边 毕真烤肉，强烈推荐","categories":[{"name":"人生总结","slug":"人生总结","permalink":"https://example.com/categories/%E4%BA%BA%E7%94%9F%E6%80%BB%E7%BB%93/"}],"tags":[{"name":"生活","slug":"生活","permalink":"https://example.com/tags/%E7%94%9F%E6%B4%BB/"}]},{"title":"离散数学","slug":"离散数学","date":"2025-03-18T06:16:34.000Z","updated":"2025-04-10T06:46:25.978Z","comments":true,"path":"2025/03/18/离散数学/","permalink":"https://example.com/2025/03/18/%E7%A6%BB%E6%95%A3%E6%95%B0%E5%AD%A6/","excerpt":"01 逻辑和证明 1.1 命题逻辑 相关概念：命题、原子命题、命题变元、命题真值、逻辑运算符（否定、合取、析取、条件、双条件）、真值表、永真式、矛盾式、可能式 逻辑等价式：1.证明逻辑表达式等价：真值表相同；构造逻辑等价式。2.若 p ↔︎ q 是永真式，则 p ≡ q。3.德·摩根律、分配律、结合律。","text":"01 逻辑和证明 1.1 命题逻辑 相关概念：命题、原子命题、命题变元、命题真值、逻辑运算符（否定、合取、析取、条件、双条件）、真值表、永真式、矛盾式、可能式 逻辑等价式：1.证明逻辑表达式等价：真值表相同；构造逻辑等价式。2.若 p ↔︎ q 是永真式，则 p ≡ q。3.德·摩根律、分配律、结合律。 重要逻辑等价式：p → q ≡ ¬p ∨ q；p ↔︎ q ≡ p → q ∧ q → p ≡ (p∧q) ∨ (¬p∧¬q)。 对偶（dual）：将命题公式 A 中的全部 ∨ 换成 ∧，∧ 换成 ∨，F 换成 T，T 换成 F，得到其对偶命题 A*。 对偶定理： ¬A(P1,P2,…,Pn) ⇔ A*(¬P1,¬P2,…,¬Pn)；A(¬P1,¬P2,…,¬Pn) ⇔ ¬A*(P1,P2,…,Pn) 若 A ⇔ B，则 A* ⇔ B* 范式：析取范式、合取范式、主析取范式、主合取范式 命题的可满足性：如果一个命题存在一个赋值为真，一个复合命题是可满足的（相容），不存在就是不可满足的（不相容）。 1.2 谓词逻辑 相关概念：命题函数、谓词、量词（全称、存在、唯一性）、论域、约束论域、量化表达式的否定 量词的德·摩根律： ¬∀xP(x) ≡ ∃x¬P(x) ¬∃xQ(x) ≡ ∀x¬Q(x) 语句到逻辑表达式的翻译：定义命题函数、定义论域、给出逻辑表达式 1.3 嵌套量词 嵌套量词即出现在其他量词的作用域中的量词，比如：∀x∃y(x+y=0)。 量词的顺序会对量化式的含义造成影响，但如果全是全称或存在量词，那就没有影响。 ∃y∀xQ(x,y) 为真，则 ∀x∃yQ(x,y) 为真，但反过来不一定。 会翻译和使用含有嵌套量词的量化式。 嵌套量词的否定：可以通过将否定词按否定规则（量词的德摩根律）依次移入所有量词里来得到。 1.4 推理规则 1.4.1 命题逻辑的推理规则 证明一个命题正确的方法： 真值表法 等价演算法 演绎推理法：P 规则、E 规则、T 规则 附加前提法（有两种，第二种也叫 CP 规则）： 当要证明 A ⇒ B 时，即证明 A, ¬B ⇒ 0，也就是将结论否定，加入前提，证明其为矛盾式。 要证明当 A 成立时，从 B 能推出 C，即 A ⇒ B → C，可以转换为证明当 A、B 都成立时，C 成立，即 A, B ⇒ C，话句话说，就是将结论中的一部分拿出来当作前提。 假言推理（分离规则）：(p∧(p→q)) → q。这是一个永真式，它是假言推理的基础。 假言三段论（递推）、附加律（p → p ∨ q）、化简律（p ∧ q → p）、取拒式（若 p → q 且 ¬q，则 ¬p）、析取三段论（若 p ∨ q 且 ¬p，则 q）、消解律（p发生或者q发生，p不发生或者r发生，这两个前提恒成立，则q和r中至少有一个发生） 1.4.2 量化命题的推理规则 全称实例（UI规则）：若 ∀xP(x) 成立且 c 是该论域内的一个元素，则 P(c) 成立。 全称引入（UG规则）：若对某一论域内的任意 c，P(c) 均成立，则 ∀xP(x) 成立。 存在实例（EI规则）：若 ∃xP(x) 成立且 c 是该论域内满足要求的一个元素，则 P(c) 成立。 存在引入（EG规则）：若某一论域内的某个 c 使 P(c) 成立，则 ∃xP(x) 成立。 1.4.3 间接证明 反证法是一种重要的间接证明方式：要证明 s → c 是永真式，即证明其逆否命题 ¬c → ¬s 是永真式，其逆否命题也可以写做 c ∨ ¬s，其否定形式就是 ¬c ∧ s，若能证明 ¬c ∧ s 为矛盾式，则证明 s → c 为永真式。 02 集合、函数、基数、矩阵 2.1 集合 定义集合的方法： 花名册法：列出集合中全部的元素 集合构造器：通过描述成员的性质来构造集合 文氏图 注意：空集表示为 {} 或 ∅，但 {∅} 不是空集！ 两个集合相等：当且仅当两个集合 A 和 B 中的元素都相等时，A 和 B 相等，即 ∀x(x∈A↔︎x∈B)。 2.1.1 子集 A 是 B 的子集写做 A ⊆ B，当且仅当 ∀x(x∈A→x∈B) 成立 空集是任何集合的子集，一个集合本身是子集的子集 A 是 B 的真子集写做 A ⊊ B，当且仅当 ∀x(x∈A→x∈B) ∧ ∃x(x∈B∧x∉A) 成立 基数：集合 S 中不重复元素的个数称为 S 的基数，记作 |S| 幂集：S 的幂集是 S 所有子集构成的集合，记作 𝒫(S)，一个有 n 个元素的集合的幂集的基数为 2n（注意：幂集中包括一个空集 ∅） 2.1.2 笛卡尔积 有序 n 元组 笛卡尔积：集合 A 和 B 的笛卡尔积用 A × B 来表示，定义为 A × B = {(a,b)|a ∈ A ∧ b ∈ B}，其中 (a,b) 称为序偶 n 个集合的笛卡尔积：A1 × A2 × … × An = {(a1,a2,…,an)|ai ∈ Ai, for i = 1, 2, …, n} 2.1.3 真值集 给定谓词 P 和论域 D，定义 P 的真值集为 D 中使 P(x) 为真的元素 x 组成的集合。 P(x) 的真值集记为 {x ∈ D|P(x)}。 当且仅当 P 的真值集为 U 时，∀xP(x) 在论域 U 上为真。 当且仅当 P 的真值集非空时，∃xP(x) 在论域 U 上为真。 2.2 集合运算 2.2.1 运算 并 交 不相交（交集为空） 容斥原理：|A∪B| = |A| + |B| − |A∩B| 差 补：令 A 的全集为 U，则 A 关于 U 的补集为 $\\overline A=U-A$，也可记作 ∼ A 对称差：A 和 B 的对称差是指 A 和 B 含有的但并不共同含有的元素构成的集合，记作 A ⊕ B = (A−B) ∪ (B−A) 2.2.2 集合恒等式 重点关注分配律、德摩根律、吸收律、互补律 证明集合相等： 证明集合互为对方的子集； 使用集合构造器和逻辑等价式； 使用成员表（可以把 ∪ 和 ∩ 分别看作 ∨ 和 ∧，然后就直接构造真值表）； 使用集合恒等式推导。 2.3 函数 定义域、陪域、值域、像、原像 定义：(f1+f2)(x) = f1(x) + f2(x)、f1f2(x) = f1(x)f2(x) 单射(一对一)函数：满足一对一关系的函数，严格单调递增和严格单调递减的函数一定是单射函数。 映上(满射)函数：陪域中的每一个元素都能对应到定义域中的一个元素，称为映上函数（也可以理解为陪域和值域相等）。 双射(一一对应)函数：一个函数既是单射函数又是满射函数，则称为双射函数。 恒等函数：f(x) = x 反函数：只有单射函数和映上函数才有反函数。 复合函数：(f◦g)(a) = f(g(a)) 2.4 基数 当且仅当存在一个单射函数 f 使得 f(A) = B 时，集合 A 和集合 B 有相同的基数。 如果一个无限集 S 是可数（可数是指能枚举）的，那我们就称 S 有基数 ℵ0（阿里夫零）。 可数集的子集（无论是有限子集还是无限子集）当然也是可数集。 一个集合的子集不可数，则该集合也不可数。 如果存在单射函数 f 将 A 映射到 B，还存在单射函数 g 将 B 映射到 A，则 |A| = |B|，即 A 与 B 之间存在一一对应关系（双射函数）。 2.5 矩阵 唯一陌生的点：布尔积（⊙）、布尔积的幂（A[p]） 03 计数 3.1 鸽巢原理 陈述：k + 1 只鸽子要飞往 k 个鸽巢，则有一个鸽巢至少有 2 只鸽子。 推论 1：一个从 k + 1 个甚至更多元素到 k 个元素的集合的映射 f 一定不是单射函数。 广义鸽巢原理：如果 N 个物体放入 k 个盒子，那么至少有一个盒子包含了至少 ⌈N/k⌉ 个物体。 定理：每个由 n2 + 1 个不同实数构成的序列都包含一个长为 n + 1 的严格递增子序列或严格递减子序列。 拉姆齐数：R(m,n)，表示一个舞会上，使得或者 m 个人两两是朋友，或者 n 个人两两是敌人的最少人数。 3.2 排列组合 r 排列： 对一个集合中 r 个元素的有序排列称为 r 排列。 具有 n 个不同元素的集合的 r 排列数是 $P(n,r)=n(n-1)(n-2)\\dots(n-r+1)=\\frac{n!}{(n-r)!}$。 r 组合： 对一个集合中 r 个元素的无序排列称为 r 组合。 具有 n 个不同元素的集合的 r 组合数是 $C(n,r)=\\frac{n!}{r!(n-r)!}$，组合数的性质 C(n,r) = C(n,n−r)。 二项式定理： $$ (x+y)^n=\\sum_{j=0}^{n}C(n,j)x^{n-j}y^j $$ 推论：设 n 为非负整数，令 x = 1 和 y = 1，我们有 $$ \\sum_{k=0}^{n}C(n,k)=2^n $$ $$ \\sum_{k=0}^{n}(-1)^kC(n,k)=0 $$ $$ \\sum_{k=0}^{n}2^kC(n,k)=3^n $$ 帕斯卡恒等式： Ckn + 1 = Ck − 1n + Ckn 范德蒙德恒等式： $$ C_{r}^{m+n}=\\sum_{k=0}^{r}C_{r-k}^mC^n_k $$ 推论4：如果 n 是一个非负整数，那么 $$ C_n^{2n}=\\sum_{k=0}^{n}(C_{k}^{n})^2 $$ 定理4：设 n 和 r 是非负整数，且有 r ≤ n，那么 $$ C_{r+1}^{n+1}=\\sum_{j=r}^{n}C_{r}^{j} $$ 3.3 排列组合的推广 定理1：具有n个对象的集合允许重复的r排列数为nr。 定理2：具有n个对象的集合允许重复的r组合数为C(n+r−1,r)。 具有不可区别物体的集合的排列：类型1的相同的物体有n1个，类型2的相同的物体有n2个，……，类型k的相同的物体有nk个，那么n个物体的不同排列数是$\\frac{n!}{n_1!n_2!\\dots n_k!}$。 把物体放入盒子：将n个不同的物体分配到k个不同的盒子使得ni个物体放入第i（i = 1, 2, ..., k）个盒子的方式数为$\\frac{n!}{n_1!n_2!\\dots n_k!}$。 04 求解线性递推关系式 所谓求解线性递推关系式（linear recurrence relation），其实就是高中数列的通项公式求解。 4.1 线性常系数齐次递推关系式 一个常系数 k 阶线性齐次递推关系是形如 an = c1an − 1 + c2an − 2 + … + ckan − k 的递推关系，其中 c1, c2, …, ck 为实数，且 ck ≠ 0（若 ck 为 0 的话就不是 k 阶了）。 求解线性常系数齐次递推关系式： 基本方法是寻找形如 an = rn 的解，其中 r 是常数，代入到递推关系式就有： rn = c1rn − 1 + c2rn − 2 + … + ckrn − k 等式两边同时除以 rn − k，移项得到特征方程： rk − c1rk − 1 − c2rk − 2 − … − ck − 1r − ck = 0 当且仅当 r 是特征方程的解时，具有 an = rn 的序列 {an} 是一个解。特征方程的解称为递推关系的特征根，可用特根给出递推关系的所有解的显式表达式。 特征方程其实就是一个一元 k 次方程，可证明的是，当 k ≥ 5 时，方程没有解析解。 4.1.1 单根 定理 1：若特征方程 r2 − c1r − c2 = 0 有两个不相等的根 r1、r2，当且仅当 an = α1r1n + α2r2n 时，{an} 就是递推关系式 an = c1an − 1 + c2an − 2 的解。（其中 α1 和 α2 可以靠初始条件得到） 4.1.2 重根 定理 2：若特征方程 r2 − c1r − c2 = 0 有一个重根 r0，当且仅当 an = α1r0n + α2nr0n 时，{an} 就是递推关系式 an = c1an − 1 + c2an − 2 的解。（其中 α1 和 α2 可以靠初始条件得到） 4.1.3 任意阶 定理 3：若特征方程 rk − c1rk − 1 − … − ck = 0 有 k 个不相等的根 r1、r2、…、rk，当且仅当 an = α1r1n + α2r2n + … + αkrkn 时，{an} 就是递推关系式 an = c1an − 1 + c2an − 2 + … + ckan − k 的解。 4.1.4 部分重根 定理 4：假设特征方程 rk − c1rk − 1 − … − ck = 0 有 t 个不同的根，每个根的重数分别为 m1, m2, …, mt，则解为 $$ a_n=\\sum_{i=1}^{t}(\\sum_{j=1}^{m_t}\\alpha_{t,j}n^{j-1})r_t^n $$ 4.2 线性常系数非齐次递推关系式 形如 an = c1an − 1 + c2an − 2 + … + ckan − k + F(n) 的递推式就是线性常系数非齐次递推关系式，其中 F(n) 是一个只依赖与 n 且不等于零的函数。其中，an = c1an − 1 + c2an − 2 + … + ckan − k 叫做相伴的线性齐次递推关系。 类似于线性代数中的特解+通解，线性常系数非齐次递推关系式的每个解，都是一个特解和相伴线性齐次递推关系的一个解的和。 求线性常系数非齐次递推关系式的通解： 先求相伴的线性齐次递推关系的解 根据 F(n) 的形式，设出一个特解形式，将特解代回通项公式中解出该特解 将两个解加起来就是通解 将初始条件带入通解，得到一个特解。 下面的定理给出了特解形式的设法： 若一个非齐次递推关系式中的非齐次项 F(n) 有如下形式： F(n) = (btnt+bt − 1nt − 1+…+b0)sn 若其中 s 不是相伴齐次递推关系的特征方程的根，存在一个下述形式的特解： (ptnt+pt − 1nt − 1+…+p0)sn 若其中 s 是相伴齐次递推关系的特征方程的 m 重根，存在一个下述形式的特解： nm(ptnt+pt − 1nt − 1+…+p0)sn 05 容斥原理 基本的容斥： |A∪B| = |A| + |B| − |A∩B| 三个有限集： |A∪B∪C| = |A| + |B| + |C| − |A∩B| − |B∩C| − |A∩C| + |A∩B∩C| 推广到 n 个有限集： $$ \\begin{aligned} |A_1\\cup A_2\\cup\\dots\\cup A_n|=&amp;\\sum_{i\\le i\\le n}|A_i|\\\\&amp;-\\sum_{1\\le i\\le j\\le n}|A_i\\cap A_j|\\\\&amp;+\\sum_{1\\le i\\le j\\le k\\le n}|A_i\\cap A_j\\cap A_k|\\\\&amp;-\\dots\\\\&amp;+(-1)^{n+1}|A_1\\cap A_2\\cap\\dots\\cap A_n| \\end{aligned} $$ 5.1 错位排列 n 个元素的集合的错位排列数为 $$ D_n=n![1-\\frac{1}{1!}+\\frac{1}{2!}-\\frac{1}{3!}+\\dots+(-1)^n\\frac{1}{n!}] $$ 5.2 埃拉托色尼筛(伊拉脱森筛) 埃拉托色尼筛是一种找出一定范围内所有素数的算法。 其原理很简单：假设要求出 100 以内所有素数，则先列出 2-100 范围内所有的整数，执行下面的步骤： 选出序列中未被标记的第一个数，将其之后所有倍数都进行标记； 重复步骤 1，直到找不到未被标记的数为止（要跳过步骤 1 选取的数）； 剩余序列中未被标记的元素就是 100 以内所有的素数。 时间复杂度为 O(nloglogn)。 埃拉托色尼筛的本质是一种容斥，基于不超过 100 的合数肯定有一个不超过 10 的素因子，因此选出 10 以内所有的素因子有 2、3、5、7，令 P1 表示能被 2 整除的性质，P2 表示能被 3 整除的性质，P3 表示能被 5 整除的性质，P4 表示能被 7 整除的性质，则不超过 100 的素数的个数就是 N(P1′P2′P3′P4′)，然后应用容斥原理就可以求了。 06 关系 6.1 二元关系 由有序对构成的集合称为一个关系，例如（1,b）。一个关系表示一个集合 A 到另一个集合 B 的映射，若存在 1 ∈ A，b ∈ B，且有关系 (1,b)，则写做 1Rb，否则在 R 标记上画一条斜对角线表示不存在这样的关系 1R̸b。 关系其实就是两个集合的笛卡尔积的子集。那么，含有 n 个元素的集合 A 上最多有多少关系？其实就是考虑 A × A 的子集个数，A × A 有 n2 个元素，m 个元素构成的集合有 2m 个子集，所以 A × A 有 2|A|2 个关系。 二元关系的记号： 前缀表示法：R(x,y) 中缀表示法：xRy 后缀表示法：(x,y) ∈ R 函数是一种关系，但关系不一定是函数，因为关系可以一对多。 6.1.1 性质 若对于 A 中每个元素 a，都存在 (a,a) ∈ R，则称 R 是自反的。如果这样的有序对一个都不存在，那就称 R 是反自反的。（空关系既是自反的又是反自反的） 若对任意 (a,b) ∈ R 都有 (b,a) ∈ R，则称 R 是对称的。如果 (a,b) ∈ R 并且 (b,a) ∈ R，则有 a = b，则称 R 是反对称的。（恒等关系既是对称的又是反对称的） 若对 (a,b) ∈ R 和 (b,c) ∈ R，都存在 (a,c) ∈ R，则称 R 是传递的。 6.1.2 关系的合成 假设 R1 是集合 A 到集合 B 的关系，R2 是集合 B 到集合 C 的关系，则 R1 和 R2 的合成是 A 到 C 的关系，记作 R1 ∘ R2。 关系的幂就是多个关系合成：Rn + 1 = Rn ∘ R。 6.1.3 逆关系 记作 R−1 = {(y,x)|(x,y) ∈ R}。 定理：(R1∘R2)−1 = R2−1 ∘ R1−1。 6.2 关系表示 6.2.1 矩阵 自反关系的矩阵对角线元素都是 1。 对称关系的矩阵是对称矩阵。 反对称关系的矩阵对角线元素都是 1，并且 aij 在 i ≠ j 的情况下，若 aij = 1，则 aji = 0。 关系的交、并运算可以转换为矩阵的或、与运算。 关系的合成可以转换为矩阵的布尔积运算，布尔幂的表示要在幂次上加个方括号。 6.2.2 图 自反关系的图中的每个结点都有一个指向自己的边。 对称关系的图中每条边都是双向边。 反对称关系的图中每条边都是单边。 传递关系的图中，若 a 与 b 有连线，且 b 与 c 有连线，则 a 与 c 也有连线。 6.3 关闭的闭包 所谓闭包，就是指为了让关系满足某个我们所需要的性质（传递、自反、对称）而往其中添加外来关系的集合。 若某个关系 R 不满足自反关系，则我们用 r(R) 表示 R 的一个自反闭包，该闭包有如下特点： r(R) 是自反的 R ⊂ r(R) 在所有满足 1、2 的集合中，r(R) 是最小的 同理还有传递闭包、对称闭包。 如果一个关系是对称（自反、对称）关系，则它本身就是自己的对称（自反、对称）闭包。 如果一个关系是另一个关系的子集，那么它的闭包也是另一个闭包的子集。 三种闭包的求法： 自反闭包：令 Δ = {(a,a)|a ∈ A}，则 R 的自反闭包就是 R ∪ Δ。 对称闭包：R ∪ R−1 传递闭包：R ∪ R2 ∪ R3 ∪ … 6.4 路径 定理：a 和 b 之间存在一条长度为 n 的路径，当且仅当 (a,b) ∈ Rn. 6.5 传递闭包 给出连通性闭包的定义： $$ R^{*}=\\bigcup_{n=1}^{\\infty}R^{n} $$ 可以证明，连通性闭包和传递闭包是等价的。 令 MR 是定义在 n 个元素集合上的关系 R 的 0 − 1 矩阵，那么传递闭包 R* 的 0 − 1 矩阵是： MR* = MR ∨ MR[2] ∨ MR[3] ∨ … ∨ MR[n] 如果使用算法实现，则计算一次矩阵的布尔积需要进行 n2(2n−1) 次位运算，计算 n 个矩阵的布尔积则共需要 n2(2n−1)(n−1) 次位运算，故算法的时间复杂度为 O(n4)。 沃舍尔算法对这种朴素的计算方式进行了优化，使得只需要 2n3 次位运算就可以求出这个传递闭包。 6.6 等价关系 定义 1：当一个关系同时是自反、对称、传递的，就称其为等价关系。（tip：由于等价关系是自反的，因此定义在集合 A 上的等价关系必然涵盖了集合 A 中全部元素） 定义 2：假设 aRb，且 R 是等价关系，则称 a 与 b 等价，记作 a ∼ b 定义 3：如果 R 是 S 上的等价关系，且 a ∈ S，则将 R 中所有与 a 有关联的元素 s 构成的集合称为 a 的等价类，可表示为 [a]R = {s|(a,s) ∈ S}，而 s 称为 a 的代表元。（tip：由于自反性，元素 a 的等价类中必定包括了 a） 定理 1：若 R 是定义在集合 A 上的等价关系，则下面三种表示等价： aRb [a] = [b] [a] ∩ [b] ≠ ∅ 等价类可以划分集合：假设 R 是定义在集合 A 上的等价关系。等价类和等价类之间必不相交，并且所有等价类的并集就是等价于集合 A。（可以参考模 4 同余关系） 定理 2：从上面的讨论我们已经知道给定一个集合和在这个集合上定义的等价关系，我们可以构造该集合的一个划分；那么反过来，给定该集合的一个划分，我们也能找到这样一个等价关系，能划分该集合。 6.7 偏序 6.7.1 基本概念 定义 1：如果定义在集合 S 上的关系 R 是自反、反对称、传递的，则称其为偏序。集合 S 与定义在其上的偏序 R 一起称为偏序集，用 (S,R) 表示。集合 S 中的成员称为偏序集的元素。 通常使用 ≼ 表示偏序关系，如果 (a,b) ∈ R 则记作 a ≼ b，若 a ≠ b，则记作 a ≺ b，这说明 a 和 b 是可比的。 偏序集中的元素并不都是可比的，这句话的意思是，并不是所有的序偶组合都存在于偏序集中，只有存在 (a,b) ∈ (S,≼) 时才说 a 和 b 可比，即有 a ≼ b，否则就是不可比的。例如在偏序集 $(\\textbf Z^+, |)$ 中（| 表示整除关系），5 和 7 就不是可比的，因为 $(5,7)\\notin(\\textbf Z^+,|)$。 事实上，之所以说叫“偏”序，就是因为可能存在有些元素是不可比的。如果集合中的每个元素都可比，那么就称这个关系为全序，称这个集合为全序集（或线序集或链）。 对于偏序集 (S,≼)，如果 ≼ 是全序，并且 S 的每个非空子集都有一个最小元素，那么就称它为良序集。例如，$\\textbf Z^+\\times\\textbf Z^+$ 的元素是形如 (a,b) 的序偶，如果我们按顺序比较两个元素的大小，就有 (1,5) &lt; (2,3) 或 (2,3) &lt; (2,6)，由于 $\\textbf Z^+$ 是正整数集，所以最小元素就是 0，因此 $\\textbf Z^+\\times\\textbf Z^+$ 的最小元素就是 (0,0)，这说明 $\\textbf Z^+\\times\\textbf Z^+$ 是良序集。但 $\\textbf Z\\times\\textbf Z$ 就不是良序集，因为其中包含负数，那就不存在这样的最小元素。 6.7.2 哈塞图 偏序仍然是一种关系，关系可以用图来表示，这里引出哈塞图，它忽略由于偏序的自反性和传递性而必须出现的边。如果关系是全序的（各个元素之间都存在关系，即都可比），你就会发现其哈塞图是一条链条，这就是称全序集为链的原因。 构造哈塞图的过程： 去掉所有结点上的环（这是由于自反性造成的） 去掉所有这样的边 (x,y)：存在元素 z ∈ S 满足 x ≼ z 和 z ≼ y（这是由传递性造成的） 排列每条边，使大的元素在上，去掉所有箭头（因为所有箭头都指向顶线） 通过哈塞图可以很直观地看出极大元/极小元（maximal/minimal）、最大元/最小元（greatest/least），只需要看哈塞图最顶端和最底端的元素是什么就行了。但是注意，极大元/极小元是指集合中没有其他元素大于/小于该元素，但是可以无法比较，这意味着极大元/极小元允许有多个；而最大元/最小元要求集合中所有元素都严格小于/大于它，不允许存在与它处于同一层但是无法比较的元素，也就是说如果一个哈塞图中，顶端元素不止一个，那这个偏序就不存在极大元；如果底端元素不止一个，那这个偏序就不存在极小元。 再介绍上/下界、上/下确界的概念：设 (S,≼) 为偏序集，且 A 是 S 的一个子集，若 S 中存在一个元素 a，使得 A 中任意一个元素 b 都存在 b ≼ a，则称 a 为 A 的上界；反之得到下界（上/下界中的元素不一定要和 A 中元素都可比，只需比可比元素大/小就行了）。若 a 是 A 的上界集合中的最小元，则称 a 为上确界；若 a 是 A 的下界集合中的最大元，则称 a 为下确界。 6.7.3 格 如果一个偏序集的每一对元素都有上确界和下确界，就称这个偏序集为格。 下面来看一个例子（哈塞图本不应该带箭头，下图在这点上有误，请不要被误导）： graph TD a --> b b --> c b --> d c --> e d --> e e --> f 假设我们选取 {c, d}，那么凡是在它们上面的（a 和 b）都是上界，凡是在它们下面的（e 和 f）都是下界。但上确界只有 b，下确界只有 e。 假设我们选取 {c, b}，注意，它们的上界并不只有 a，还有 b，为什么？注意上下界的定义是 ∀b ∈ A(b≼a)，这里是允许等于的，这里显然 b ≼ b 且 b ≼ c，所以 b 自然也属于上界，同理，下界有 {c, e, f}。因此，{c, b} 的上确界就是 b，下确界就是 c。 那为什么刚刚讨论 {c, d} 的上下界时没有 c 和 d？因为 c 和 d 压根就不可比。 这里要注意一件事：找上/下界、上/下确界时我们是在全集中找，但是找最大/小值、极大/小值时，我们仅在子集中找。例如在上例中找 {c, b, e} 的极大值，我们只能找到 b，而不会有 a。 显然，上图中任意对元素都存在上下确界，因此属于格，下面来看一个不属于格的例子。 graph TD a --> b a --> c b --> d c --> e d --> f e --> f b --> e c --> d 假设选取 {b, e}，则上确界为 a，这一点毫无疑问。下界为 {d, e, f}，哪个是下确界？下确界是下界集合中的最大值，根据最大值的定义，由于 f 小于 d 和 e，d 和 e 又不可比，所以不存在最大值，因此不存在下确界，故上图不是格。 6.7.4 拓扑排序 这个简单。 07 图论 7.1 基本概念 简单图：不存在多重边（不同的边连接同一对顶点）和环（存在指向自身的边）的图 多重图：存在多重边但不存在环的图 伪图：存在多重边或环的图 上面的概念都针对无向图，下面是有向图： 简单有向图：不允许出现环和多重边的有向图 有向多重图：允许出现环和多重边的有向图 在某些情况下，我们可能需要一张图中既有无向的边，又有有向的边，我们称这样的边为 混合图。 7.2 图的术语 7.2.1 基本概念 设有图 G = (V,E)，我们将顶点 v ∈ V 的相邻顶点记作 N(v)，若有顶点集合 A ∈ V，则将 A 中所有顶点的相邻顶点构成的集合记作 V(A)，则有 V(A) = ⋃v ∈ AN(v)。 我们将顶点的度记作 deg (v)，特殊地，如果 v 是带有环的无向图结点，那么环对 v 的度做出双倍贡献。 度为 0 的顶点称为孤立的，度为 1 的顶点称为悬挂的。 握手定理：无向图中，所有顶点度数之和为边数的两倍。 定理：无向图有偶数个度为奇数的顶点。 有向图中，将顶点的入度记为 deg−(v)，将出度记为 deg+(v)。 在有向图中，所有顶点的入度之和 等于 出度之和 等于 边数。 7.2.2 特殊简单图 完全图：n 个结点的完全图记作 Kn，完全图的特点是每个结点之间都有一条直接相连的边。 圈图：n 个顶点的圈图记作 Cn，圈图的特点是所有结点围成一个圈。 轮图：在 Cn 中添加一个顶点，并将这个顶点与其他顶点相连，就得到一个轮图 Wn，像一个车轮。 n 立方图：记作 Qn，有 2n 个顶点，每个顶点用二进制编号，当且仅当两个顶点编号的海明距离为 1 时两个顶点相连。 7.2.3 二分图 有些图可以将顶点分成两部分，就像小时候做的连线题一样。例如表示一个村庄里的婚姻状况，那么有一堆顶点代表男性，一堆顶点代表女性，有婚姻关系的就可以连在一起，由于同性无法建立婚姻关系（至少在中国的法律层面不行）所以同性顶点之间不会有连线，故可以把表示两种性别的顶点分别拉到左右两边。 若一个图能被二分，则有 V1, V2 ∈ V，称 (V1,V2) 为 G 的顶点集的一个二部划分。 如何判断一个图是否为二分图：给图中每个顶点赋予两种不同的颜色，当且仅当能找到一种着色方法，使得同颜色的顶点之间不存在连线，说明这个图是二分图。 完全二分图：V1 和 V2 中的结点之间都有连线，记作 Km, n，其中 m 和 n 分别是 V1 和 V2 中的顶点个数。 7.2.4 从旧图构造新图 从一个图中抽出一部分顶点和边构成一张新图，后者被称为前者的子图，如果两张图不相等，就称后者为前者的真子图。 导出子图：如果有 G = (V,E)，G′ = (W,F)，其中 W ⊂ V，且被选出的结点间，该有的边全都有，就称 G′ 是 G 的导出子图（但凡删除一条边都不再是导出子图）。 并图：将两张图结合到一起得到的图称为并图，假设 G1 = (V1,E1)，G2 = (V2,E2)，则它们的并图为 G = G1 ∪ G2 = (V1∪V2,E1∪E2). 7.3 图的表示和同构 7.3.1 邻接表 这还用讲？ 7.3.2 邻阶矩阵 当用邻接矩阵表示伪图时，元素值代表边的数量。 7.3.3 关联矩阵 关联矩阵的行号代表顶点编号，列号代表边编号。 7.3.4 同构 两个图同构是指当忽略图的顶点编号后，两个图相等。 如何证明同构：假设 G1 中的任意顶点为 a，G2 中的任意顶点为 b，找一个满射函数 f 使得 f(a) = b，若当 G1 中 a1, a2 相邻，G2 中 f(a1) 和 f(a2) 也相邻时，则说明 G1 和 G2 同构。 说明两个图同构是一件很难的事情，但是我们可以通过同构不变量来简单说明两个图不同构，例如顶点数量、顶点的度、边的数量等。另外，还可以这样判断不同构：两张图中度相同的结点与连接它们的边所构成的图一定也是同构的（经常使用两个度相同的顶点之间的路径来是否长度相同来判断图不同构）。 一种比较简单的方式来说明两个图同构：找一个满射函数 f，使得两个图的邻接矩阵相等。 7.4 连通性 通路是边的序列。当一个图是简单图时，我们可以直接用顶点的有序序列来表示一条通路（否则不行，因为这样的话一对顶点无法唯一确定一条边）。当通路的首尾顶点相同且长度大于 0 时，就构成了回路。一条不含重复边的回路叫做简单回路。 7.4.1 无向图的连通性 当图中任意两个顶点间都存在一条通路时，一个无向图是连通的。 在连通无向图中，每一对顶点之间都存在一条简单通路。 一个不连通的图实际就是几个连通的子图构成的图。连通分支就是指极大连通子图，所谓极大连通子图，首先是连通子图，齐次它不是其他任何连通子图的真子图。 有时候，删除某个顶点以及与之相连的边，会导致一个图中产生更多的连通子图，这种点称为割点。删除某条边导致一个图中产生更多的连通子图，这种边称为割边或桥。 7.4.2 有向图的连通性 如果有向图中，对于任意一对顶点 a 和 b，都有从 a 到 b 和 b 到 a 的通路，那就称该图是强连通的。 如果把一个有向图改成无向图后（这个无向图称为该有向图的基本无向图），任意两个顶点都连通，那就说这个有向图是弱连通的。 强连通分支：是有向图的连通分支且是强连通的。 7.4.3 通路与同构 有多种方式来利用通路和回路判断一个图是否同构。比如，特定长度的回路就是一个有用的同构不变量。 7.4.4 计算通路数 假设一个图的邻接矩阵为 A，那么 An （是正常的矩阵乘法不是布尔积！）中 (i,j) 上的元素就代表了该图中顶点 i 和顶点 j 之间长为 n 的路径数量。 7.5 欧拉通路和哈密顿通路 7.5.1 欧拉通路和欧拉回路 欧拉通路：指一张图中包含了该图全部边的简单通路。 欧拉回路：指一张图中包含了该图全部边的简单回路。 一个有 2 个及以上的连通多重图有欧拉回路的充要条件：每个顶点的度都是偶数。 构造欧拉回路的算法（O(|E|)）： 从图中的任意一个顶点出发 ，沿着边行走，形成一条回路，要求每条边只能走一次，但可以重复经过顶点，直到无法继续前进（即所有与当前顶点相连的边都已被走过）。 若此时回路包含了图中的所有边 ，则该回路就是欧拉回路。 如果回路中还没有包含所有边 ，则在回路中找到一个顶点，该顶点存在还未被走过的边。从这个顶点出发，继续沿着未走过的边行走，形成一个新的回路，并将这个新回路插入到原回路中，替换掉原来的那个顶点。重复这个过程，直到所有的边都被包含在回路中。 一个连通多重图只有欧拉通路没有欧拉回路的充要条件：恰有 2 个度为奇数的顶点。 7.5.2 哈密顿通路和哈密顿回路 哈密顿通路：指一张图中包含了该图全部顶点的简单通路。 哈密顿回路：指一张图中包含了该图全部顶点的简单回路。 目前不存在已知的简单的充要条件来判断一个图中是否含有哈密顿通路和哈密顿回路。但是有这么几个性质： 存在度为1的顶点的图不存在哈密顿通路 若一个顶点的度为2，且该图存在哈密顿通路，那么这个顶点的两条边肯定属于哈密顿通路 一个哈密顿回路不可能含有其他回路 狄拉克定理：如果G是有n个顶点的简单图，其中n ≥ 3，并且G中每个顶点的度都至少是⌈n/2⌉，则G有哈密顿回路。 欧尔定理：如果G是有n个顶点的简单图，其中n ≥ 3，并且对于G中每一对不相邻的顶点u和v来说，都有deg (u) + deg (v) ≥ n，则G有哈密顿回路。 狄拉克定理可以看作是欧尔定理的推论。不论是狄拉克定理还是欧尔定理，都只是充分条件，并没有给出必要条件。也就是说这里存在一些图，既不满足狄拉克定理，也不满足欧尔定理，但它含有哈密顿回路。 7.6 平面图 如果有办法可以使边无交叉地画出一幅图，那么就称这幅图为平面图，把这个画法称为前者的平面表示。 7.6.1 欧拉公式 一个平面图可以把纸面分隔成好几个二维空间（其中有一个是无限平面），这样的二维空间称为一个面。 欧拉公式：设 G 是带 e 条边和 v 个顶点的连通平面简单图，设 r 是 G 的平面图表示中的面数，则 r = e − v + 2. 注：如果不考虑外部面，则 r = e − v + 1。 推论 1：若 G 是 e 条边和 v 个顶点的连通平面简单图，其中 v ≥ 3，则 e ≤ 2v − 6。 推论 2：若 G 是连通平面简单图，则 G 中有度数不超过 5 的顶点。 推论 3：若 G 是 e 条边和 v 个顶点的连通平面简单图，且 v ≥ 3 且没有长度为 3 的回路，则 e ≤ 2v − 4。 7.6.2 库拉图斯基定理 若G是一个平面图，在它的某条边{a, c}上新建一个中间结点b，使得原本的{a, c}被新增的两条边{a, b}和{b, c}替换掉，这样形成的新图仍然是平面图，这种操作称为初等细分。若两幅图可以通过对同一幅图进行初等细分来得到，则称这两张图是同胚的。 库拉图斯基定理：当且仅当图G包含了同胚于K3, 3或K5的子图，G是非平面图。 一般来说，直接应用库拉图斯基定理来判断非平面图是比较困难的，我们可以考虑对图进行逆向初等细分，即一步一步去掉中间结点，看是否能把图还原成K3, 3或K5。 7.7 图着色 对偶图：将一张地图的每块区域看作一个顶点，如果区域与区域之间有公共边界，那就认为它们对应的顶点是相邻的（如果仅仅是相交于一个点则不算），这样画出来的图叫这张地图的对偶图。 图着色是指为图中的每个顶点着色，要求没有一对相邻顶点的颜色是相同的。 着色数是指使图G中结点颜色各不相同所需要的最少颜色数，记作χ(G)。 四色定理：平面图的着色数不超过4.（仅适用于平面图） 几个结论： χ(Kn) = n χ(Kn, m) = 2 当n为偶数时，χ(Cn) = 2，当n为奇数时，χ(Cn) = 3","categories":[{"name":"计算机专业基础","slug":"计算机专业基础","permalink":"https://example.com/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E4%B8%93%E4%B8%9A%E5%9F%BA%E7%A1%80/"}],"tags":[{"name":"discrete math","slug":"discrete-math","permalink":"https://example.com/tags/discrete-math/"}]},{"title":"图注意力机制的原理","slug":"图注意力机制","date":"2024-04-03T03:23:32.000Z","updated":"2025-04-09T14:28:54.811Z","comments":true,"path":"2024/04/03/图注意力机制/","permalink":"https://example.com/2024/04/03/%E5%9B%BE%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/","excerpt":"该篇笔记总结了图注意力网络（Graph Attantion Network）中的图注意力层的数学原理，参考的资料为 GAT 网络的原始论文. 假设输入是一组节点特征，表示为 $h=\\{\\vec{h_1},\\vec{h_2},\\cdots,\\vec{h_N}\\},\\vec{h_i}\\in\\mathbb{R}^F$，其中 N 表示节点的个数，F 表示特征维数。GAL 将会输出一组新的节点特征，并且这组特征的特征维数并不一定与原特征相同，设 F′ 为 GAL 输出的特征维数，则新的节点特征可表示为 $h^{\\prime}=\\{\\vec{h_1^{\\prime}},\\vec{h_2^{\\prime}},\\cdots,\\vec{h_N^{\\prime}}\\},\\vec{h_i^{\\prime}}\\in\\mathbb{R}^{F^{\\prime}}$。","text":"该篇笔记总结了图注意力网络（Graph Attantion Network）中的图注意力层的数学原理，参考的资料为 GAT 网络的原始论文. 假设输入是一组节点特征，表示为 $h=\\{\\vec{h_1},\\vec{h_2},\\cdots,\\vec{h_N}\\},\\vec{h_i}\\in\\mathbb{R}^F$，其中 N 表示节点的个数，F 表示特征维数。GAL 将会输出一组新的节点特征，并且这组特征的特征维数并不一定与原特征相同，设 F′ 为 GAL 输出的特征维数，则新的节点特征可表示为 $h^{\\prime}=\\{\\vec{h_1^{\\prime}},\\vec{h_2^{\\prime}},\\cdots,\\vec{h_N^{\\prime}}\\},\\vec{h_i^{\\prime}}\\in\\mathbb{R}^{F^{\\prime}}$。 为了把输入特征转换为高维特征，并且使高维特征具有足够高的表达原特征的能力，我们需要执行至少一个可学习的线性变换。考虑到这一点，我们将对每一个节点进行一次线性变换，即将每个节点的特征乘上一个共享的权重矩阵 W ∈ ℝF′ × F。然后我们会对每一个节点进行一次自注意力 (self-attention) 操作，我们用 a 表示这一操作，该操作将会计算得出节点 i 和节点 j 之间的注意力系数 (attention coefficients)： $$ e_{ij}=a(W\\vec{h_i},W\\vec{h_j}) $$ 注意力系数 eij 表示了节点 j 的特征对于节点 i 的重要性。在自注意力机制最广泛应用的公式中，模型将会计算每个节点之间的注意力系数，导致结构信息被丢弃。我们通过执行掩码注意力机制 (masked attention) 来保留图的结构信息——我们只计算相邻节点之间的注意力系数。注意，这里的相邻节点指的是直接相邻 (first-order) 的节点，而不是连通的节点，并且一个节点本身也是其自己的相邻节点。为了能对注意力系数进行跨节点的比较，我们对它们进行归一化，用以衡量不同节点 j ∈ 𝒩i 对节点 i 的重要程度 (这里的 𝒩i 表示节点 i 的邻居节点的编号)，归一化的过程由 softmax 函数实现： $$ \\alpha_{ij}=\\operatorname{softmax}(e_{ij})=\\frac{\\operatorname{exp}(e_{ij})}{\\sum_{k\\in\\mathcal{N}_i}\\operatorname{exp}(e_{ik})}. $$ 在我们的实验中，注意力机制 a 是一个单层的前馈神经网络，以一个权重向量 $\\vec{\\mathbf{a}}\\in\\mathbb{R}^{2F^\\prime}$​ 作为参数，使用 LeakyReLU 函数实现非线性变换 (negative slope 设置为 0.2)。展开后，系数的计算可表示为： $$ \\alpha_{ij}=\\frac{\\operatorname{exp}(\\operatorname{LeakyReLU}(\\vec{\\mathbf{a}}^{T}[\\mathbf{W}\\vec{h_i}||\\mathbf{W}\\vec{h_j}]))}{\\sum_{k\\in\\mathcal{N}_i}\\operatorname{exp}(\\operatorname{LeakyReLU}(\\vec{\\mathbf{a}}^{T}[\\mathbf{W}\\vec{h_i}||\\mathbf{W}\\vec{h_k}]))} $$ 其中，·T 表示转置操作，|| 是矩阵拼接操作。 一旦得到注意力系数，GAL 就会使用它们来计算出各个特征对应的线性组合，作为节点最终的输出特征，整个过程如下左图所示 (右图是多头注意力机制)： 输出特征 h⃗i′ 的计算公式如下： h⃗i′ = σ(∑j ∈ 𝒩iαijWh⃗j) 其中，σ 代表一种非线性变换。 为了使整个自注意力过程更加稳定，我们引入了多头注意力 (multi-head attention)。具体来说，K 个独立的注意力机制将分别执行上式的变换，随后，它们的结果将会被拼接起来，得到下面的输出特征表示： $$ \\vec{h}_i^\\prime=\\overset{K}{\\underset{k=1}{\\|}}\\sigma(\\sum_{j\\in\\mathcal{N_i}}\\alpha_{ij}^k\\mathbf{W}^k\\vec{h}_j) $$ 特殊的，如果我们在输出层执行多头注意力，拼接过程将不再是显式的，我们会使用平均聚合 (averaging)，然后施加一个非线性变换来得到最终输出： $$ \\vec{h}_i^\\prime=\\sigma(\\frac{1}{K}\\sum_{k=1}^{K}\\sum_{j\\in\\mathcal{N_i}}\\alpha_{ij}^k\\mathbf{W}^k\\vec{h}_j) $$","categories":[{"name":"深度学习","slug":"深度学习","permalink":"https://example.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"gnn","slug":"gnn","permalink":"https://example.com/tags/gnn/"},{"name":"attention","slug":"attention","permalink":"https://example.com/tags/attention/"}]},{"title":"Vue3入门","slug":"vue3","date":"2024-02-17T06:54:18.000Z","updated":"2025-04-13T06:16:10.992Z","comments":true,"path":"2024/02/17/vue3/","permalink":"https://example.com/2024/02/17/vue3/","excerpt":"","text":"01 第一个 Vue3 项目 1.1 搭建开发环境 本节介绍如何使用官方推荐的方式创建一个 vue3 项目。 在控制台中输入如下代码： 1npm create vue@latest 控制台会显示要求输入项目名称，以及有关于 vue 项目的各种配置，按需设置即可： 123456789101112131415161718Vue.js - The Progressive JavaScript Framework√ 请输入项目名称： ... hello_vue3√ 是否使用 TypeScript 语法？ ... 否 / 是√ 是否启用 JSX 支持？ ... 否 / 是√ 是否引入 Vue Router 进行单页面应用开发？ ... 否 / 是√ 是否引入 Pinia 用于状态管理？ ... 否 / 是√ 是否引入 Vitest 用于单元测试？ ... 否 / 是√ 是否要引入一款端到端（End to End）测试工具？ » 不需要√ 是否引入 ESLint 用于代码质量检测？ ... 否 / 是正在构建项目 C:\\Users\\24159\\Desktop\\hello_vue3...项目构建完成，可执行以下命令： cd hello_vue3 npm install npm run dev 随后，在控制台所在的路径上会出现一个新的文件夹，名称和你输入的项目名称一致，这便是你的项目文件，进入该文件 (cd 文件名)，输入 npm install 安装项目的所有依赖，最后输入 npm run dev 便可以开发模式启动项目。 编辑 vue 代码的 IDE 建议选择 VSCode，官方专门为 VSCode 开发了两个编写 vue 的插件，分别叫 Vue Language Features (Volar) 和 TypeScript Vue Plugin (Volar)，在插件市场搜索 Volar，排名最靠前的两个就是。 1.2 App 组件 打开项目文件夹，其中有一个叫做 src 的文件夹，这个文件夹是我们的“主战场”，意思是代码基本上都写在这里面。这个文件中最重要的一个文件是 main.ts，这是项目的入口文件，其内容很简单： 123456import &#x27;./assets/main.css&#x27;import &#123; createApp &#125; from &#x27;vue&#x27;import App from &#x27;./App.vue&#x27;createApp(App).mount(&#x27;#app&#x27;) 第一句话引入 css 样式。第二句话是从 vue 中引入创建应用的方法 createApp。第三句话从 App.vue 中引入 App 根组件。第四句话则是使用 createApp 方法，将 App 组件创建出来，mount 的意思是挂载，即创建完后要将组件装入哪里，参数 #app 是指挂在到 index.html 中 id 为 app 的容器。 所以，index.html 中可以什么都没有，但是不能没有 id 为 app 的标签，而且必须引入 main.ts。 上面我们提到了 App 组件，这是一个根组件，编写在 App.vue 中。在 src 下还有一个文件夹叫做 components，这个文件夹就是组件文件夹，用来存放其他组件。 组件的一个标志就是，其文件后缀名是 .vue。在一个 .vue 文件中，我们可以些三种标签：&lt;template&gt;、&lt;script&gt;、&lt;style&gt;。&lt;template&gt; 中写 html 结构，&lt;script&gt; 中写 js 或 ts 代码，&lt;style&gt; 中写 css 代码。直接写 &lt;script&gt; 可能会飘黄，这是因为 vue3 推荐我们使用 ts 语法，解决方法是添加 lang 属性：&lt;script lang='ts'&gt;。虽然 lang 声明了使用的语言，但是由于 ts 是 js 的超集，所以 js 也同样可以使用。 1.3 Options API 和 Composition API 在 vue2 中，我们可以在 &lt;script&gt; 标签中通过如下方式创建 API： 123456export default &#123; name: &#x27;App&#x27;, data() &#123;&#125;, computed: &#123;&#125;, methods: &#123;&#125;&#125; 这种 API 称为选项式 API (Options API)，其中所有的配置项，例如 name、data、computed、methods 等，这些都是一种选项。这种 API 的弊端在于过于分散，实现一个功能可能需要去修改各个配置项。 Vue3 对 Options API 进行了改进，提出了组合式 API (Composition API)，以功能为单位，将实现某一个功能的各种数据、方法、监视等放到一起，封装成一个个函数，比 Options API 更加集中。 02 setup 2.1 初识 setup Setup 是 vue3 提供的新配置项，其写法跟 Options API 是一样的： 123456789&lt;script lang=&#x27;ts&#x27;&gt; export default &#123; name: &#x27;App&#x27;, setup() &#123; ... return ... &#125; &#125;&lt;/script&gt; 可以看到，setup 在写法上类似于 vue2 中的 computed、methods，但在 vue3 当中，仅凭 setup 就能完成 data、computed、methods 等配置项的任务。如果我们需要定义新的数据或者方法，直接用 js 的语法在 setup 中定义就行了，例如： 123456789101112&lt;script lang=&#x27;ts&#x27;&gt; export default &#123; name: &#x27;App&#x27;, setup() &#123; let name = &#x27;xiaoming&#x27;; function changeName() &#123; name = &#x27;xiaozhi&#x27;; &#125; return &#123;name: name, changeName: changeName&#125;; &#125; &#125;&lt;/script&gt; 如上，我们就成功创建了数据 name 和方法 changeName，组件已经可以使用它们了。 return 的内容是一组键值对 (对象)，组件中若想使用数据或方法，需要使用键；而当键和值相同时，可以直接写值。例如上面的例子里就可以直接写成 return &#123;name, changeName&#125;;。 另外要注意的一点是，如上所写的内容还不是响应式的，意思是如果 changeName 方法被调用，name 被修改，页面上的内容也不会发生变化。如何解决这一点我们暂且不讲，因为这与 setup 的知识无关。 2.2 setup 和 Options API Vue3 是兼容 vue2 的，所以 Options API 的写法在 vue3 中仍能使用。这里就诞生了一个问题：我们在 data 或者 computed 这些 Options API 中写的数据，能否访问到 setup 中的内容？ 这里稍微复习一下 vue2 的知识，在 vue2，this 是一个极为重要的东西，它指向 app 组件本身，通过它可以读取到我们配置的所有属性和方法。那么现在，假设我们这样写代码： 123456789101112&lt;script lang=&#x27;ts&#x27;&gt; export default &#123; name: &#x27;App&#x27;, data() &#123; return &#123;a: this.name&#125;; // 重点在这！！ &#125;, setup() &#123; let name = &#x27;xiaoming&#x27;; return &#123;name&#125;; &#125; &#125;&lt;/script&gt; 请问，data 中设置的数据 a 能否正常访问到 setup 中的 name 呢？答案是可以。这里要引出一个很重要的知识点：==setup 的调用时机在 beforeCreate 之前==。如果你学过 vue2 你就会知道，beforeCreate 这个生命周期钩子是最早执行的生命周期钩子，在这一步，组件尚未被创建，this 还未存在，而 setup 的调用时机比 beforeCreate 还早，说明在 setup 中，我们是无法使用 this 的！可以说，vue3 已经在弱化 this 的作用了。Setup 中定义的数据和方法会在 this 被创建之后加入到 this 中，而 data 的调用时机则要晚得多，所以它是可以访问到 this 以及 this 上的数据 name 的。因此，上述代码是正确的。 通过上面的例子，我们主要要明确两点： Vue2 中 Options API 的写法是可以和 vue3 中 setup 的写法共存的。 Options API 可以访问到 setup 中的数据和方法，但是反过来不行。 2.3 setup 语法糖 所谓语法糖，就是能让代码更加简洁的写法。上面讲到的 setup 的写法过于繁琐，假设设置的数据和方法很多，那么 return 的内容也会很长，这是我们绝对不希望看到的。一种更加简洁的写法是： 123456&lt;script lang=&#x27;ts&#x27; setup&gt; let name = &#x27;xiaoming&#x27;; function changeName() &#123; name = &#x27;xiaozhi&#x27;; &#125;&lt;/script&gt; 只要在 &lt;script&gt; 标签中加入 setup 这个属性，我们就可以直接在其中写我们想在 setup 中写的内容了。并且我们所定义的数据和方法会自动返回，不需要手动写 return，这样可以极大方便我们编码。 但是这种写法还有一个小瑕疵，因为在原本的写法中，我们还要设置 name 这个配置项，所以我们还要保留这样一段代码： 12345&lt;script lang=&#x27;ts&#x27;&gt; export default &#123; name: &#x27;App&#x27; &#125;&lt;/script&gt; 这个 name 配置的是这个组件的名称，这个名称决定了父组件如何“称呼”它。这段代码是可以省略的，如果省略，那么组件的名字就默认是 vue 文件的名字。但是假设我们一定要这个组件的名字和文件名不同，那么上面这段代码就不能省略了。这样显然会显得很冗余，不过这里有一个插件可以解决这个问题，该插件可以通过下面这段代码进行安装： 1npm i vite-plugin-vue-setup-extend -D 光下载还不够，还需要找到项目目录下的 vite.config.ts 这个文件，对其做以下修改： 1234567891011121314151617import &#123; fileURLToPath, URL &#125; from &#x27;node:url&#x27;import &#123; defineConfig &#125; from &#x27;vite&#x27;import vue from &#x27;@vitejs/plugin-vue&#x27;import VueSetupExtend from &#x27;vue-plugin-vue-setup-extend&#x27; // 添加这句！！export default defineConfig(&#123; plugins: [ vue(), VueSetupExtend() // 这里也要改！！ ], resolve: &#123; alias: &#123; &#x27;@&#x27;: fileURLToPath(new URL(&#x27;./src&#x27;, import.meta.url)) &#125; &#125;&#125;) 然后，我们就只需要在 &lt;script&gt; 标签中配置一个 name 属性，就可以做到设置组件的名称了： 123456&lt;script lang=&#x27;ts&#x27; setup name=&#x27;App&#x27;&gt; let name = &#x27;xiaoming&#x27;; function changeName() &#123; name = &#x27;xiaozhi&#x27;; &#125;&lt;/script&gt; 有关于插件的知识仅作了解，因为大多数应用场景下组件的名称和组件文件的名称都是相同的。 03 响应式数据 这一节，我们来解决之前遗留的问题 —— 如何让在 setup 中设置的数据变成响应式的。 3.1 基本类型的响应式 在 vue3 当中，我们需要用到 ref 方法来实现基本类型数据的响应式。该方法的使用方法如下： 1234&lt;script lang=&#x27;ts&#x27; setup&gt; import &#123;ref&#125; from &#x27;vue&#x27;; // 要使用ref，必须先引入 let age = ref(19); // 这样就能实现age的响应式了&lt;/script&gt; 如此一来，age 就不再是一个单纯的整型变量，而是一个对象，如果在控制台中打印会显示： 由上图可见，age 被封装成了一个 RefImpl 对象，其中有一个属性 value，这个属性就是 age 的值。==当我们需要在 js/ts 中访问 age 的值时，我们就需要使用 age.value，否则是无法拿到 age 的值的；但是在 html 中不需要加 .value，系统会自动加上==。现在，我们尝试定义在组件中使用并修改此数据： 123456789101112131415161718&lt;script setup lang=&quot;ts&quot;&gt; import &#123;ref&#125; from &#x27;vue&#x27;; let age = ref(18); function ageInc() &#123; console.log(age); age.value += 1; &#125;&lt;/script&gt;&lt;template&gt; &lt;div&gt; &lt;p&gt;年龄：&#123;&#123; age &#125;&#125;&lt;/p&gt; &lt;button @click=&quot;ageInc&quot;&gt;增加年龄&lt;/button&gt; &lt;/div&gt;&lt;/template&gt;&lt;style scoped&gt;&lt;/style&gt; 运行上面的代码，应该能得到如下的效果： 可见，经过 ref 方法包装过之后，数据就变成了响应式数据，数据被改变，页面也会随之改变。 3.2 对象类型的响应式 上面介绍的是基本类型数据的响应式，如果是对象类型的数据，则可以选择使用 reactive 方法： 123456789101112131415161718192021222324252627282930313233343536373839&lt;script setup lang=&quot;ts&quot;&gt; import &#123; reactive &#125; from &#x27;vue&#x27;; let stud = reactive(&#123; &quot;name&quot;: &quot;王小明&quot;, &quot;age&quot;: 19, &quot;sex&quot;: &quot;男&quot;, &quot;address&quot;: &quot;浙江省杭州市xx区xx街xx号&quot; &#125;); function changeName() &#123; stud.name = &quot;荧姐&quot;; &#125; function changeAddress() &#123; stud.address = &quot;提瓦特大陆&quot;; &#125; function changeSex() &#123; stud.sex = &quot;女&quot;; &#125; function ageInc() &#123; stud.age++; &#125;&lt;/script&gt;&lt;template&gt; &lt;div&gt; &lt;h2&gt;学生信息：&lt;/h2&gt; &lt;li v-for=&quot;a, b in stud&quot; :key=&quot;b&quot;&gt;&#123;&#123; b &#125;&#125;: &#123;&#123; a &#125;&#125;&lt;/li&gt; &lt;hr&gt; &lt;li v-for=&quot;a in stud&quot; :key=&quot;a&quot;&gt;&#123;&#123; a &#125;&#125;&lt;/li&gt; &lt;/div&gt; &lt;div&gt; &lt;button @click=&quot;changeName&quot;&gt;改姓名&lt;/button&gt; &lt;button @click=&quot;changeSex&quot;&gt;改性别&lt;/button&gt; &lt;button @click=&quot;changeAddress&quot;&gt;改地址&lt;/button&gt; &lt;button @click=&quot;ageInc&quot;&gt;加年龄&lt;/button&gt; &lt;/div&gt;&lt;/template&gt;&lt;style scoped&gt;&lt;/style&gt; 效果： 在 js 中，数组也是一种对象，所以数组也可以使用 reactive。此外，对于深层的对象，reactive 也适用。reactive 的原理是 Proxy，这是 js 原生的一个方法。在控制台打印被 reactive 定义的数据，就会得到如下结果： 3.3 ref 和 reactive 虽然上面介绍的是 reactive 实现对象类型数据的响应式，但其实，ref 也可以实现。那么有什么区别呢？之前提到过，ref 定义的数据需要先访问 value 属性才能获取到数据，那么如果用 ref 定义对象类型数据也是同理。如果使用 ref 来代替 reactive，那么上面的 js 代码需要这么变： 12345678910111213141516171819202122&lt;script setup lang=&quot;ts&quot;&gt; import &#123; ref &#125; from &#x27;vue&#x27;; let stud = ref(&#123; &quot;name&quot;: &quot;王小明&quot;, &quot;age&quot;: 19, &quot;sex&quot;: &quot;男&quot;, &quot;address&quot;: &quot;浙江省杭州市xx区xx街xx号&quot; &#125;); console.log(stud); function changeName() &#123; stud.value.name = &quot;荧姐&quot;; &#125; function changeAddress() &#123; stud.value.address = &quot;提瓦特大陆&quot;; &#125; function changeSex() &#123; stud.value.sex = &quot;女&quot;; &#125; function ageInc() &#123; stud.value.age++; &#125;&lt;/script&gt; 既然如此，ref 和 reactive 在定义对象类型数据时有什么区别呢？其实，如果我们在控制台打印输出一下 ref 定义的对象类型数据就会立马看出它们之间的关联了： 从上图可以看出，当 ref 定义的是对象类型数据时，它会去请求 reactive 来帮忙，所以 RefImpl 对象上的属性 value 也变成了 Proxy 定义的对象。因此，如果 reactive 没有了，ref 也没法实现对象类型数据的响应式。 3.4 ref vs reactive 那么日常开发中到底使用 ref 还是 reactive 呢？虽然看起来 ref 使用起来要比 reactive 更加繁琐，但是 ref 也具有 reactive 不可比拟的优势，例如，reactive 定义的对象在重新赋值之后会失去响应式。看这个例子： 12345678910111213141516171819202122232425262728293031323334&lt;script setup lang=&quot;ts&quot;&gt; import &#123; reactive, ref &#125; from &#x27;vue&#x27;; let stud = reactive(&#123; // 创建响应式对象 &quot;name&quot;: &quot;王小明&quot;, &quot;age&quot;: 19, &quot;sex&quot;: &quot;男&quot;, &quot;address&quot;: &quot;浙江省杭州市xx区xx街xx号&quot; &#125;); console.log(&quot;原本的对象：&quot;, stud); function changeStud() &#123; // 为对象重新赋值 stud = &#123; &quot;name&quot;: &quot;荧姐&quot;, &quot;age&quot;: 19, &quot;sex&quot;: &quot;女&quot;, &quot;address&quot;: &quot;提瓦特大陆&quot; &#125;; console.log(&quot;现在的对象：&quot;, stud); &#125;&lt;/script&gt;&lt;template&gt; &lt;div&gt; &lt;h2&gt;学生信息：&lt;/h2&gt; &lt;li v-for=&quot;a, b in stud&quot; :key=&quot;b&quot;&gt;&#123;&#123; b &#125;&#125;: &#123;&#123; a &#125;&#125;&lt;/li&gt; &lt;hr&gt; &lt;li v-for=&quot;a in stud&quot; :key=&quot;a&quot;&gt;&#123;&#123; a &#125;&#125;&lt;/li&gt; &lt;/div&gt; &lt;div&gt; &lt;button @click=&quot;changeStud&quot;&gt;改变信息&lt;/button&gt; &lt;/div&gt;&lt;/template&gt;&lt;style scoped&gt;&lt;/style&gt; 运行结果如下： 可以看到，按钮点击之后，页面没有发生变化，但是通过控制台的打印信息可以发现，变量是被成功重新赋值了，但是新的变量并没有经过 Proxy 定义，所以不是响应式数据。 但是如果我们使用 ref 定义： 12345678910111213141516171819&lt;script setup lang=&quot;ts&quot;&gt; import &#123; reactive, ref &#125; from &#x27;vue&#x27;; let stud = ref(&#123; // 创建响应式对象 &quot;name&quot;: &quot;王小明&quot;, &quot;age&quot;: 19, &quot;sex&quot;: &quot;男&quot;, &quot;address&quot;: &quot;浙江省杭州市xx区xx街xx号&quot; &#125;); console.log(&quot;原本的对象：&quot;, stud); function changeStud() &#123; // 为对象重新赋值 stud.value = &#123; // 注意访问ref定义的对象数据要用.value &quot;name&quot;: &quot;荧姐&quot;, &quot;age&quot;: 19, &quot;sex&quot;: &quot;女&quot;, &quot;address&quot;: &quot;提瓦特大陆&quot; &#125;; console.log(&quot;现在的对象：&quot;, stud); &#125;&lt;/script&gt; 结果如下： 可以发现，即使被重新赋值，被 ref 定义的对象也仍然是响应式的。但是要注意，虽然 ref 定义的数据被赋值之后还能保持响应式，但这是有前提的，前提就是使用 .value，如果绕过了 value，直接去赋值，那结果跟 reactive 是一样的，响应式会消失。 不过，reactive 的这种缺点也不是不能克服的，当需要给对象重新赋值并且希望保留响应式时，可以使用 Object.assign 方法，该方法接收两个参数，第一个参数是原对象，第二个参数是要赋值的新对象： 12345678910111213141516171819&lt;script setup lang=&quot;ts&quot;&gt; import &#123; reactive, ref &#125; from &#x27;vue&#x27;; let stud = reactive(&#123; // 创建响应式对象 &quot;name&quot;: &quot;王小明&quot;, &quot;age&quot;: 19, &quot;sex&quot;: &quot;男&quot;, &quot;address&quot;: &quot;浙江省杭州市xx区xx街xx号&quot; &#125;); console.log(&quot;原本的对象：&quot;, stud); function changeStud() &#123; // 为对象重新赋值 Object.assign(stud, &#123; &quot;name&quot;: &quot;荧姐&quot;, &quot;age&quot;: 19, &quot;sex&quot;: &quot;女&quot;, &quot;address&quot;: &quot;提瓦特大陆&quot; &#125;); console.log(&quot;现在的对象：&quot;, stud); &#125;&lt;/script&gt; 在开发中，到底是使用 ref 还是使用 reactive，可以遵循以下原则： 如果需要响应式基本类型数据，直接使用 ref。 如果需要响应式对象类型数据，且层级不深，ref 和 reactive 均可。 如果需要响应式对象类型数据，且层级较深，推荐使用 reactive。 当然这个原则也不是绝对的，实际开发中按个人喜好来也没问题。 总结 ref 和 reactive 的区别： ref 既可以定义基本类型数据的响应式，也可以定义对象类型数据的响应式。 ref 定义的数据，不论是基本类型，还是对象类型，都必须加上 .value。 reactive 定义的对象类型数据不可以直接整体赋值，赋值之后的对象不再是原本的对象，会失去响应式，页面内容不会改变，不过可以用 Object.assign 方法进行赋值来弥补这种缺点；但是 ref 定义的对象类型数据可以直接赋值，且不会失去响应式，页面内容也会跟着变。 3.5 toRefs &amp; toRef 这两个 API 个人感觉作用不是很大，但是了解一下也好。解构赋值是 ES6 的新语法，它可以把一个对象中的属性拆解出来赋值给单个变量，例如： 12345let stud = &#123; &quot;name&quot;: &quot;xiaoming&quot;, &quot;age&quot;: 19&#125;;let &#123;name, age&#125; = stud; 如上，变量 name 和 age 将会继承 stud 对象上的 name 和 age 属性的值，这就是解构赋值。那么假设 stud 是一个 reactive 定义的响应式对象，使用解构赋值之后，改变 name 和 age 的值会影响到 stud 的值吗？页面上的内容会改变吗？直接说答案：不！解构赋值相当于是新建了两个变量，这两个变量只是得到了对象的属性值而已，跟对象无关。那么有没有办法使它们关联起来？当然是有的，这就是我们要介绍的 toRefs，我们可以把上述代码改成： 1234567891011121314&lt;script setup lang=&quot;ts&quot;&gt; import &#123; reactive, toRefs &#125; from &#x27;vue&#x27;; let stud = reactive(&#123; // 创建响应式对象 &quot;name&quot;: &quot;王小明&quot;, &quot;age&quot;: 19 &#125;); let &#123;name, age&#125; = toRefs(stud); function changeName() &#123; name.value = &quot;空哥&quot;; &#125; function ageInc() &#123; age.value++; &#125;&lt;/script&gt; toRefs 会将解构后的对象属性封装成一个 objectRefImpl 对象，这个对象与原来的对象相关联 (可以理解为指针)，改变其值会影响到响应式对象，而且会使页面刷新。 toRef 的作用和 toRefs 是一样的，只不过 toRef 是一次只处理一个属性，例如我只需要获取 stud 对象上的 name 的响应式数据，就可以这么做： 12345678&lt;script setup lang=&quot;ts&quot;&gt; import &#123; reactive, toRef &#125; from &#x27;vue&#x27;; let stud = reactive(&#123; // 创建响应式对象 &quot;name&quot;: &quot;王小明&quot;, &quot;age&quot;: 19 &#125;); let name = toRef(stud, &#x27;name&#x27;);&lt;/script&gt; 04 computed 计算属性 4.1 计算属性的用法 假设我们希望实现如下效果： 先来做需求分析： 页面内容受到用户行为影响发生变化，同时对应的数据要发生变化，即实现数据与页面之间的双向绑定，需要使用 v-model。 获取到用户输入后，要对用户输入进行处理 (首字母大写，加空格)，计算出新的数据，将新数据展示到页面上，需要用到计算属性。 所谓计算属性，就是根据已有数据生成的新数据，它能随着源数据的改变而改变。在 vue2 中，计算属性通过配置项 computed 定义，而在 vue3 中，computed 变成了一个方法，其接收一个函数作为参数，该函数的返回值就是计算属性的值。由于在 vue3 的 setup 中已经不再需要 this，所以这个函数可以放心地写箭头函数。 上述效果的 js 代码可以这样写： 1234567891011&lt;script setup lang=&quot;ts&quot;&gt; import &#123;ref, computed&#125; from &quot;vue&quot;; let firstName = ref(&quot;&quot;); let lastName = ref(&quot;&quot;); let fullName = computed(()=&gt;&#123; // 创建计算属性，实现首字母大写，并在姓和名之间加上空格 // slice方法对字符串进行切片，toUpperCase将字母转换为大写 let n1 = firstName.value.slice(0, 1).toUpperCase() + firstName.value.slice(1); let n2 = lastName.value.slice(0, 1).toUpperCase() + lastName.value.slice(1); return n1 + &quot; &quot; + n2; &#125;);&lt;/script&gt; 补全模板 (注意 v-model 的使用)： 123456789&lt;template&gt; &lt;div&gt; &lt;label for=&quot;firstName&quot;&gt;姓：&lt;/label&gt;&lt;input type=&quot;text&quot; id=&quot;firstName&quot; v-model=&quot;firstName&quot;/&gt; &lt;br&gt; &lt;label for=&quot;secondName&quot;&gt;名：&lt;/label&gt;&lt;input type=&quot;text&quot; id=&quot;lastName&quot; v-model=&quot;lastName&quot;/&gt; &lt;br&gt; &lt;p&gt;全名是: &#123;&#123; fullName &#125;&#125;&lt;/p&gt; &lt;/div&gt;&lt;/template&gt; 复习 v-model： v-model 将某个数据绑定到某个元素上，之后，该元素的 value 属性的变化将会影响到数据的变化，数据的变化也会影响到元素的变化，造成页面内容的改变，是一种双向数据绑定 (v-bind 是单向)。 v-model 只能用在具有 value 属性的元素上，例如 &lt;input&gt;、&lt;select&gt;、&lt;textarea&gt;。 打开 vue 的浏览器开发插件，查看该组件所有的数据，你能发现如下内容： firstName 和 lastName 都是 Ref 数据，而 fullName 是 Computed 数据，证明 fullName 的性质与 firstName 和 lastName 确实不一样。 计算属性的特点就是，==一旦它所依赖的数据发生变化，计算属性也会发生变化==。 4.2 setter 和 getter 如果我们在控制台打印一下 fullName 的值，我们会得到如下内容： 发现 fullName 是一个 ComputedRefImpl 对象，并且它和 RefImpl 一样有同样的结构，其中也有 value 这个属性。也就是说，假设我们要在 js 里访问 fullName 的值，也同样要加上 .value。那么问题来了：计算属性的值可不可以改变呢？也就是说我们可不可以写出 fullName.value = xxx 这种语句？答案是不可以！如果你这样写，编译器会提醒你计算属性的 value 值是一个只读属性，那么这是否意味着计算属性是不可修改的，只能依据源数据的改变而改变吗？答案也是否定的，这就要引出 getter 和 setter 了。 getter 和 setter 是两个方法，其中，getter 是计算属性的计算方法，setter 是当计算属性被直接修改时要调用的方法。我们之前的计算属性的创建方式只是省略了 setter 的一种写法，如果写完整，那么计算属性应该这样创建： 123456789let 属性名 = computed (&#123; get() &#123; ... return 属性值; &#125;, set(val) &#123; ... &#125;&#125;) 也就是说，computed 方法其实是有两个参数的，第一个参数是 get 方法，第二个参数是 set 方法，只不过第二个参数可以省略，set 方法不设置，我们就无法修改计算属性的值。 当 set 方法被设置，我们就可以去使用 fullName.value = xxx 语句来对计算属性进行赋值，但是赋值不是直接进行的，这一行为只会引起 set 方法的调用，此时，赋值符号右边的 xxx 将会作为参数传到 set 方法中。计算属性是依赖于其他数据的数据，所以计算属性被修改，而其依赖数据不改变是不合适的，因此我们可以在 set 方法中去修改源数据，而修改源数据又会引起 get 方法的调用，从而达到修改计算属性的目的。下面是一个示例： 12345678910111213141516171819202122232425262728293031&lt;script setup lang=&quot;ts&quot;&gt; import &#123;ref, computed&#125; from &quot;vue&quot;; let firstName = ref(&quot;&quot;); let lastName = ref(&quot;&quot;); let fullName = computed(&#123; get() &#123; // getter let n1 = firstName.value.slice(0, 1).toUpperCase() + firstName.value.slice(1); let n2 = lastName.value.slice(0, 1).toUpperCase() + lastName.value.slice(1); return n1 + &quot; &quot; + n2; &#125;, set(val) &#123; // setter let [f, l] = val.split(&quot; &quot;); firstName.value = f; lastName.value = l; &#125; &#125;); function changeFullName() &#123; fullName.value = &quot;li xiaolang&quot;; &#125;&lt;/script&gt;&lt;template&gt; &lt;div&gt; &lt;label for=&quot;firstName&quot;&gt;姓：&lt;/label&gt;&lt;input type=&quot;text&quot; id=&quot;firstName&quot; v-model=&quot;firstName&quot;/&gt; &lt;br&gt; &lt;label for=&quot;secondName&quot;&gt;名：&lt;/label&gt;&lt;input type=&quot;text&quot; id=&quot;lastName&quot; v-model=&quot;lastName&quot;/&gt; &lt;br&gt; &lt;p&gt;全名是: &#123;&#123; fullName &#125;&#125;&lt;/p&gt; &lt;button id=&quot;confirm&quot; @click=&quot;changeFullName&quot;&gt;更改全名为LiXiaolang&lt;/button&gt; &lt;/div&gt;&lt;/template&gt; 效果： 05 watch 监视 Vue3 中 watch 这个 API 的作用跟 vue2 是一样的，就是==监视数据的变化==。Vue 的官网明确了 watch 能监视的四种数据： ref 定义的数据。 reactive 定义的数据。 getter 函数的返回值。 由上述数据组成的数组。 虽然 watch 能监视的数据类型是 4 种，但是实际使用中我们能遇到 5 种情况，下面逐一介绍。 5.1 情况一：ref 定义的基本类型数据 在 Vue3 中，watch 是一个函数，其基本使用方法为： 123watch(要监视的数据, (newValue, oldValue)=&gt;&#123; // 接收两个参数，第一个是要监视的数据，第二个是一个回调函数 ...&#125;); 当被监视的数据发生变化时，回调函数就会被调用，并且新值和旧值会作为参数被传入。下面是一个例子： 123456789101112131415161718&lt;script setup lang=&quot;ts&quot;&gt; import &#123;ref, watch&#125; from &#x27;vue&#x27;; let age = ref(18); watch(age, (newValue, oldValue)=&gt;&#123; console.log(&quot;age变化了&quot;, &quot;新值：&quot;, newValue, &quot; 旧值：&quot;, oldValue); &#125;); function ageInc() &#123; age.value += 1; &#125;&lt;/script&gt;&lt;template&gt; &lt;div&gt; &lt;p&gt;年龄：&#123;&#123; age &#125;&#125;&lt;/p&gt; &lt;button @click=&quot;ageInc&quot;&gt;增加年龄&lt;/button&gt; &lt;/div&gt;&lt;/template&gt; 效果： 这里要注意的一件事是：watch 的第一个参数要传变量，不要加 .value。 如何停止监视？ watch 有返回值，该返回值是一个函数，通过调用该函数，我们可以解除监视。假设有需求：当 age 增加到 22 就不再进行监视，那么我们可以这样写： 12345678910111213&lt;script setup lang=&quot;ts&quot;&gt; import &#123;ref, watch&#125; from &#x27;vue&#x27;; let age = ref(18); let stopWatch = watch(age, (newValue, oldValue)=&gt;&#123; // 将watch的返回值接住 console.log(&quot;age变化了&quot;, &quot;新值：&quot;, newValue, &quot; 旧值：&quot;, oldValue); if (newValue &gt;= 22) &#123; stopWatch(); // 调用stopWatch &#125; &#125;); function ageInc() &#123; age.value += 1; &#125;&lt;/script&gt; 这样就会得到： 5.2 情况二：ref 定义的对象类型数据 对象类型数据的监视稍有特殊，先来看结果，我们定义一个对象，该对象上有两个属性，我们再定义三个按钮，三个按钮的作用分别是修改对象的两个属性及对象整体的值，代码如下： 1234567891011121314151617181920212223242526272829303132&lt;script setup lang=&quot;ts&quot;&gt; import &#123;ref, watch&#125; from &#x27;vue&#x27;; let stud = ref(&#123; name: &quot;王小明&quot;, age: 15 &#125;); watch(stud, (newValue, oldValue)=&gt;&#123; console.log(&quot;对象被修改了&quot;, newValue, oldValue); &#125;); function ageInc() &#123; stud.value.age++; &#125; function changeName() &#123; stud.value.name += &quot;~&quot;; &#125; function changeAll() &#123; stud.value = &#123; name: &quot;李小狼&quot;, age: 20 &#125; &#125;&lt;/script&gt;&lt;template&gt; &lt;div&gt; &lt;p&gt;姓名：&#123;&#123; stud.name &#125;&#125;&lt;/p&gt; &lt;p&gt;年龄：&#123;&#123; stud.age &#125;&#125;&lt;/p&gt; &lt;button style=&quot;margin-right: 2px;&quot; @click=&quot;ageInc&quot;&gt;增加年龄&lt;/button&gt; &lt;button style=&quot;margin-right: 2px;&quot; @click=&quot;changeName&quot;&gt;修改姓名&lt;/button&gt; &lt;button style=&quot;margin-right: 2px;&quot; @click=&quot;changeAll&quot;&gt;更改整个对象&lt;/button&gt; &lt;/div&gt;&lt;/template&gt; 我们将得到以下效果： 可以发现，当一个对象被监视时，对象上的属性发生改变都不会引起 watch 的注意，但是当对象整体被重新赋值时，watch 就出动了，也就是说 wacth 监视对象时，它监视的是对象的地址。 如果想要 wacth 能够监视到对象的属性值的变化，就需要开启深度监视，开启的方式在于 watch 的第三个参数。watch 的第三个参数是一个对象，用于传入各种配置项，其中有一个属性就叫 deep，当该属性被设置为 true，深度监视就会开启，watch 将能检测到对象属性值的改变。所以我们将逻辑代码重写为： 123watch(stud, (newValue, oldValue)=&gt;&#123; console.log(&quot;对象被修改了&quot;, newValue, oldValue);&#125;, &#123;deep: true&#125;); // 开启深度监视 此时，页面的展示效果就会变成： 但是注意，这里有一个 bug，仔细观察上面控制台输出的内容，你会发现，当对象属性被修改时，传入回调函数的两个参数 —— newValue 和 oldValue 是一样的，只有当对象整体被修改时，newValue 和 oldValue 才不一样。这个 bug 其实自 vue2 起就存在了，不过由于大多数情况下我们都不需要去管 oldValue，不影响正常使用，所以问题不大，就算回调函数中只写一个参数也是没问题的。 5.3 情况三：reactive 定义的对象类型数据 watch 用在 reactive 定义的对象类型数据时，造成的效果与 ref，也就是情况二正好相反。当 wacth 发现要监视的对象是由 reactive 定义的时，就会自动开启深度监视，并且，根据之前所学的知识，reactive 定义的对象数据不支持整体赋值，所以直接改变 reactive 定义的对象反而不会被监视到。看下面这个例子： 1234567891011121314151617181920212223242526272829303132&lt;script setup lang=&quot;ts&quot;&gt; import &#123;reactive, ref, watch&#125; from &#x27;vue&#x27;; let stud = reactive(&#123; name: &quot;王小明&quot;, age: 15 &#125;); watch(stud, (newValue, oldValue)=&gt;&#123; console.log(&quot;对象被修改了&quot;, newValue, oldValue); &#125;); function ageInc() &#123; stud.age++; &#125; function changeName() &#123; stud.name += &quot;~&quot;; &#125; function changeAll() &#123; stud = &#123; name: &quot;李小狼&quot;, age: 20 &#125; &#125;&lt;/script&gt;&lt;template&gt; &lt;div&gt; &lt;p&gt;姓名：&#123;&#123; stud.name &#125;&#125;&lt;/p&gt; &lt;p&gt;年龄：&#123;&#123; stud.age &#125;&#125;&lt;/p&gt; &lt;button style=&quot;margin-right: 2px;&quot; @click=&quot;ageInc&quot;&gt;增加年龄&lt;/button&gt; &lt;button style=&quot;margin-right: 2px;&quot; @click=&quot;changeName&quot;&gt;修改姓名&lt;/button&gt; &lt;button style=&quot;margin-right: 2px;&quot; @click=&quot;changeAll&quot;&gt;更改整个对象&lt;/button&gt; &lt;/div&gt;&lt;/template&gt; 效果： 可以发现，当对象被整体改变时，对象地址发生了改变，对象失去了响应式，断开了与页面的联系，并且 watch 也监视不到了。 但是如果我们使用 Object.assign 方法来改变对象的整体值，那么情况就会大不一样： 123456function changeAll() &#123; Object.assign(stud, &#123; name: &quot;李小狼&quot;, age: 20 &#125;);&#125; 效果： 总结：监视 reactive 定义的对象时，默认开启深度监视，而且这个深度监视无法通过配置项进行关闭。 5.4 情况四：监视对象上的某个值 上面展示的例子都是监视整个对象，那么如果现在我们只需要监视一个对象上的某个属性呢？这就是情况四，不过，根据属性值类型的不同，还要分成两种情况： 属性值是基本类型数据。 属性值是对象类型数据。 为了方便演示，我们先定义一下演示要用的例子： 123456789101112131415161718192021222324252627282930313233343536373839&lt;script setup lang=&quot;ts&quot;&gt; import &#123;reactive, ref, watch&#125; from &#x27;vue&#x27;; let person = reactive(&#123; name: &quot;王小明&quot;, age: 15, cars: &#123; c1: &quot;宝马&quot;, c2: &quot;奥迪&quot; &#125; &#125;); watch(person, (newValue, oldValue)=&gt;&#123; console.log(&quot;对象被修改了&quot;, newValue, oldValue); &#125;); function ageInc() &#123; person.age++; &#125; function changeName() &#123; person.name += &quot;~&quot;; &#125; function changeC1() &#123; person.cars.c1 = &quot;大众&quot;; &#125; function changeC2() &#123; person.cars.c2 = &quot;奔驰&quot;; &#125;&lt;/script&gt;&lt;template&gt; &lt;div&gt; &lt;p&gt;姓名：&#123;&#123; person.name &#125;&#125;&lt;/p&gt; &lt;p&gt;年龄：&#123;&#123; person.age &#125;&#125;&lt;/p&gt; &lt;p&gt;第一辆车：&#123;&#123; person.cars.c1 &#125;&#125;&lt;/p&gt; &lt;p&gt;第二辆车：&#123;&#123; person.cars.c2 &#125;&#125;&lt;/p&gt; &lt;button style=&quot;margin-right: 2px;&quot; @click=&quot;ageInc&quot;&gt;增加年龄&lt;/button&gt; &lt;button style=&quot;margin-right: 2px;&quot; @click=&quot;changeName&quot;&gt;修改姓名&lt;/button&gt; &lt;button style=&quot;margin-right: 2px;&quot; @click=&quot;changeC1&quot;&gt;修改第一辆车&lt;/button&gt; &lt;button style=&quot;margin-right: 2px;&quot; @click=&quot;changeC2&quot;&gt;修改第二辆车&lt;/button&gt; &lt;/div&gt;&lt;/template&gt; 上例中，我们监视的仍然是对象的整体，所以修改对象上的任何一个属性，都会被监视到： 5.4.1 基本类型属性 现在，我们需求是：只监视 person 对象上的年龄属性，其他属性都不监视，我们需要这样写监视函数： 123watch(()=&gt;&#123;return person.age&#125;, (newValue, oldValue)=&gt;&#123; console.log(&quot;对象被修改了&quot;, newValue, oldValue);&#125;); 注意，此时，watch 的第一个参数不再是对象本身，而是一个函数，这就是我们之前说的 watch 可监视函数返回值。每当对象被修改，watch 就根据这个函数的返回值来判断要不要调用监视函数，这样一来，只要 person.age 不发生改变，watch 就不会调用监视函数。换句话来说，这样就实现了只监视对象上的某一属性。作出上述修改后，页面的效果就会变成： 另外，由于这里我们所写的箭头函数只有一个 return 语句，所以可以进一步简写为： 123watch(()=&gt;person.age, (newValue, oldValue)=&gt;&#123; console.log(&quot;对象被修改了&quot;, newValue, oldValue);&#125;); 5.4.2 对象类型属性 如果我们只需要监视对象上的某个属性，而且这个属性又是一个对象，那么我们有两种监视的方法，先介绍第一种 —— 直接监视这个对象属性，例如我们要监视上例中的 person.cars，代码可以这么写： 123watch(stud.cars, (newValue, oldValue)=&gt;&#123; console.log(&quot;对象被修改了&quot;, newValue, oldValue);&#125;) 效果为： 但是注意：如果属性值是基本类型不能这样写，会报错，必须按 5.4.1 所讲的方法进行设置。 不过这样做有个弊端，那就是==当 person.cars 被整体替换，也就是有语句 persons.cars = &#123;xxx&#125; 时，监视就不再起作用了==： 那有什么解决方法吗？一种方法是使用 Object.assign，因为这个方法本质上是修改属性值，不会替换整个对象；另一种方法就是 watch 的另一种写法 —— 监视一个函数的返回值，也就是使用同 5.4.1 一样的写法： 123watch(()=&gt;person.cars, (newValue, oldValue)=&gt;&#123; console.log(&quot;对象被修改了&quot;, newValue, oldValue);&#125;); 这样一来，当对象整体被替换，watch 照样能监视，不过这种写法同样有弊端，那就是修改属性对象上的某个值时，它又监视不到了，和前一种方法正好相反： 但是好在这种弊端可以通过开启深度监视来解决： 123watch(()=&gt;person.cars, (newValue, oldValue)=&gt;&#123; console.log(&quot;对象被修改了&quot;, newValue, oldValue);&#125;, &#123;deep: true&#125;); 效果： 总结：当需要监视对象上的某个属性值时，如果要监视的属性是基本类型，那么使用函数式；如果要监视的属性是对象类型，函数式或变量均可，但是推荐使用函数式，但是函数式只能监视到地址的变化，如果需要监视到细枝末节的变化 (对象上的某一属性发生改变) 就需要开启深度监视。 5.5 情况五：监视对象上的某些值 如果我们想一次性监视对象上的多个值该怎么做？一个个去绑定？这样太繁琐，更简单的方式是使用数组，例如我们需要监视 person 对象上的 age、c1 这两个属性，那么我们可以这样写： 123watch([()=&gt;person.age, ()=&gt;person.cars.c1], (newValue, oldValue)=&gt;&#123; console.log(&quot;对象被修改了&quot;, newValue, oldValue);&#125;); 这样以来，我们就只监视了 person 上的两个属性： 5.6 watchEffect 假设我们有一个需求：当检测到水温达到 60℃，或者水位达到 50cm 时，就向服务器发送请求，那么基于我们上面所学的知识，我们要做的就是对水温和水位这两个变量进行监视，在回调函数中写一个 if 语句来进行逻辑判断。这样写的一个弊端是：如果涉及到的变量很多，例如这里只涉及到水温和水位两个，但万一遇到十几个，那么岂不是要监视很多变量，watch 的参数列表长度会变得很长，这是我们不希望看到的。这个时候，watchEffect 就派上用场了。 watchEffect：立即运行一个函数，同时响应式地追踪其依赖，并在依赖更改时重新执行该函数。 watchEffect 可以看作是 watch 的全自动版，它不需要用户指定要监视的变量，而是可以根据用户写的回调函数去猜要监视哪些变量，例如为了应对上面的需求，我们就可以这样写： 12345678import &#123;ref, watchEffect&#125; from &quot;vue&quot;;let sw = ref(0); // 表示水位let wd = ref(0); // 表示温度watchEffect(()=&gt;&#123; if (wd.value &gt;= 60 || sw.value &gt;= 50) &#123; console.log(&quot;发送请求&quot;); &#125;&#125;); 这样，当 wd 或 sw 被修改时，监视函数就会立刻被调用： 要注意一点是，watchEffect 会在一开始就调用一次回调函数，也就是说即使一开始变量没发生改变，回调函数也会被调用一次，相当于是 immediate 配置项被设为 true 的 watch。 06 标签的 ref 属性 本节重点：ref 标记、defineExpose 方法。 6.1 ref 属性 虽然 vue 中的组件是分散开的，写在一个个 .vue 文件中，但是最终渲染的时候，所有组件还是会被封装到一起去，大多数时候都是一大群人开发一个项目，每个人负责写一部分组件，这就会导致一个问题：组件 id 有可能会重复，完全有可能出现两个人或多个人给各自的标签用了同一个 id 的情况，这是我们不希望出现的。 举个例子，假设 App 组件中，我们给一个标签设置 id 为 title： 12345678&lt;script setup lang=&quot;ts&quot;&gt; import Person from &quot;./components/Person.vue&quot;;&lt;/script&gt;&lt;template&gt; &lt;h1 id=&quot;title&quot;&gt;你好&lt;/h1&gt; &lt;Person&gt;&lt;/Person&gt;&lt;/template&gt; 然后，我们在引入的 Person.vue 中写入如下内容，同样给一个标签设置 id 为 title，并且在控制台输出一下 id 为 title 的 DOM 元素： 1234567&lt;script setup lang=&quot;ts&quot;&gt; console.log(document.getElementById(&quot;title&quot;));&lt;/script&gt;&lt;template&gt; &lt;h1 id=&quot;title&quot;&gt;今天天气真好&lt;/h1&gt;&lt;/template&gt; 得到控制台的输出为： 我们会发现，虽然我们是在 Person.vue 里使用了 getElementById(\"title\")，但是获取到的 DOM 元素却是 App.vue 中的 &lt;h1&gt; 标签，这是因为 App.vue 中的内容先于子组件被渲染。可见，id 冲突会带来很大的麻烦。 为了解决这个问题，vue 提供了 ref 属性，用来代替 id，将 id 改为 ref 之后，我们可以用这种方式来获取对应的 DOM 元素： Person.vue： 123456789101112&lt;script setup lang=&quot;ts&quot;&gt; import &#123;ref&#125; from &quot;vue&quot;; let title = ref(); function getTitle() &#123; console.log(title.value); &#125;&lt;/script&gt;&lt;template&gt; &lt;h1 ref=&quot;title&quot;&gt;今天天气真好&lt;/h1&gt; &lt;button @click=&quot;getTitle&quot;&gt;点我获取title&lt;/button&gt;&lt;/template&gt; App.vue： 12345678910111213&lt;script setup lang=&quot;ts&quot;&gt; import &#123;ref&#125; from &quot;vue&quot;; let title = ref(); function getTitle() &#123; console.log(title.value); &#125;&lt;/script&gt;&lt;template&gt; &lt;h1 ref=&quot;title&quot;&gt;你好&lt;/h1&gt; &lt;button @click=&quot;getTitle&quot;&gt;点我获取App中的title&lt;/button&gt; &lt;Person&gt;&lt;/Person&gt;&lt;/template&gt; 没错，就是这么简单。ref 在不传入任何参数的时候，就会去寻找 DOM 元素，并且，我们的==变量名必须与要寻找的 DOM 元素的 ref 属性相同==才行。这样得到的效果为： 这里要注意：ref 标记的标签不能直接获取，要像上面一样写成按钮的点击事件，否则将输出 undefined，猜测原因是直接写在 js 环境中，js 代码会在 vue 还没有处理 ref 标记前被执行。 6.2 defineExpose 现在考虑这样一种情况：如果我们给组件标签加上 ref 标签会怎么样？会返回组件的 DOM 元素吗？如果会，那我们能通过组件的 DOM 元素去访问组件下的全部数据吗？先说答案：可以，但是无法访问子组件数据。 我们现在给 Person 组件加上 ref 标记： 1234567891011121314151617181920&lt;script setup lang=&quot;ts&quot;&gt; import Person from &quot;./components/Person.vue&quot;; import &#123;ref&#125; from &quot;vue&quot;; let title = ref(); let test = ref(); // 获取Person组件的DOM元素 function getTitle() &#123; console.log(title.value); &#125; function getPerson() &#123; console.log(test.value); &#125;&lt;/script&gt;&lt;template&gt; &lt;h1 ref=&quot;title&quot;&gt;你好&lt;/h1&gt; &lt;button @click=&quot;getTitle&quot;&gt;点我获取App中的title&lt;/button&gt; &lt;Person ref=&quot;test&quot;&gt;&lt;/Person&gt; &lt;br&gt;&lt;br&gt; &lt;button @click=&quot;getPerson&quot;&gt;点我获取Person组件&lt;/button&gt;&lt;/template&gt; 同时，我们在 Person.vue 中新建几个数据： 123456789101112131415&lt;script setup lang=&quot;ts&quot;&gt; import &#123;ref&#125; from &quot;vue&quot;; let title = ref(); function getTitle() &#123; console.log(title.value); &#125; let a = ref(0); let b = ref(1); let c = ref(2);&lt;/script&gt;&lt;template&gt; &lt;h1 ref=&quot;title&quot;&gt;今天天气真好&lt;/h1&gt; &lt;button @click=&quot;getTitle&quot;&gt;点我获取Person中的title&lt;/button&gt;&lt;/template&gt; 控制台输出的内容为： 虽然控制台正确输出了 Person 组件的对象，但是其中并没有任何与 Person 中数据有关的信息。这是 vue 的保密措施，虽说 Person 是 App 的子组件，但这并不意味着父组件就拥有访问子组件的全部数据的权限。如果需要使子组件向父组件展示数据，让父组件可以通过子组件的对象来访问子组件中的数据，就要借助一个 API —— defineExpose。 其使用方法为： 12345import &#123;defineExpose&#125; from &#x27;vue&#x27;;let a = ref(0);let b = ref(1);let c = ref(2);defineExpose(&#123;a, b, c&#125;); 这样一来，我们再来看看控制台输出的内容： 可以看到，现在 Person 组件的对象中就出现了 a、b、c 三个数据的信息了。 07 props 7.1 props 的使用 当父组件需要给子组件传值的时候，props 就派上用场了。 先来看看父组件如何给子组件传值，如果父组件要给子组件传值，只需要在子组件的标签上加一个属性即可，属性名就是要传过去的数据的名字，属性值就是要传过去的值： 1234567&lt;script setup lang=&quot;ts&quot;&gt; import Student from &#x27;./components/Student.vue&#x27;;&lt;/script&gt;&lt;template&gt; &lt;Student name=&quot;李小狼&quot;&gt;&lt;/Student&gt; // 传值&lt;/template&gt; 子组件中，可以通过 defineProps 函数来进行接收，该函数接收一个列表，列表中为要接收的变量名称，且为字符串： 12345678&lt;script setup lang=&quot;ts&quot;&gt; import &#123; defineProps &#125; from &#x27;vue&#x27;; defineProps([&#x27;name&#x27;]);&lt;/script&gt;&lt;template&gt; &lt;p&gt;姓名：&#123;&#123; name &#125;&#125;&lt;/p&gt;&lt;/template&gt; 如此一来，子组件中就能正常展示父组件传来的数据了： 但是这还不够，虽然在 template 中可以顺利访问到 name，但是在 script 中是访问不到的，毕竟变量都没有创建，直接访问肯定会出错，那么如何在 js 中保存传过来的数据呢？defineProps 函数是有返回值的，返回值就是封装了所有引入数据的对象，我们可以打印一下 defineProps 的返回值： 假设现在父组件不再传简单的基本类型数据，而是对象类型数据： 123456789101112&lt;script setup lang=&quot;ts&quot;&gt; import Student from &#x27;./components/Student.vue&#x27;; let person = &#123; name: &quot;小樱&quot;, sex: &quot;女&quot;, age: &quot;15&quot; &#125;&lt;/script&gt;&lt;template&gt; &lt;Student name=&quot;李小狼&quot; :person=&quot;person&quot;&gt;&lt;/Student&gt;&lt;/template&gt; 注意：这里必须要在属性名前加上 :，因为只有加上 :，vue 才会将等号后面的内容当作表达式去处理。随后只要在子组件中正确接收数据就可以了： 1234567891011&lt;script setup lang=&quot;ts&quot;&gt; import &#123; defineProps &#125; from &#x27;vue&#x27;; let x = defineProps([&#x27;name&#x27;, &#x27;person&#x27;]); console.log(x);&lt;/script&gt;&lt;template&gt; &lt;p&gt;姓名：&#123;&#123; person.name &#125;&#125;&lt;/p&gt; &lt;p&gt;性别：&#123;&#123; person.sex &#125;&#125;&lt;/p&gt; &lt;p&gt;年龄：&#123;&#123; person.age &#125;&#125;&lt;/p&gt;&lt;/template&gt; 页面效果： 同时，由于我们在 js 中用变量 x 接收了父组件传过来的所有数据，所以也可以用 x 来进行访问： 1234567891011&lt;script setup lang=&quot;ts&quot;&gt; import &#123; defineProps &#125; from &#x27;vue&#x27;; let x = defineProps([&#x27;name&#x27;, &#x27;person&#x27;]); console.log(x);&lt;/script&gt;&lt;template&gt; &lt;p&gt;姓名：&#123;&#123; x.person.name &#125;&#125;&lt;/p&gt; &lt;p&gt;性别：&#123;&#123; x.person.sex &#125;&#125;&lt;/p&gt; &lt;p&gt;年龄：&#123;&#123; x.person.age &#125;&#125;&lt;/p&gt;&lt;/template&gt; 总之显示效果是一样的就是了。 7.2 类型限制 上面介绍的方法有一个缺点：父组件给我们的数据是什么类型的我们是不知道的，例如，我们需要一个列表，但是父组件却给了我们一个数字，那么页面可能就会出错，那么有没有办法对传过来的数据进行类型限制呢？这里就要用到 ts 中的泛型了。 TypeScript 的内容在这里就不过多展开了，有兴趣请自行了解，这里只介绍如何对 props 的类型做出限制： 1defineProps&lt;&#123;变量名1:数据类型, 变量名2:数据类型&#125;&gt;(); 假设父组件给我们一个 name 数据，子组件希望这个数据是字符串类型的，那么就可以这么写： 1defineProps&lt;&#123;name: string&#125;&gt;(); 这个时候，假如父组件给我们的数据是其他类型，比如说数字，那么就会飘红： 这里为什么要在 name 属性前加 :？因为只有加了 : 才会将后面的内容当作表达式执行，如果不加，那传过去的就只是一个字符串类型的 1。 7.3 必要性限制 假如和上面一样，子组件中已经对 name 做出了类型限制： 1defineProps&lt;&#123;name: string&#125;&gt;(); 那么要注意，不仅类型不对会飘红，父组件不传该值也会飘红： 相当于说，上面这种写法不仅对数据类型做了限制，对数据的必要性也做了限制，那么如何设置一个可选的数据，使得就算父组件不传也不会报错呢？很简单，方法就是在变量名后面加个问号 ?： 1defineProps&lt;&#123;name?: string&#125;&gt;(); 这样一来上面的报错就消失了。不仅如此，我们还可以为可选数据设置默认值，这要用到另一个 API —— withDefaults，比如说为 name 设置一个默认值： 123withDefaults(defineProps&lt;&#123;name?: string&#125;&gt;(), &#123; name: ()=&gt;&quot;木之本樱&quot;&#125;) 08 生命周期 8.1 Vue3 的生命周期 下面是官方给出的 vue3 生命周期图： 生命周期，说白了，就是 vue 构建、更新和销毁的过程。在每个阶段，vue 都会去调用一些特定的函数，这些函数我们称之为生命周期函数，也称为钩子。我们可以利用这些函数，在每个阶段做一些特别的事情。上图中红框中标出的，就是 vue3 中的生命周期中的各个阶段。 Vue3 中的第一个生命周期钩子是 setup，这个其实是老朋友了，我们已经用了很久了。从上图中可以看出，setup 在 beforeCreate 之前被调用，说明 setup 被调用的时候，vue 的构建都还没正式开始呢。所以我们说，setup 里没有 this 了。 beforeCreate 是 vue3 的第二个生命阶段，此时 vue 仍为开始构建实例化对象。 created 是 vue3 的第三个生命阶段，此时 vue 对象构建完成，this 已经存在，并且已经将 setup 中我们设置的数据、函数等都已经配置好了。 之后就是 beforeMounte，这一步称为挂载之前，所谓挂载就是指将编译好的模板渲染到页面上去的过程。在 beforeMounte 中，页面展示的是还未经编译的结构，我们对 DOM 元素做出的任何修改都不作数。 在 mounted 之后，页面就已经渲染完成了，能够正常显示，之后如果我们对页面做出任何修改，比如响应式数据发生改变，都会导致页面更新，这时就会进入 beforeUpdate 和 updated 这两个生命阶段，且这两个生命阶段可能被重复数次。 最后就是 vue 对象的销毁，需要将页面“卸载”，也就是 beforeUnmount 和 unmounted 阶段，在这里我们可以做一些善后工作。 8.2 生命周期钩子的使用 上面介绍了 vue3 的生命周期，但是生命周期函数的名称并不是那也一一对应，vue3 对生命周期做出了一些调整，例如在 vue2 中，还存在 beforeCreate 和 onCreated 这两个钩子，但是在 vue3 中这两个钩子都没了，直接用一个 setup 就替代了；再比如 vue2 中最后一个阶段叫销毁 (destroy)，但在 vue3 中变成了卸载 (unmount)。所以就有了下面这张表格： 阶段 钩子 创建 setup 挂载 (之前，之后) onBeforeMount、onMounted 更新 (之前，之后) onBeforeUpdate、onUpdated 卸载 (之前，之后) onBeforeUnmount、onUnmounted ==每个钩子都接收一个回调函数作为参数，当进入特定生命阶段时，就会调用该钩子接收到的回调函数。== 为了演示每个钩子的调用时机，我们在父组件中用 v-if 来控制子组件的销毁： 1234567891011121314&lt;script setup lang=&quot;ts&quot;&gt; import LifeCycle from &#x27;./components/LifeCycle.vue&#x27;; import &#123;ref&#125; from &#x27;vue&#x27;; let isShown = ref(true); function destroy() &#123; isShown.value = false; &#125;&lt;/script&gt;&lt;template&gt; &lt;LifeCycle v-if=&quot;isShown&quot;&gt;&lt;/LifeCycle&gt; &lt;br&gt;&lt;br&gt; &lt;button @click=&quot;destroy&quot;&gt;销毁子组件&lt;/button&gt;&lt;/template&gt; 在子组件 LifeCycle 中，我们分别调用所有的钩子： 1234567891011121314151617181920212223242526272829303132333435363738&lt;script setup lang=&quot;ts&quot;&gt; import &#123;ref, onBeforeMount, onMounted, onBeforeUpdate, onUpdated, onBeforeUnmount, onUnmounted&#125; from &quot;vue&quot;; let age = ref(10); function ageInc() &#123; age.value++; &#125; // 创建 console.log(&quot;创建&quot;); // 挂载前 onBeforeMount(()=&gt;&#123; console.log(&quot;挂载前&quot;); &#125;); // 挂载后 onMounted(()=&gt;&#123; console.log(&quot;挂载后&quot;); &#125;); // 更新前 onBeforeUpdate(()=&gt;&#123; console.log(&quot;更新前&quot;); &#125;); // 更新后 onUpdated(()=&gt;&#123; console.log(&quot;更新后&quot;); &#125;); // 卸载前 onBeforeUnmount(()=&gt;&#123; console.log(&quot;卸载前&quot;); &#125;); // 卸载后 onUnmounted(()=&gt;&#123; console.log(&quot;卸载后&quot;); &#125;);&lt;/script&gt;&lt;template&gt; &lt;p&gt;年龄：&#123;&#123; age &#125;&#125;&lt;/p&gt; &lt;button @click=&quot;ageInc&quot;&gt;年龄加一&lt;/button&gt;&lt;/template&gt; 最后观察到的结果： 这里补充一点：子组件和父组件都有自己的生命周期，那么到底是子组件先挂载，还是父组件先挂载？卸载的时候呢？其实可以把这一结构想象成一个栈，==挂载的时候子组件先挂载，卸载的时候父组件先卸载==。 09 自定义 hooks 还记得我们在第一章的时候说过，vue3 的组合式 API 的一大优势，就是可以将不同功能模块的代码分开吗？但是貌似到目前为止，我们都没有体会到这一点，不过不用担心，这一章就是来回收第一章的伏笔的。 hooks，说白了，就是把相同功能的代码封装到一块去，放到一个 js 或 ts 文件里，这个文件就被称为一个 hook。 来看看下面这个例子： 1234567891011121314151617181920212223242526272829303132333435&lt;script setup lang=&quot;ts&quot;&gt; import &#123;ref, watch&#125; from &quot;vue&quot;; import axios from &quot;axios&quot;; let count = ref(0); let dogList = ref([&quot;https://images.dog.ceo/breeds/pembroke/n02113023_6570.jpg&quot;]); function countNum() &#123; count.value++; &#125; function addDog(dog:string) &#123; dogList.value.push(dog); &#125; function getDog() &#123; axios.get(&quot;https://dog.ceo/api/breed/pembroke/images/random&quot;).then(value=&gt;&#123; addDog(value.data.message); &#125;) &#125;&lt;/script&gt;&lt;template&gt; &lt;h2&gt;计数器&lt;/h2&gt; &lt;p&gt;当前：&#123;&#123; count &#125;&#125;&lt;/p&gt; &lt;button @click=&quot;countNum&quot;&gt;计数器加一&lt;/button&gt; &lt;hr&gt; &lt;h2&gt;狗狗图片&lt;/h2&gt; &lt;img v-for=&quot;dog, index in dogList&quot; :key=&quot;index&quot; :src=&quot;dog&quot;/&gt; &lt;br&gt; &lt;button @click=&quot;getDog&quot;&gt;获取一只狗狗&lt;/button&gt;&lt;/template&gt;&lt;style scoped&gt; img &#123; height: 100px; margin-right: 5px; &#125;&lt;/style&gt; 上面展示的这个组件可以实现两个功能：点击按钮计数、获取一只狗狗的图片。效果如下： 但是可以看到，这两个功能的代码此时是完全混在一起的，很不方便阅读，为此，我们将其封装到两个 ts 文件中。 在 src 目录下新建 hooks 目录，并在其下创建 useCount.ts 和 useDog.ts (为什么要这样命名？约定俗成的，hooks 的一般命名格式就是 use*.ts)。在 useCount.ts 中，我们需要把有关计数器的代码摘出来，封装到一个函数中，并将这个函数暴露出去，不要忘记必须在函数的最后返回所有相关的数据和函数。 useCount.ts 的内容如下： 123456789import &#123;ref&#125; from &quot;vue&quot;;export default function () &#123; let count = ref(0); function countNum() &#123; count.value++; &#125; return &#123;count, countNum&#125;;&#125; useDog.ts 的内容如下： 123456789101112131415import &#123;ref&#125; from &quot;vue&quot;;import axios from &quot;axios&quot;;export default function () &#123; let dogList = ref([&quot;https://images.dog.ceo/breeds/pembroke/n02113023_6570.jpg&quot;]); function addDog(dog:string) &#123; dogList.value.push(dog); &#125; function getDog() &#123; axios.get(&quot;https://dog.ceo/api/breed/pembroke/images/random&quot;).then(value=&gt;&#123; addDog(value.data.message); &#125;) &#125; return &#123;dogList, getDog&#125;;&#125; 在组件中的使用方式如下： 123456789101112131415161718192021222324&lt;script setup lang=&quot;ts&quot;&gt; import useCount from &quot;@/hooks/useCount&quot;; // 引入hook，@表示src目录 import useDog from &quot;@/hooks/useDog&quot;; // 引入hook，@表示src目录 let &#123;count, countNum&#125; = useCount(); // 解构赋值，拿到hook中的数据和函数 let &#123;dogList, getDog&#125; = useDog(); // 解构赋值，拿到hook中的数据和函数&lt;/script&gt;&lt;template&gt; &lt;h2&gt;计数器&lt;/h2&gt; &lt;p&gt;当前：&#123;&#123; count &#125;&#125;&lt;/p&gt; &lt;button @click=&quot;countNum&quot;&gt;计数器加一&lt;/button&gt; &lt;hr&gt; &lt;h2&gt;狗狗图片&lt;/h2&gt; &lt;img v-for=&quot;dog, index in dogList&quot; :key=&quot;index&quot; :src=&quot;dog&quot;/&gt; &lt;br&gt; &lt;button @click=&quot;getDog&quot;&gt;获取一只狗狗&lt;/button&gt;&lt;/template&gt;&lt;style scoped&gt; img &#123; height: 100px; margin-right: 5px; &#125;&lt;/style&gt; 很明显，用这样的方式封装好各功能模块的代码后，代码就整洁多了，也便于修改和阅读。 10 路由 [TOC] 这一章的内容很重要！ 路由，其实就是一种映射规则，它将一个 url 映射到一个对应的页面。当浏览器的 url，也就是导航栏的路径发生变化时，页面就会刷新。Vue 可以做到局部刷新，即当 url 改变时，不需要刷新整个页面，而只需要刷新部分需要改变的页面。 本章，我们将通过路由来实现下面这个效果 (请注意导航栏的变化)： 10.1 路由的基本使用 要使用路由，首先要下载一个包： 1npm i vue-router 接下来将分步骤讲解路由的配置方法。 首先要准备好组件，页面的布局已经写好，显示主体部分要根据路由来确定，待显示的三个组件 StudentManage.vue、ClassManage.vue、SystemSetting.vue 已经准备好，等待填充。 在 src 下新建一个文件夹 router，在 router 中创建 index.ts，该文件是路由器的配置文件，命名可以随意，但是写 index 会比较方便 (后面会解释原因)。index.ts 中应该有如下内容： 12345678910111213141516171819202122232425262728// 第一步：引入createRouterimport &#123;createRouter, createWebHistory&#125; from &quot;vue-router&quot;;// 路由对应的组件要先引入import ClassManage from &quot;@/components/ClassManage.vue&quot;;import StudentManage from &quot;@/components/StudentManage.vue&quot;;import SystemSetting from &quot;@/components/SystemSetting.vue&quot;;// 第二步：创建路由器const router = createRouter(&#123; history: createWebHistory(), // 这句是什么意思之后会讲 routes: [ // 配置路由规则，采用对象的方式，path代表一个路由，component是路由对应的组件，组件应该在上面引入 &#123; path: &quot;/student&quot;, component: StudentManage &#125;, &#123; path: &quot;/class&quot;, component: ClassManage &#125;, &#123; path: &quot;/system&quot;, component: SystemSetting &#125; ]&#125;);export default router; // 将定义好的路由器暴露出去 注册路由器，打开 src 下的 main.ts，在其中引入我们在 index.ts 中定义的路由器 router 并进行注册，具体如下： 1234567import &#123; createApp &#125; from &#x27;vue&#x27;;import App from &#x27;./App.vue&#x27;;import router from &quot;./router&quot;; // 引入routerconst app = createApp(App);app.use(router); // 使用routerapp.mount(&#x27;#app&#x27;); 至此，我们已经定义好了路由规则，但是此时路由还无法正常访问，因为我们没有告诉 vue 组件应该放在哪里。放置路由对应的组件要使用 &lt;RouterView&gt; 标签，当路由切换时，系统会去找该路由对应的组件，并将该组件放到 &lt;RouterView&gt; 出现的位置。要注意，RouterView 标签是由 vue-router 提供的，所以使用前要先引入。 到目前为止，如果手动改变导航栏的 url，我们就已经能正常看到效果了，但是还不够，因为不可能让用户去修改导航栏来访问页面的，我们应该提供按钮，用户点击按钮之后页面自动跳转。你可能立马想到了 &lt;a&gt; 标签，&lt;a&gt; 标签当然没有问题，但是 &lt;a&gt; 标签会造成全局刷新，vue-router 提供了另一个标签 &lt;RouterLink&gt;，这个标签的本质也是 &lt;a&gt; 标签，但是它只会刷新局部，不会导致整个页面的刷新，该标签要跳转的路由通过 to 属性设置，而不是 href，这点请注意。同样的，要使用 &lt;RouterLink&gt; 也要先引入。 以上就是 vue-router 的基本使用，这里还要提到 &lt;RouterLink&gt; 上的一个属性 active-class，我们经常会遇到当某个链接被按下时，对应的按钮要改变样式以突出，我们一般的做法就是在被点击的按钮上加一个 class，而 active-class 这个属性非常方便，它用于设置当某个路由被激活时，对应的 &lt;RouterLink&gt; 上的 class。注意，这里说的是当某个路由被激活时，而不是当元素被点击时，这意味着绑定了当前路由的所有 &lt;RouterLink&gt; 都会处于 active 状态，于是都被添加上同样的 class。 10.2 开发标准 这一节主要讲上面没提到的两个注意事项： 在一般工程的开发标准中，我们需要把一般组件和路由组件区分开来，一般组件放在 components 目录下，而路由组件则放在 pages 或 views 目录下。 路由切换时，上一个路由组件其实是被卸载掉了，需要的时候会再次挂载。 10.3 路由器的工作模式 路由器的工作模式有 history 和 hash 两种模式，这两种模式的显式区别在于 url，当使用 history 模式时，导航栏的 url 中会出现一个井号 # ，而 hash 模式不会，至于具体的区别这里就不讲了，涉及到服务端的知识。 history 模式 优点：URL 更加美观，不带有 #，更接近传统的网站 URL。 缺点：后期项目上线，需要服务端配合处理路径问题，否则刷新会有 404 错误。 1234const router = createRouter(&#123; history:createWebHistory(), // history模式，需要事先引入该函数 /******/ &#125;) hash 模式 优点：兼容性更好，因为不需要服务器端处理路径。 缺点：URL 带有 # 不太美观，且在 SEO 优化方面相对较差。 1234const router = createRouter(&#123; history:createWebHashHistory(), // hash模式，需要事先引入该函数 /******/ &#125;) 10.4 to 的第二种写法 在 10.1 中，我们介绍了 &lt;RouterLink&gt; 标签中 to 属性的一种写法，直接写路由： 1&lt;RouterLink to=&quot;/student&quot; active-class=&quot;routeActive&quot;&gt;链接&lt;/RouterLink&gt; 这里还有第二种写法，可以写一个对象，既然要写对象，那么就必须先在 to 前面加一个冒号，否则我们写的只会是一个字符串。该对象中应该有一个属性 path，这个属性的值就是路由 (字符串)，所以上面的代码可以改写成下面的样子： 1&lt;RouterLink :to=&quot;&#123;path: &#x27;/path&#x27;&#125;&quot; active-class=&quot;routeActive&quot;&gt;链接&lt;/RouterLink&gt; 现在还看不出来这种写法的优势，但后面我们就能体会到了。 10.5 路由命名 通过 name 属性，我们可以给路由起一个名字，然后通过路由的名字来直接访问路由。打开 router/index.ts，通过下面这种方式为 /student 路由起一个名字 xs： 1234567891011const router = createRouter(&#123; history: createWebHistory(), routes: [ &#123; name: &quot;xs&quot;, path: &quot;/student&quot;, component: StudentManage &#125;, ··· ]&#125;); 随后，在 &lt;RouterLink&gt; 中，我们就可以这样配置要跳转的路由： 1&lt;RouterLink :to=&quot;&#123;name: &#x27;xs&#x27;&#125;&quot; active-class=&quot;routeActive&quot;&gt;链接&lt;/RouterLink&gt; 那么为什么要使用 name？在学习了路由传参和嵌套路由之后，你就会发现路由可能会非常长，写起来很麻烦，这种时候就能凸显出 name 的作用了。 10.6 嵌套路由 接下来，我们要实现这样的效果： 要实现上面的效果，我们需要学习嵌套路由和路由传参的知识。首先来认识一下嵌套路由，嵌套路由就是子级路由，从上图中我们可以看到，原本的路由处于 /student 下，点击链接后，路由变成了 /student/detail (问号后面的参数我们暂时不理会)，这就是嵌套路由，一个路由在另一个路由之下。通过分析页面关系我们也能看出来，一开始我们已经处于学生管理界面了，但是通过点击链接，在该页面下又出现了新页面。 由上可见，/detail 是 /student 的子级路由，子级路由的配置方式是： 123456789101112131415161718const router = createRouter(&#123; history: createWebHistory(), routes: [ &#123; name: &quot;xs&quot;, path: &quot;/student&quot;, component: StudentManage, children: [ // 子级路由 &#123; name: &#x27;xijie&#x27;, path: &#x27;detail&#x27;, component: Detail &#125;, ] &#125;, ··· ]&#125;); 注意事项：子级路由的 path 不需要加斜杠！ 对应的 &lt;RouterLink&gt; 应该这样写： 1&lt;RouterLink to=&quot;/student/detail&quot;&gt;&lt;/RouterLink&gt; 这种时候你就能想到了，如果层级很多，那么路由可能就会非常长，这种情况下给路由起一个名字来直接跳转是最方便的。 10.7 路由传参 要想实现 10.6 的效果，光靠嵌套路由是不够的。当我们要查看具体某个学生的信息时，父级路由应该告诉子级路由，我们要查看的学生是哪一位，这就要涉及到路由的传参。路由的参数有两种 —— query 和 params。 10.7.1 query 参数 最简单的传参方式： 1&lt;RouterLink to=&quot;/student/detail?参数1=值1&amp;参数2=值2&amp;参数3=值3&quot;&gt;&lt;/RouterLink&gt; 参数是传给子级路由的，所以到子级路由的视图 (view) 中去接收。接收路由参数需要引入一个 hook：useRoute。这个 hook 返回一个对象： 这个对象上有诸多属性，其中有一个属性叫 query，其值也是一个对象，这个 query 是我们需要关注的，它的身上携带着路由的参数。例如上图中，我们就传来了一个 cname 参数，其值为“李小狼”。 上面展示的是最普通的传参方式，别忘记还有一种对象写法： 12345678&lt;RouterLink :to=&quot;&#123; path: &#x27;/student/detail&#x27;, query: &#123; 参数1: 值1, 参数2: 值2, 参数3: 值3 &#125;&#125;&quot;&gt;&lt;/RouterLink&gt; 上面这种写法的好处在于，字符串被当作表达式来进行解析，有语法高亮，而且拼串的问题会更好解决。 这里要注意一件事情：当只有路由参数变化时，路由是发生了改变，但是这并不会引起原本的组件被卸载，只会导致原本的组件被更新，setup 中的代码是不会被重新运行的，可以使用 onBeforeUpdate 在组件更新时做一些操作。 另外，如果打算解构赋值获取 useRoute 返回的对象上的 query 属性，然后在模板中直接使用，那么需要给 useRoute() 套一层 toRefs，否则会失去响应式。 10.7.2 params 参数 params 参数写起来比 query 参数要简洁，query 参数需要以键值对的方式进行书写，而 params 直接写值就行： 1&lt;RouterLink to=&quot;/student/detail/参数值1/参数值2/参数值3&quot;&gt;&lt;/RouterLink&gt; 虽然话是这么说，但参数不能没有名字，不然我们怎么去获取参数呢。params 中的参数并非没有名字，而是事先已经约定好了，每个位置上的参数值有对应的固定的名字，因此传参的时候可以省略而已。要配置 params 参数的名字，需要到 index.ts 中做以下修改： 123456789101112131415161718const router = createRouter(&#123; history: createWebHistory(), routes: [ &#123; name: &quot;xs&quot;, path: &quot;/student&quot;, component: StudentManage, children: [ &#123; name: &#x27;xijie&#x27;, path: &#x27;detail/:参数名1/:参数名2/:参数名3&#x27;, // 看这里！ component: Detail &#125;, ] &#125;, ··· ]&#125;); 如上我们就可以正常传参了，获取参数仍然使用 useRoute，但是不同的是，这一次我们不再是从 query 对象上获取参数，而是从 params 对象属性上获取参数，除此以外没有什么区别，故不再赘诉。 和 query 参数一样，params 参数也有对象写法，两者稍有区别： params 参数传参时不能使用 path，而必须要使用 name，所以必须给路由起名。 要把属性名从 query 改成 params。 12345678&lt;RouterLink :to=&quot;&#123; name: &#x27;xijie&#x27;, params: &#123; 参数1: 值1, 参数2: 值2, 参数3: 值3 &#125;&#125;&quot;&gt;&lt;/RouterLink&gt; 几个注意事项： 按照上面的方法设置的 params 参数是必填参数，意思是在路由中配置了某一参数之后就必须传递该参数，否则会报错，如果要将某参数设置为可选参数，可在参数名后面加一问号：'detail/:参数名1/:参数名2/:参数名3?'，如此以来，就算不传参数 3 也不会报错。 参数值不接受数组等对象类型数据。 10.8 路由的 props 配置 这里要特别强调一下是路由的 props 配置，而不是组件的 props 配置。按照上面讲的，当我们要获取路由参数时，我们都需要从一个 hook，也就是 useRoute 上获取一个对象，然后再去找这个对象上的 query 或 params 属性，这两个属性又是一个对象，它们上携带着真正的路由参数，如此一来，当我们需要在模板中使用路由参数时需要使用 route.query.xxx 或者 route.params.xxx，非常麻烦。 路由的 props 配置项提供了一个解决方案，当路由的 props 开启时，所有的路由参数会像组件间通信一样，传给子组件，随后在子组件中通过 defineProps 函数就能接收全部参数，无需使用 useRoute 就能直接使用所有参数。 第一种开启 props 的方法： 12345678910111213141516171819const router = createRouter(&#123; history: createWebHistory(), routes: [ &#123; name: &quot;xs&quot;, path: &quot;/student&quot;, component: StudentManage, children: [ &#123; name: &#x27;xijie&#x27;, path: &#x27;detail/:name/:age/:sex&#x27;, component: Detail, props: true // 看这里 &#125;, ] &#125;, ··· ]&#125;); 然后在子组件中用 defineProps 接收： 1234&lt;script setup lang=&#x27;ts&#x27;&gt; import &#123;defineProps&#125; from &#x27;vue&#x27;; definePros([&#x27;name&#x27;, &#x27;age&#x27;, &#x27;sex&#x27;]);&lt;/script&gt; 随后，便可在模板中直接使用接收到的三个参数了。 ==但是注意：上面这种设置 props 的方法只适用于 params 参数，对 query 参数是无效的。==不过，props 还有第二种写法以解决这一问题。第二种写法要求将 props 写成一个函数，该函数接收一个参数，该参数就是我们之前通过 useRoute 获取的 route 对象。函数的返回值也为一个对象，将所有我们需要的参数封装到这个对象上，我们就能正常的在子组件中使用 defineProps 来接收了。函数的返回值是一个对象，route 也是一个对象，所以我们可以直接返回 route 上的 params 或 query 即可： 123456789101112131415161718192021const router = createRouter(&#123; history: createWebHistory(), routes: [ &#123; name: &quot;xs&quot;, path: &quot;/student&quot;, component: StudentManage, children: [ &#123; name: &#x27;xijie&#x27;, path: &#x27;detail/:name/:age/:sex&#x27;, component: Detail, props (route) &#123; return route.query; // 如果是params就return route.params；但如果真是params，其实用props:true一行解决就行了 &#125; &#125;, ] &#125;, ··· ]&#125;); 其实这里还有第三种写法，那就是直接把 props 的属性值写成一个对象，不过这种写法只能写死，没有意义，常用的还是上面两种，故不赘述。 疑问：这样使用的话，路由参数和组件间参数不就混在一起了吗？但其实并没有混在一起，路由组件不是一般组件，出现在模板中的组件不是组件本身而是 &lt;RouterView&gt;，父级组件根本没有机会通过 props 配置项直接给路由组件传参，只能通过路由传参，因此不存在路由参数和组件参数混淆的情况。 10.9 replace 属性 路由跳转有两种模式，一种是 push，一种是 replace。所谓 push，就是像堆栈一样，每进行一次跳转，就把新路由入栈，之前访问过的路由将会作为历史记录保存，浏览器仍然可以通过后退来进行访问。而 replace 就不一样，路由每次跳转时，如果采用 replace 模式，新路由将会直接替换掉栈顶的路由，导致上一次访问的路由直接消失，因此浏览器无法通过后退来回到上一次的路由。 Vue 默认采用的路由跳转模式是 push，这样浏览器将可以后退。如果想设置为 replace，只需要在 &lt;RouterLink&gt; 上加一个属性 replace 就行了： 1&lt;RouterLink to=&quot;/xxx&quot; replace&gt;&lt;/RouterLink&gt; 这样以来，当该 &lt;RouterLink&gt; 被点击时，浏览器将会用该标签的 to 属性中的路由直接替换原本的路由，浏览器无法后退至上一次访问过的路由。 10.10 编程式路由导航 讲了这么多，前面我们都是通过 &lt;RouterLink&gt; 来进行路由跳转，这个标签的本质其实是一个 &lt;a&gt; 标签，vue 编译完后还是使用 &lt;a&gt; 标签进行的跳转，这就有一个问题：难道我们只能通过 &lt;a&gt; 标签进行跳转吗？如果我有需求，要使用 &lt;button&gt; 或其他标签进行路由跳转呢？这就要引出编程式路由导航了。 所谓编程式，其实就是指在 &lt;script&gt; 标签中通过 js 或 ts 代码来进行路由跳转。要实现这一点，就要从 vue-router 中引入一个工具：useRouter (注意与前面学的 useRoute 区分开)。useRouter 也是一个 hook，它会返回一个对象 —— router，这就是管理整个路由导航的路由器，使用它可以轻松实现路由跳转。 router 上有两个重要的函数：replace 和 push，这两个函数的作用不用多说，上一节已经讲过了。重要的是如何使用，其实它们的使用方法和 &lt;RouterLink&gt; 中 to 属性的使用方法是一模一样的，to 属性中可以写什么，它们的参数就可以写什么，所以，它们的参数既可以写字符串，也可以写对象，当要用编程式路由导航替换 &lt;RouterLink&gt; 的时候，直接把它的 to 属性的内容原封不动地拿过来就行了。 10.11 重定向 这一节我们来处理一个被我们一直忽视的问题，打开我们的网页，什么都不要做，打开开发者工具，进入控制台，能看到这样一个报错： 这个报错是由于，一上来，我们没有选中任何一个路由，我们的路由位于 /，但是在 index.ts 中我们又没有配置 / 路由，所以 vue 返回不了对应的页面。 我们希望的是，当网页被打开时，帮我们跳转到一个默认的路由上去，这一过程就叫做重定向。要实现重定向很简单，我们只需要在 index.ts 中加上这么一个路由： 1234&#123; path: &quot;/&quot;, redirect: &quot;/student&quot;&#125; 之后，当我们打开页面时，如果页面的路由为 /，页面就会自动跳转到 /student 路由下。 11 Pinia 11.1 安装 Pinia Pinia 是 vue3 中的集中式状态管理工具 (在 vue2 中我们使用的是 vuex，状态可以理解为数据)，它的作用是管理组件间的数据共享。有些数据应该是公共的，例如对于一个商城来讲，用户的头像等信息，可能在不同的组件中都要展示，那么显然，每个组件都拥有一个用户头像是不太合适的，因此头像应该作为共享数据，供有需要的组件使用。但同时，还有一部分信息，例如与组件自身功能有关的信息，就应该由组件自己持有。 要想使用 pinia，要先安装 pinia： 1npm i pinia 安装完后引入 pinia，在 main.ts 中写入： 12345678import &#123; createApp &#125; from &#x27;vue&#x27;;import App from &#x27;./App.vue&#x27;;import &#123; createPinia &#125; from &#x27;pinia&#x27;;const pinia = createPinia();const app = createApp(App);app.use(pinia);app.mount(&#x27;#app&#x27;); 11.2 存储和读取数据 Pinia 的使用方法类似于之前学过的 hooks，都是由一个 ts 文件向外暴露一个函数，该函数返回一个对象，对象中封装的，就是实现某一功能所要用到的全部数据，这也再一次体现了组合式 API 的思想。 根据规范，我们要在 src 目录下新建一个目录 stores，所有状态管理相关的 ts 文件都放在该目录下。ts 文件的文件名应该尽量体现其功能，以下是这些 ts 文件的大致写法： 123456789101112import &#123; defineStore &#125; from &quot;pinia&quot;; // 引入defineStore// 命名规范：useXxxStoreexport const useNumberStore = defineStore( &quot;number&quot;, // store的id值，最好和文件名保持一致 &#123; state() &#123; // state函数 return &#123; // 返回一个封装了相关状态(数据)的对象 num: 1 &#125; &#125;,&#125;); 上示代码中， defineStore 用来创建一个 store，如果你熟悉 vue2 的话就应该知道，store 是仓库的意思，可理解为数据的仓库。defineStore 接收两个参数：store 的 id 值和一个对象。id 值最好和文件名保持一致，而第二个参数则要给出一个包含了 state 函数的对象，该函数返回一个封装了所需状态 (数据) 的对象。上述代码中，我们创建了一个名为 useNumberStore 的函数并暴露了出去，它可以返回一个 store，该 store 的 id 是 number，其中携带了一个数据 num，其初始值为 1。 以上是存储数据的方法，接下来演示如何读取数据。在对应的组件中，我们通过引入上面暴露的函数来获取该 store： 1234import &#123; useNumberStore &#125; from &quot;@/stores/number&quot;;const numberStore = useNumberStore();console.log(numberStore) 打印一下获取到的 store 对象，我们可以得到如下结果： 可以看到，store 上有一个名为 num 的 Ref 对象，那么这是否意味着我们可以使用 numberStore.num.value 的方式来读取数据？并不是，我们只需要写 numberStore.value 就行了，你可能会问为什么，这是一个比较 tricky 的点，我们稍微解释一下，看下面这段代码： 1234567891011let a = ref(1);console.log(a.value); // 正常let b = reactive(&#123; x: 1, y: ref(2)&#125;);console.log(b.x); // 正常console.log(b.y); // 正常console.log(b.y.value); // 报错 上示代码中，a 是 ref 对象，所以需要 .value 来访问其值；而 b 是一个 reactive 对象，其中，y 是 b 上一个 ref 对象属性，虽然也是 ref 对象，但是不需要 b.y.value 就可以直接访问其值，加了 .value 反而是错的，这是因为当一个 reactive 对象中有 ref 对象属性时，它会自动帮你解包，不需要手动加 .value，这是一个要注意的小点。 11.3 三种修改数据的方式 11.3.1 直接修改 上一节介绍了存储和读取数据的方法，这一节介绍一下修改数据的方法。最简单的一种方法就是直接修改，例如： 123456789101112131415161718192021&lt;script setup lang=&quot;ts&quot;&gt;import &#123; useNumberStore &#125; from &quot;@/stores/selectNum&quot;;const numberStore = useNumberStore();function IncNum() &#123; numberStore.num++; // 直接修改num&#125;function DecNum() &#123; numberStore.num--; // 直接修改num&#125;&lt;/script&gt;&lt;template&gt; &lt;div class=&quot;container&quot;&gt; &lt;span&gt;学号：&lt;/span&gt; &lt;h1&gt;&#123;&#123; numberStore.num &#125;&#125;&lt;/h1&gt; &lt;button id=&quot;incBtn&quot; @click=&quot;IncNum&quot;&gt;+&lt;/button&gt; &lt;button id=&quot;decBtn&quot; @click=&quot;DecNum&quot;&gt;-&lt;/button&gt; &lt;/div&gt;&lt;/template&gt;&lt;style scoped&gt;&lt;/style&gt; 这种直接修改数据的方式只有 vue3 支持，在 vue2 中，vuex 不支持直接修改。 11.3.2 $patch 第二种修改数据的方法是调用 store 上的 $patch 函数，该接收一个对象，可以一次性修改多个数据的值。假设我们有一个 store，其中有 sex、age、name 等属性，我们使用 $patch 直接修改其值： 1234567import &#123; useStudentInfoStore &#125; from &quot;@/stores/studentInfo&quot;;const studentInfoStore = useStudentInfoStore();studentInfoStore.$patch(&#123; sex: &#x27;男&#x27;, age: 20, name: &#x27;unknown&#x27;&#125;) $patch 相当于是直接赋值，一次性替换掉原数据，当你需要一次性修改很多数据时，推荐使用这种方法。 11.3.3 actions actions 是一个配置项，可以定义“动作”，所谓“动作”其实就是一系列事先定义好的函数，调用这些函数来实现一些数据的修改行为，由于是事先写好的，所以具有更高的安全性和封装性。例： 1234567891011121314151617import &#123; defineStore &#125; from &quot;pinia&quot;;export const useNumberStore = defineStore(&quot;number&quot;, &#123; actions: &#123; increment() &#123; this.num += 2; &#125;, decrement() &#123; this.num -= 2; &#125; &#125;, state() &#123; return &#123; num: 1 &#125; &#125;,&#125;); 在上面，我们定义了名为 increment 和 decrement 的 action，当调用这两个函数时，数据 num 将会加 2 或减 2。下面是调用该动作的方法： 123456789101112131415161718192021222324&lt;script setup lang=&quot;ts&quot;&gt;import &#123; ref, reactive &#125; from &quot;vue&quot;;import &#123; useNumberStore &#125; from &quot;@/stores/selectNum&quot;;import &#123; useStudentInfoStore &#125; from &quot;@/stores/studentInfo&quot;;const numberStore = useNumberStore();const studentStore = useStudentInfoStore();function IncNum() &#123; numberStore.increment();&#125;function DecNum() &#123; numberStore.decrement();&#125;&lt;/script&gt;&lt;template&gt; &lt;div class=&quot;container&quot;&gt; &lt;span&gt;学号：&lt;/span&gt; &lt;h1&gt;&#123;&#123; numberStore.num &#125;&#125;&lt;/h1&gt; &lt;button id=&quot;incBtn&quot; @click=&quot;IncNum&quot;&gt;+&lt;/button&gt; &lt;button id=&quot;decBtn&quot; @click=&quot;DecNum&quot;&gt;-&lt;/button&gt; &lt;/div&gt;&lt;/template&gt;&lt;style scoped&gt;&lt;/style&gt; 11.4 storeToRefs 当我们需要访问 store 上的数据时，我们需要采用 store.xxx 的形式去访问，如果觉得这么写太麻烦，我们自然而然就会想到解构赋值，去直接获取数据： 1let &#123;xxx, yyy, zzz&#125; = store; 但是就像之前老生常谈的，这样做会使数据失去响应式，所以我们自然而然又想到使用 toRefs： 1let &#123;xxx, yyy, zzz&#125; = toRefs(store); 那么这样做是否可行呢？答案是可行的，但是这样做不好，如果你在控制台输出一下 toRefs(store) 的话，就会发现控制台打印如下内容： 你可以发现，toRefs 函数将 store 对象上的每一个属性都变成了 ref 对象，就连函数都包括在内，这显然是不妥的。vue 的开发者自然也想到了这个问题，所以官方提供了另一个函数 storeToRefs 来解决，这个函数只会将 store 上的数据变成 ref 对象，所以数据被解构赋值之后不会丢失响应式，打印 storeToRefs(store) 后控制台的内容为： 这样一来，只有数据会被转换成 ref 对象。一个小注意点：storeToRefs 函数由 pinia 组件提供，不要从 vue 中引入。 11.5 getters getters 用于创建经过加工后的数据，相当于是 pinia 中的计算属性。例如在 state 中已有数据 num，而我们还需要一个数据 absNum，其值为 num 的绝对值，那么我们可以在 getters 中追加一个属性： 1234567891011121314import &#123; defineStore &#125; from &quot;pinia&quot;;export const useNumberStore = defineStore(&quot;number&quot;, &#123; state() &#123; return &#123; num: 1 &#125; &#125;, getters: &#123; absNum(state) &#123; return Math.abs(state.num); &#125; &#125;&#125;); getters 中的属性都是函数，函数接收一个参数 state，通过它我们可以获取我们在 pinia 中配置的数据。当然，除了直接使用 state，我们还可以使用 this，效果是一样的： 12345getters: &#123; absNum() &#123; return Math.abs(this.num); &#125;&#125; 但如果不打算使用 this，那么就可以直接写成箭头函数： 12345getters: &#123; absNum: state=&gt;&#123; return Math.abs(state.num); &#125;&#125; 11.6 $subscribe $subscribe 函数是 store 对象上的一个函数，它可以监视 store 的变化。当 store 中的数据发生变化，$subscribe 中传入的回调函数就会被调用，例如： 12345import &#123; useNumberStore &#125; from &quot;@/stores/selectNum&quot;;const numberStore = useNumberStore();numberStore.$subscribe(()=&gt;&#123; console.log(&#x27;数据被修改了&#x27;);&#125;) $subscribe 中可以接收两个参数，一个是 mutate，这是一个对象，记录了数据的变化，一般我们不关注；还有一个参数是 state，这个就是 store 中当前保存的数据。 11.7 store 组合式写法 上面介绍的是选项式写法，即将动作都放在 actions 里，把数据放在 state 里，把计算属性放在 getters 里，但其实，defineStore 可以直接接收一个函数，然后我们把要写的 actions、state、getters 都作为这个函数的返回值写在函数里。 接下来展示一个例子，假设原本我们有选项式写法： 12345678910111213141516171819202122232425import &#123; defineStore &#125; from &quot;pinia&quot;;export const useNumberStore = defineStore(&quot;number&quot;, &#123; actions: &#123; increment() &#123; this.num += 2; &#125;, decrement() &#123; this.num -= 2; &#125; &#125;, state() &#123; return &#123; num: 1, name: &quot;wxm&quot;, age: 19, sex: &quot;male&quot; &#125; &#125;, getters: &#123; absNum: (state) =&gt; &#123; return Math.abs(state.num); &#125; &#125;&#125;); 将上示代码改写成组合式写法就是： 1234567891011121314151617181920import &#123; defineStore &#125; from &quot;pinia&quot;;import &#123; ref, computed &#125; from &quot;vue&quot;;export const useNumberStore = defineStore(&quot;number&quot;, () =&gt; &#123; // state let num = ref(1); let name = ref(&quot;wxm&quot;); let age = ref(19); let sex = ref(&quot;male&quot;); // actions function increment() &#123; num.value += 2; &#125; function decrement() &#123; num.value -= 2; &#125; // getters let absNum = computed(() =&gt; Math.abs(num.value)); return &#123; num, name, age, sex, increment, decrement, absNum &#125;;&#125;) 12 组件通信 12.1 props props 的用法其实在 07 props 已经说过了，它可以实现父组件到子组件的通信，但是它不适合用于子组件给父组件通信。如果想要使用 props 来给父组件发信息，可以这样做：父组件先给子组件一个函数，子组件调用该函数，由于函数是属于父组件的，所以该函数将能影响到父组件的数据，以此可以实现子组件到父组件的通信。例如在子组件中，我们定义了一个对象 studInfo，该对象封装了一个学生的信息，父组件则负责展示学生的信息，因此子组件应该向父组件发送该对象。使用 props 完成这一任务的方法： 在父组件中定义函数 getStudInfo，并将该函数通过 props 传给子组件： 1234567891011121314151617181920212223242526272829303132333435&lt;script lang=&quot;ts&quot; setup&gt;import Son from &quot;@/components/communication/Son.vue&quot;;import &#123; reactive &#125; from &quot;vue&quot;;let stud = reactive(&#123; // 事先定义好对象，准备接收数据 name: &quot;&quot;, imageLink: &quot;&quot;, intro: &quot;&quot;&#125;);function getStudInfo(studInfo:object) &#123; // 通信用的函数 Object.assign(stud, studInfo);&#125;&lt;/script&gt;&lt;template&gt; &lt;div class=&quot;wrapper&quot;&gt; &lt;span v-show=&quot;stud.name&quot;&gt;姓名：&#123;&#123; stud.name &#125;&#125;&lt;/span&gt; &lt;img v-show=&quot;stud.imageLink&quot; :src=&quot;stud.imageLink&quot; /&gt; &lt;span v-show=&quot;stud.intro&quot;&gt;介绍：&#123;&#123; stud.intro &#125;&#125;&lt;/span&gt; &lt;Son :getStudInfo=&quot;getStudInfo&quot;&gt;&lt;/Son&gt; &lt;/div&gt;&lt;/template&gt;&lt;style scoped&gt; .wrapper &#123; background-color: #efc; box-shadow: 2px 1px 10px #878787; border-radius: .5rem; padding: 25px; box-sizing: border-box; &#125; img &#123; height: 100px; display: block; &#125;&lt;/style&gt; 在子组件中获取 getStudInfo 函数，定义 studInfo 对象，并调用 getStudInfo 函数，将子组件的 studInfo 传递给父组件。 123456789101112131415161718192021222324252627&lt;script lang=&quot;ts&quot; setup&gt;import &#123; reactive, defineProps &#125; from &quot;vue&quot;;defineProps([&quot;getStudInfo&quot;]);let studInfo = &#123; name: &quot;李小狼&quot;, imageLink: &quot;https://c-ssl.duitang.com/uploads/blog/202209/14/20220914164706_1c29a.jpeg&quot;, intro: &quot;李小狼，是CLAMP所著日本漫画《魔卡少女樱》及其衍生作品中的男主角，出生于中国香港。李小狼和木之本樱同年级的男生，友枝中学1年级2班/3班的学生。与魔法师库洛·里德是远亲（库洛里多的母亲出身中国李家），自身亦拥有很强的魔力。为了寻找被解除封印的库洛牌，来到日本，遇到了木之本樱。两人一起行动，逐渐相互产生了爱慕之情。擅长使用水、火、风、雷魔法，支持小樱收集库洛牌。虽然不爱说话给人一种酷酷的感觉，但其实是直爽又害羞的孩子。另外，还擅长弹钢琴、制作中式糕点。收集完卡片之后返回了香港。&quot;,&#125;;&lt;/script&gt;&lt;template&gt; &lt;div class=&quot;bkg&quot;&gt; &lt;!-- 直接@click调用函数 --&gt; &lt;button @click=&quot;getStudInfo(studInfo)&quot;&gt;发送学生信息&lt;/button&gt; &lt;/div&gt;&lt;/template&gt;&lt;style scoped&gt;.bkg &#123; background-color: cadetblue; box-shadow: 2px 1px 10px #878787; border-radius: 0.5rem; padding: 10px; margin-top: 20px;&#125;&lt;/style&gt; 效果： 但是显然，这种通信方式是很麻烦的，尤其是当层级关系变复杂时，子组件到父组件的通信非常困难，因此，props 一般只用于父组件到子组件的通信。 12.2 自定义事件 自定义事件允许我们定义自己的事件，它也可以实现子组件到父组件的传参。 12.2.1 $event 在聊自定义事件之前，我们先来回顾 @click。学过 vue2 的同学应该对 @click 很熟悉，它代表点击事件，当某个元素被点击时，就会触发对应的回调函数。这里来回顾一个比较容易被遗忘的点，那就是回调函数中的 $event 对象。这是一个隐藏参数，是一个事件对象，携带了事件信息，如果你想获取这个对象，那就必须在回调函数中写上这个参数： 12345678910111213141516171819202122232425&lt;script lang=&quot;ts&quot; setup&gt;function showEventObj($event) &#123; console.log($event);&#125;&lt;/script&gt;&lt;template&gt; &lt;div class=&quot;wrapper&quot;&gt; &lt;button @click=&quot;showEventObj&quot;&gt;打印事件对象&lt;/button&gt; &lt;/div&gt;&lt;/template&gt;&lt;style scoped&gt;.wrapper &#123; background-color: #efc; box-shadow: 2px 1px 10px #878787; border-radius: 0.5rem; padding: 25px; box-sizing: border-box;&#125;img &#123; height: 100px; display: block;&#125;&lt;/style&gt; 得到结果： 有关于这个对象的内容就不具体聊了，这是 DOM 的基础知识。但是有这么几件事应该清楚：当回调函数中有多个参数时，如果需要用到 $event，定义时就必须把 $event 参数写在最右，因为 $event 永远是作为最后一个参数传入的，调用时可写可不写。而当回调函数是无参时，如果需要用到 $event，调用时要么不写参数，要么就写 $event。 12.2.2 emit 接下来介绍自定义事件，所谓自定义事件，就是指我们自己定义一个事件，当我们说这个事件被触发了时，它才会触发。假设我们定义了一个名为 send-student 的事件，当该事件被触发时，会调用一个名为 getStudInfo 的函数，那么我们可以在一个元素上绑定该事件和回调函数： 1&lt;div @send-student=&quot;getStudInfo(stud)&quot;&gt;&lt;/div&gt; 那么如何定义和触发这个 send-student 呢？自定义事件可以定义在任何组件里，当某个组件的该事件被触发时，都会导致回调函数被调用，所以自定义事件和 props 一样，都可以利用这一点来进行父子之间的通信。下面，我们在父组件，为子组件的元素绑定 send-student 事件： 12345678910111213141516171819202122232425262728293031323334353637import type &#123; log &#125; from &#x27;console&#x27;;&lt;script lang=&quot;ts&quot; setup&gt;import Son from &quot;@/components/communication/custom_event/Son.vue&quot;;import &#123; reactive &#125; from &quot;vue&quot;;let stud = reactive(&#123; name: &quot;&quot;, imageLink: &quot;&quot;, intro: &quot;&quot;,&#125;);function getStudInfo(studInfo: object) &#123; Object.assign(stud, studInfo);&#125;&lt;/script&gt;&lt;template&gt; &lt;div class=&quot;wrapper&quot;&gt; &lt;span v-show=&quot;stud.name&quot;&gt;姓名：&#123;&#123; stud.name &#125;&#125;&lt;/span&gt; &lt;img v-show=&quot;stud.imageLink&quot; :src=&quot;stud.imageLink&quot; /&gt; &lt;span v-show=&quot;stud.intro&quot;&gt;介绍：&#123;&#123; stud.intro &#125;&#125;&lt;/span&gt; &lt;!-- 绑定事件 --&gt; &lt;Son @send-student=&quot;getStudInfo&quot;&gt;&lt;/Son&gt; &lt;/div&gt;&lt;/template&gt;&lt;style scoped&gt;.wrapper &#123; background-color: #efc; box-shadow: 2px 1px 10px #878787; border-radius: 0.5rem; padding: 25px; box-sizing: border-box;&#125;img &#123; height: 100px; display: block;&#125;&lt;/style&gt; 接下来在子组件中定义和触发 send-student 事件。定义自定义事件需要用到 defineEmits 这个函数，该函数是一个宏，所以使用前不需要进行引入。函数接收一个事件数组，数组中的内容就是事件的名字。该函数返回一个 emit 函数，通过 emit 可以触发自定义事件。emit 接收两种参数，第一个参数是自定义事件的名字，表示声明被触发的是哪一个自定义事件，第二类参数就是事件的回调函数的参数，这些参数会被传入回调函数中。下面是子组件中定义和触发自定义事件的方法，自定义事件 send-student 将在按钮被点击后触发： 12345678910111213141516171819202122232425262728293031323334&lt;script lang=&quot;ts&quot; setup&gt;let studInfo = &#123; name: &quot;李小狼&quot;, imageLink: &quot;https://c-ssl.duitang.com/uploads/blog/202209/14/20220914164706_1c29a.jpeg&quot;, intro: &quot;李小狼，是CLAMP所著日本漫画《魔卡少女樱》及其衍生作品中的男主角，出生于中国香港。李小狼和木之本樱同年级的男生，友枝中学1年级2班/3班的学生。与魔法师库洛·里德是远亲（库洛里多的母亲出身中国李家），自身亦拥有很强的魔力。为了寻找被解除封印的库洛牌，来到日本，遇到了木之本樱。两人一起行动，逐渐相互产生了爱慕之情。擅长使用水、火、风、雷魔法，支持小樱收集库洛牌。虽然不爱说话给人一种酷酷的感觉，但其实是直爽又害羞的孩子。另外，还擅长弹钢琴、制作中式糕点。收集完卡片之后返回了香港。&quot;,&#125;;const emit = defineEmits([&quot;send-student&quot;]); // 定义自定义事件&lt;/script&gt;&lt;template&gt; &lt;div class=&quot;bkg&quot;&gt; &lt;span v-show=&quot;studInfo.name&quot;&gt;姓名：&#123;&#123; studInfo.name &#125;&#125;&lt;/span&gt; &lt;img v-show=&quot;studInfo.imageLink&quot; :src=&quot;studInfo.imageLink&quot; /&gt; &lt;span v-show=&quot;studInfo.intro&quot;&gt;介绍：&#123;&#123; studInfo.intro &#125;&#125;&lt;/span&gt; &lt;br&gt;&lt;br&gt; &lt;!-- 触发自定义事件 --&gt; &lt;button @click=&quot;emit(&#x27;send-student&#x27;, studInfo)&quot;&gt;发送学生信息&lt;/button&gt; &lt;/div&gt;&lt;/template&gt;&lt;style scoped&gt;.bkg &#123; background-color: cadetblue; box-shadow: 2px 1px 10px #878787; border-radius: 0.5rem; padding: 10px; margin-top: 20px;&#125;img &#123; height: 100px; display: block;&#125;&lt;/style&gt; 效果： 如此一来，通过自定义事件也完成了子组件到父组件的通信。一个注意事项：自定义事件的命名一般采用 kebab-case 的命名方式，即单词之间采用短横线来分隔，就像上面写的一样，这是官方推荐的写法。 12.3 mitt 之前介绍的 props 和自定义事件都可以实现组件间通信，但缺点是在处理子组件到父组件的通信时比较复杂，mitt 可以实现任意组件之间的通信。","categories":[{"name":"技术开发","slug":"技术开发","permalink":"https://example.com/categories/%E6%8A%80%E6%9C%AF%E5%BC%80%E5%8F%91/"},{"name":"前端","slug":"技术开发/前端","permalink":"https://example.com/categories/%E6%8A%80%E6%9C%AF%E5%BC%80%E5%8F%91/%E5%89%8D%E7%AB%AF/"}],"tags":[{"name":"web","slug":"web","permalink":"https://example.com/tags/web/"},{"name":"vue3","slug":"vue3","permalink":"https://example.com/tags/vue3/"},{"name":"前端","slug":"前端","permalink":"https://example.com/tags/%E5%89%8D%E7%AB%AF/"}]},{"title":"人工智能技术杂谈","slug":"人工智能技术","date":"2023-12-11T06:15:50.000Z","updated":"2025-04-09T11:50:01.800Z","comments":true,"path":"2023/12/11/人工智能技术/","permalink":"https://example.com/2023/12/11/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E6%8A%80%E6%9C%AF/","excerpt":"本文档内容包括支持向量机、隐马尔可夫模型、鲁滨逊归结原理和A*搜索。 支持向量机 支持向量机（support vector machines, SVM）是一种二分类模型，它的基本模型是定义在特征空间上的间隔最大的线性分类器，间隔最大使它有别于感知机；SVM还包括核技巧，这使它成为实质上的非线性分类器。SVM的的学习策略就是间隔最大化，可形式化为一个求解凸二次规划的问题，也等价于正则化的合页损失函数的最小化问题。SVM的的学习算法就是求解凸二次规划的最优化算法。","text":"本文档内容包括支持向量机、隐马尔可夫模型、鲁滨逊归结原理和A*搜索。 支持向量机 支持向量机（support vector machines, SVM）是一种二分类模型，它的基本模型是定义在特征空间上的间隔最大的线性分类器，间隔最大使它有别于感知机；SVM还包括核技巧，这使它成为实质上的非线性分类器。SVM的的学习策略就是间隔最大化，可形式化为一个求解凸二次规划的问题，也等价于正则化的合页损失函数的最小化问题。SVM的的学习算法就是求解凸二次规划的最优化算法。 01 线性模型 1.1 算法思路 假设训练样本在空间中的分布如下左图分布，圆圈和星星分别代表两类不同类别的数据，那么我们能找出一条直线，将两者分割开，我们就称这样的训练样本集为一个线性可分（Linear Separable）样本集，这样的模型就被称为线性模型；同理，倘若我们找不到这样一条直线，将两者完全分离开，如下右图所示，则称这样的训练样本集为一个线性不可分（Non-Linear Separable）样本集，这样的模型就被称为非线性模型。 支持向量机算法的思路大致是这样的：首先讨论如何在线性可分的训练样本集上找一条直线将样本分开，然后想办法将这样的方法推广到线性不可分的训练样本集上。所以，我们首先讨论第一部分：如何找到一条直线将线性可分训练样本集分开。 对于一个训练样本集，可以证明：只要存在一条直线可以将样本集分开，就肯定存在无数条直线能将该样本集分开（如下图所示）。既然如此，支持向量机提出的第一个问题就是：哪条直线是最好的？ 通过直觉来判断，我们也可以感受出上图中的红色直线应该是最好的，问题是为什么？要解答这个问题，我们就必须定义一种性能指标（Performace Measure），来评估每一条直线的好坏。 为了给出这个性能指标，支持向量机做的事情是，将上面的红线向左右两边平行移动，直到这条线碰到一个或几个样本点为止（如下图中两条虚线所示）： 然后，支持向量机给出了这个性能指标的定义，就是上图中两条虚线的距离（Gap），用 d 表示。而性能最好的那条线，就是能使 d 最大的那条线。但是这样还不完善，要知道，能使 d 最大的线也不唯一，将上图中的实线左右移动，作一条平行线，只要平行线不越过两条虚线所界定的范围，d 就是不变的，所以还得给出另一个限制条件：直线必须位于两根平行线的正中间，也就是使上图中的实线与左右两根平行虚线的距离分别为 $\\frac d2$。 1.2 数学描述 既然性能指标已经确定了，下一个问题就是如何描述这个优化过程了。在描述优化过程之前，我们还是先得给出一些定义，首先，我们将上面的 d 称为间隔（Margin），将虚线穿过的向量称为支持向量（Support Vectors）。通过上面对支持向量机算法的简单描述，我们可以发现，支持向量机找到的最优直线，只与支持向量有关，与其他向量无关，这就是为什么支持向量机也能用在小样本的数据上。 先给出线性模型的数学描述： 定义训练数据及标签为 (X1,y1)、(X2,y2)...(Xn,yn)，其中，X 是样本的特征，在上面给出的例子里，每个样本的特征是二维的，也就是说 $X=\\left[ \\begin{array}{} x_1 \\\\ x_2 \\end{array}\\right]$ ，分别对应 x 轴和 y 轴；而 y 是标签，在上面这个二分类问题里，标签只有两种，所以 y ∈ { − 1, 1}。 我们定义一个线性模型为 (W,b)，其中 W 是一个向量，其维数与特征向量 X 一致，b 是一个常数，一个线性模型确定一个超平面（Hyperplane），所谓超平面就是指划分空间的平面，超平面在二维空间里表现为我们上面所说的那条划分样本点的直线，而在更高的维度里就是一个平面，故称之为超平面。超平面由 W 和 b 确定，其方程为 WTX + b = 0。机器学习的目标，就是通过所有样本的特征 X 来找到一个 W 和 b，使能够确定一个超平面能划分所有的样本点。 一个训练集线性可分是指：对于 {(Xi,yi)}i = 1 ∼ N，$\\exist(W,b)$，使 ∀i = 1 ∼ N，有： 若 yi = + 1，则 WTXi + b ≥ 0； 若 yi = − 1，则 WTXi + b &lt; 0。 当然，上述线性可分的定义是不唯一的，将 ≥ 和 &lt; 换个位置也是一样。对于上面的定义，我们可以发现，凡是线性可分问题，一定存在 yi[WTXi+b] ≥ 0。 接下来给出支持向量机优化问题的数学描述： 目标：最小化 ∥W∥2 限制条件：yi[WTXi+b] ≥ 1 (i=1∼N) 对于上面这两个公式，相信很多人第一眼是懵的，因为按我们之前的描述，支持向量机算法就是去找一条使 d 最大且位于正中间位置的直线，怎么数学公式看起来跟这个过程完全没关系呢？ 要搞清楚这两个公式，我们得先弄清楚两个事实： 事实一：WTX + b = 0 与 aWTX + ab = 0 (a∈R+) 表示的是同一个平面。 事实二：向量 X0 到超平面 WTX + b = 0 的距离是 $d=\\frac{|W^TX_0+b|}{\\lVert W\\rVert}$。（不要慌，这其实就是高中学的点到平面的距离公式，以一维平面 w1x + w2y + b = 0，也就是直线为例，点 (x0,y0) 到这条直线的距离就是 $d=\\frac{|w_1x_0+w_2y_0+b|}{\\sqrt{w_1^2+w_2^2}}$，前面的那个公式只不是这个公式在高维情况下的推广） 基于事实二，我们知道，支持向量机要做的事情，就是在 X0 是支持向量的情况下，使 d 最大。 基于事实一，我们知道，我们可以找到一个正实数 a 来缩放 W 和 b，即 (W,b) → (aW,ab)，使 d 公式的分子 |WTX0+b| = 1。这样的话，d 的公式就变成了 $d=\\frac{1}{\\lVert W\\rVert}$。看到这个公式，就能明白为什么支持向量机的优化目标是最小化 ∥W∥2 了，因为最小化 ∥W∥2 就是最大化 d。 现在再来看限制条件，限制条件其实就是规定了，所有样本点到超平面的距离 WTXi + b，要么等于 d（支持向量），要么大于 d（非支持向量）。 至于为什么要再乘上一个 yi，其实看线性可分的定义就知道了，乘上 yi 是为了与线性可分的定义相统一。 补充： 为什么一定要使 |WTX0+b| = 1， |WTX0+b| = 2 可不可以？可以，等于 1 还是等于 2 或是其他值都没有任何关系，这只取决于 a 的大小，而 a 并不改变超平面。 对于任何线性可分样本集，一定能找到一个超平面分割所有样本点；反之，如果是线性不可分，那么将找不到任何一个能满足要求的 W 和 b。 某些书上会将优化目标写成最小化 $\\frac12\\lVert W\\rVert^2$，这其实没有任何问题，加上 $\\frac12$ 只是为了求导方便。 支持向量机要解决的问题其实是一个凸优化问题，而且是一个二次规划问题，二次规划问题的特点是： 目标函数（Objective Function）是二次项； 限制条件是一次项。 对于凸优化问题，要么无解，要么只有一个解。凸优化问题是计算机领域研究最多的问题，因为凸优化问题要么无解，要么只要能找到一个解，那便是它唯一的解。所以只要证明一个问题是凸优化问题，那么我们只要找到一个局部极值，也便找到了它的全局极值，我们便可认定这个问题已经被解决了。 非凸问题的目标函数图像是一条包含很多局部极值的曲线，会使得机器很容易落入局部最优解的陷阱。支持向量机算法优美的地方就在于，它将求解目标化成了一个凸优化问题。 02 非线性模型 2.1 优化目标 之前已经讨论过，非线性模型不是线性可分的，也就是说找不到一个 W 和 b，使之确定一个能完美分割所有样本点的超平面，即限制条件 yi[WTXi+b] ≥ 1 (i=1∼N) 是不可满足的，原本的优化目标是无解的。SVM 处理非线性模型的方法其实不难理解，就是在线性模型的基础上引入了一个松弛变量（Slack Variable），用 ξ (ξ≥0) 表示。新的优化目标如下： 目标：最小化 $\\frac12\\lVert W\\rVert^2+C\\sum_{i=1}^N\\xi_i$ 限制条件： yi[WTXi+b] ≥ 1 − ξi (i=1∼N) ξi ≥ 0 可以发现，新的优化目标中，限制条件变成了 yi[WTXi+b] ≥ 1 − ξi (i=1∼N)，只要这个 ξi 取得足够大，那么大于等于号右边就会无限小，那么限制条件就有了满足的可能；但同时，也不能允许 ξi 无限大，不然就没有意义了，所以新的最小化目标函数的末尾还要加上 ξi。 接下来要明确，在上面的式子中，哪些是已知的，哪些是要求解的参数。显然，X和yi是已知的，W、b以及ξ是要求的，但是这里还有个C，这个C是什么？C是一个由人事先设定的参数，这种参数一般称为超参数（Hyperparameter），作用是平衡$\\frac{1}{2}\\lVert\\ W\\rVert^{2}$和$\\sum_{i=1}^{N}\\xi_{i}$的权重。至于C具体取多少是没有定论的，一般是凭经验，选定一个区间，然后一个一个尝试。SVM很方便的一点就是，它只有这一个参数需要人来设置，但是在神经网络里，要去一个一个尝试的参数可能有很多。 2.2 高维映射 虽然通过引入松弛变量，我们将非线性问题转换为了一个线性可分问题，但是还是存在一个问题，那就是求解目标的本质没有变，最后仍然是找出一条直线，来分割样本点，也就是说，即使一个样本集用肉眼看就能看出其能被一条简单的曲线分割，SVM 还是会找一条直线来分割样本点，如下图所示： 这显然不是我们想要的。一些算法会很符合直觉地去找非直线来分割样本集，例如决策树是用矩形来分割，但是 SVM 的思想很精妙，它仍然是找直线，不过它不是在当前空间里去找，而是到高维空间里去找。它定义了一个高维映射 ϕ(X)，通过 ϕ，能将 X 这个低维向量转化成一个高维向量 ϕ(X)。也就是说，也许在低维空间中，我们不容易去找一条直线能刚刚好分割所有样本点，那么我们就去高维空间中找，或许在高维空间中，我们就能找到样一条理想的直线了。 接下来我们用异或问题的例子，来具体解释这个过程为什么有效。异或问题是二维空间下最简单的非线性问题，其在二维空间中存在如下样本点分布： 我们先将图中四个样本点表示为 X1、X2、X3 和 X4，并且 X1 和 X2 属于一个类别 C1，X3 和 X4 属于一个类别 C2，有： $$ \\begin{aligned} &amp;X_1=\\left[ \\begin{array}{} 0 \\\\ 0 \\end{array} \\right]\\quad X_2=\\left[ \\begin{array}{} 1 \\\\ 1 \\end{array} \\right]\\quad\\in C_1\\\\ &amp;X_3=\\left[ \\begin{array}{} 1 \\\\ 0 \\end{array} \\right]\\quad X_4=\\left[ \\begin{array}{} 0 \\\\ 1 \\end{array} \\right]\\quad\\in C_2\\\\ \\end{aligned} $$ 定义高维映射函数为： $$ \\phi(X):\\quad X=\\left[ \\begin{array}{} a \\\\ b \\end{array} \\right]\\overset{\\phi}{\\longrightarrow}\\phi(X)=\\left[ \\begin{array}{} a^2 \\\\ b^2 \\\\ a \\\\ b \\\\ ab \\end{array} \\right] $$ 则经过映射，四个样本点将变为： $$ \\begin{aligned} &amp;\\phi(X_1)=\\left[ \\begin{array}{} 0 \\\\ 0 \\\\ 0 \\\\ 0 \\end{array} \\right]\\quad \\phi(X_2)=\\left[ \\begin{array}{} 1 \\\\ 1 \\\\ 1 \\\\ 1 \\end{array} \\right]\\quad\\in C_1\\\\ &amp;\\phi(X_3)=\\left[ \\begin{array}{} 1 \\\\ 0 \\\\ 1 \\\\ 0 \\\\ 0 \\end{array} \\right]\\quad \\phi(X_4)=\\left[ \\begin{array}{} 0 \\\\ 1 \\\\ 0 \\\\ 1 \\\\ 0 \\end{array} \\right]\\quad\\in C_2\\\\ \\end{aligned} $$ 现在，X 变成了五维向量，则 W 也要变成五维向量，b 仍然为常量，求解的目标就变成在五维空间中找一个超平面来分割四个样本点了。能做到分割的超平面不唯一，这里举一个例子： $$ W=\\left[ \\begin{array}{} -1 \\\\ -1 \\\\ -1 \\\\ -1 \\\\ 6 \\end{array} \\right]\\quad b=1 $$ 将样本点代入超平面的方程： $$ \\begin{aligned} &amp;W^T\\phi(X_1)+b=1&gt;0\\\\ &amp;W^T\\phi(X_2)+b=3&gt;0\\\\ &amp;W^T\\phi(X_3)+b=-1&lt;0\\\\ &amp;W^T\\phi(X_4)+b=-1&lt;0\\\\ \\end{aligned} $$ 如上，该超平面刚刚好把 X1、X2 与 X3、X4 分开了。也就是说，在低维空间中线性不可分的样本集，可能在高维空间中就是线性可分的，这也就是我们要去升维的原因。关于这一点也有很多人研究过，它们的结论是，对于任何线性不可分的样本集，特征空间的维数越高，其被线性分割的概率也越大；若维数趋近无穷大，那么其被线性分割的概率将达到 1. 2.3 核函数 在引入了高维映射之后，优化式 1 就变成了 yi[WTϕ(Xi)+b] ≥ 1 − ξi (i=1∼N)，虽然看起来只有 Xi 发生了变化，但不要忘记 W 也跟着一起升维了。那么现在面临的问题就是：如何选取 ϕ ？SVM 的回答是：无限维。 将特征空间增长到无限维，线性不可分问题就绝对可以变成线性可分。但是问题在于，当 ϕ(X) 变成无限维，W 也要变成无限维，那这个问题就没有办法做了。这也是 SVM 巧妙的另一个地方，它在将特征空间映射到无限维的同时，又采用有限维的手段。 SVM 的意思是：我们可以不知道无限维映射 ϕ(X) 的显式表达，我们只要知道一个核函数（Kernel Function）： K(X1,X2) = ϕ(X1)Tϕ(X2) 则优化式 1 仍然可解。K(X1,X2) 其实计算的是 ϕ(X1) 和 ϕ(X2) 的内积，虽然 ϕ(X1) 和 ϕ(X2) 是无限维的，但是两者仍然能进行内积计算，得到的结果 K(X1,X2) 是一个数。 核函数的要求是：能将函数的形式最终拆成 ϕ(X1)Tϕ(X2) 的形式。常用的核函数有如下几个： 高斯核：$K(X_1,X_2)=e^{-\\frac{\\lVert X_1-X_2\\rVert^2}{2\\sigma^2}}=\\phi(X_1)^T\\phi(X_2)$，σ2 是方差。 多项式核：K(X1,X2) = (X1TX2+1)d = ϕ(X1)Tϕ(X2)，d 是多项式阶数。 而能将核函数 K(X1,X2) 拆成 ϕ(X1)Tϕ(X2) 的充要条件是： K(X1,X2) = K(X2,X1)，即交换性； ∀Ci, Xi (i=1∼N)，有 $\\sum_{i=1}^{N}\\sum_{j=1}^{N}C_iC_jK(X_i,X_j)\\ge0$，即半正定性，也就是说，我们选取的核函数，必须要对任意选定的常数 C 和向量 X 都满足该式； 2.4 原问题到对偶问题 现在我们有了核函数，那么我们要怎样利用核函数，来替代优化式 1 中的 ϕ(X) 呢？在这之前，请先阅读优化理论相关的内容。在稍微了解了优化理论中的原问题和对偶问题后，我们要做的，就是把 SVM 的优化问题从原问题转换成对偶问题。 首先，我们把 SVM 的优化问题转换成原问题： 对于目标函数，$\\frac12\\lVert W\\rVert^2+C\\sum_{i=1}^N\\xi_i$ 是一个凸函数。 对于限制条件，ξi ≥ 0 不满足原问题的限制条件形式，得先将大于等于号变成小于等于号，也就是变成 ξi ≤ 0，那么，目标函数就也得变换一下，变成 $\\frac12\\lVert W\\rVert^2-C\\sum_{i=1}^N\\xi_i$；同样，另一个限制条件也得变换一下，变成 yi[WTXi+b] ≥ 1 + ξi (i=1∼N)，但是这个不等式也不满足原问题的要求，必须将不等式右边变成 0，所以得到 1 + ξi − yi[WTXi+b] ≤ 0 (i=1∼N)。于是得到优化目标的原问题形式，新的优化目标： 目标：最小化 $\\frac12\\lVert W\\rVert^2-C\\sum_{i=1}^N\\xi_i$ 限制条件： 1 + ξi − yi[WTϕ(Xi)+b] ≤ 0 (i=1∼N) ξi ≤ 0 将其转换为对偶问题： 目标：最大化 $\\theta(\\alpha,\\beta)=\\underset{(w,\\xi_i,b)}{\\inf}\\{\\frac12\\lVert W\\rVert^2-C\\sum_{i=1}^N\\xi_i+\\sum_{i=1}^{N}\\alpha_i(1+\\xi_i-y_i[W^T\\phi(X_i)+b])+\\sum_{i=1}^N\\beta_i\\xi_i\\}$ 限制条件： αi ≥ 0 (i=1∼N) βi ≥ 0 (i=1∼N) 解释一下这样变换的原因： 原问题中的 w 对应了原问题要求解的变量，有三个，分别是 W、b 和 ξ，所以对偶问题中要遍历所有的 w，到这里就变成了遍历所有的 W、b 和 ξ。 根据对偶问题的定义，$L(\\omega,\\alpha,\\beta)=f(\\omega)+\\sum_{i=1}^{K}\\alpha_ig_i(\\omega)+\\sum_{i=1}^M\\beta_ih_i(\\omega)$，其中，$f(w)=\\frac12\\lVert W\\rVert^2-C\\sum_{i=1}^N\\xi_i$，这一点是没有疑问的，关键是下面，千万不要以为这里的 α 和 β 分别对应了上面的 αi 和 βi，不是这样的，在对偶问题中，α 管的是不等式条件，每个不等式条件要与 α 相乘，β 管的是等式条件，每个等式条件要与 β 相乘。但是在这里，SVM 原问题中的限制条件都是不等式，所以应该只有 α，没有 β，只是说为了方便表示，这里仍然沿用 αi 和 βi，并且，由于 α 应该大于 0，所以到这里就变成了 αi ≥ 0 (i=1∼N)，并且 βi ≥ 0 (i=1∼N)。其他部分就是照抄的关系了。 接下来我们就来求一下 L(W,ξi,b,α) 的最小值： 令偏导 $\\frac{\\partial L}{\\partial W}=0$，$\\frac{\\partial L}{\\partial \\xi_i}=0$，$\\frac{\\partial L}{\\partial b}=0$： $$ \\begin{aligned} &amp;\\frac{\\partial L}{\\partial W}=0\\Rightarrow W=\\sum_{i=1}^N\\alpha_iy_i\\phi(X_i)&amp;①\\\\ &amp;\\frac{\\partial L}{\\partial \\xi_i}=0\\Rightarrow C=\\beta_i+\\alpha_i&amp;②\\\\ &amp;\\frac{\\partial L}{\\partial b}=0\\Rightarrow \\sum_{i=1}^N\\alpha_iy_i=0&amp;③ \\end{aligned} $$ 接下来，我们要将上面得到的三个式子代回到 L(W,ξi,b,α) 中去，好消息是，将式 1 和式 3 代入之后，式子中的大部分项就能被消掉了，得到 $\\theta(\\alpha,\\beta)=\\frac12\\lVert W\\rVert^2+\\sum_{i=1}^N\\alpha_i-\\sum_{i=1}^N\\alpha_iy_iW^T\\phi(X_i)$，先来计算 $\\frac12\\lVert W\\rVert^2$： $$ \\begin{aligned} \\frac12\\lVert W\\rVert^2&amp;=\\frac12W^TW\\\\ &amp;=\\frac12(\\sum_{i=1}^N\\alpha_iy_i\\phi(X_i))^T(\\sum_{j=1}^N\\alpha_jy_j\\phi(X_j))\\\\ &amp;=\\frac12\\sum_{i=1}^N\\sum_{j=1}^N\\alpha_i\\alpha_jy_iy_j\\phi(X_i)^T\\phi(X_j)\\\\ &amp;=\\frac12\\sum_{i=1}^N\\sum_{j=1}^N\\alpha_i\\alpha_jy_iy_jK(X_i,X_j) \\end{aligned} $$ 一件惊喜的事情：上式的最终化简结果里出现了核函数！接下来化简 $-\\sum_{i=1}^N\\alpha_iy_iW^T\\phi(X_i)$： $$ \\begin{aligned} -\\sum_{i=1}^N\\alpha_iy_iW^T\\phi(X_i)&amp;=-\\sum_{i=1}^N\\alpha_iy_i(\\sum_{j=1}^N\\alpha_jy_j\\phi(X_j))^T\\phi(X_i)\\\\ &amp;=-\\sum_{i=1}^N\\sum_{j=1}^N\\alpha_i\\alpha_jy_iy_j\\phi(X_j)^T\\phi(X_i)\\\\ &amp;=-\\sum_{i=1}^N\\sum_{j=1}^N\\alpha_i\\alpha_jy_iy_jK(X_i,X_j) \\end{aligned} $$ 所以，最后会得到： $$ \\theta(\\alpha,\\beta)=\\sum_{i=1}^{N}\\alpha_i-\\frac12\\sum_{i=1}^N\\sum_{j=1}^N\\alpha_i\\alpha_jy_iy_jK(X_i,X_j) $$ 经过这样一系列的推导，最终问题的形式会变成： 目标：最大化 $\\theta(\\alpha)=\\sum_{i=1}^{N}\\alpha_i-\\frac12\\sum_{i=1}^N\\sum_{j=1}^N\\alpha_i\\alpha_jy_iy_jK(X_i,X_j)$ 限制条件： 0 ≤ αi ≤ C (i=1∼N) $\\sum_{i=1}^N\\alpha_iy_i=0$ 解释一下限制条件：根据之前求的偏导我们得到了 βi + αi = C，由于之前的限制条件规定了 βi ≥ 0 以及 αi ≥ 0，所以我们可以直接把这两个条件合并成一个条件，即 0 ≤ αi ≤ C (i=1∼N)，那么为什么要合并呢？因为我们现在的目标函数中只剩下了 αi 和 αj，已经不存在 β 了；而第二个限制条件则是直接照抄的令 $\\frac{\\partial L}{\\partial b}=0$ 得到的结果。 在这个对偶问题中，目标函数仍然是一个凸函数。并且，其中未知的参数只有 αi 和 αj，核函数是已经确定的了。由于是一个凸优化问题，所以它应该是很容易求解的。有一种凸优化问题求解算法叫做 SMO 算法，在这里不再展开叙述，感兴趣的同学自行了解。总之，我们只需要知道，这个问题是有解的。 但是到这里还没结束，我们现在已经将 SVM 的优化问题从原问题转换成了对偶问题，将 ϕ(Xi) 用核函数进行了替换，但是还有一个问题：对偶问题是求解 αi 和 αj，而我们要的是 W 和 b，如何在这两者之间进行转换？ 这里又体现了 SVM 的精妙之处，我们并不需要知道 W 具体长什么样，根据我们之前求偏导的结果，我们知道 $W=\\sum_{i=1}^N\\alpha_iy_i\\phi(X_i)$，同时我们也知道，最后分类的方法是，对于测试样本 X，若： WTϕ(X) + b ≥ 0，则 y = + 1 WTϕ(X) + b &lt; 0，则 y = − 1 我们将 $W=\\sum_{i=1}^N\\alpha_iy_i\\phi(X_i)$ 代入到不等式左边的式子中，就会得到： $$ \\begin{aligned} W^T\\phi(X)+b&amp;=\\sum_{i=1}^N[\\alpha_iy_i\\phi(X_i)]^T\\phi(X)+b\\\\ &amp;=\\sum_{i=1}^N\\alpha_iy_i\\phi(X_i)^T\\phi(X)+b\\\\ &amp;=\\sum_{i=1}^N\\alpha_iy_iK(X_i,X)+b \\end{aligned} $$ 所以说，我们并不需要知道 W 的具体值，我们只需要有核函数，就能对样本进行分类。现在的关键问题是：b 是多少？b 的求解并不简单，需要用到优化理论中的 KKT 条件。 根据 KKT 条件，当原问题和对偶问题满足强对偶定理时，∀i = 1 ∼ K，要么 βi* = 0，要么 hi(ω*) = 0；要么 αi* = 0，要么 gi(ω*) = 0，而在这个问题中，g(W) = 1 + ξi − yi[WTϕ(Xi)+b]，所以，要么 αi = 0，要么 g(W) = 1 + ξi − yi[WTϕ(Xi)+b] = 0. 现在，我们取一个 αi，使之 0 &lt; αi &lt; C（这是肯定能满足的，原因见限制条件），则根据 KKT 条件，肯定有 1 + ξi − yi[WTϕ(Xi)+b] = 0。又因为 βi + αi = C，根据 KKT 条件，所以 βi ≠ 0，h(W) = ξi = 0. 将 ξi = 0 代入前式，就有： $$ \\begin{aligned} &amp;1-y_i[W^T\\phi(X_i)+b]=0\\\\ &amp;\\Downarrow\\text{to rearrange the terms}\\\\ &amp;b=\\frac{1-y_iW^T\\phi(X_i)}{y_i}\\\\ &amp;\\Downarrow\\text{to substitute }W^T\\phi(X)=\\sum_{i=1}^N\\alpha_iy_iK(X_i,X)\\text{ into it}\\\\ &amp;b=\\frac{1-y_i\\sum_{i=1}^N\\alpha_iy_iK(X_i,X)}{y_i} \\end{aligned} $$ 于是，就连 b 我们也知道了。在现实中，我们一般会遍历所有 αi ∉ {0, C}（在上面的讨论中我们只取了一个 α），然后计算 b，最后取 b 的平均值，这样能使结果更加精确。 2.5 算法流程总结 训练流程 输入：{(Xi,yi)} i = 1 ∼ N 求解优化问题（SMO 算法）： 最大化 $\\theta(\\alpha)=\\sum_{i=1}^{N}\\alpha_i-\\frac12\\sum_{i=1}^N\\sum_{j=1}^N\\alpha_i\\alpha_jy_iy_jK(X_i,X_j)$ 限制条件： 0 ≤ αi ≤ C (i=1∼N) $\\sum_{i=1}^N\\alpha_iy_i=0$ 通过上一步计算出来的 αi 来计算 b：$b=\\frac{1-y_i\\sum_{i=1}^N\\alpha_iy_iK(X_i,X)}{y_i}$ 测试流程 输入测试样本 X 分类： 若 $\\sum_{i=1}^N\\alpha_iy_iK(X_i,X)+b\\ge0$，则 y = + 1 若 $\\sum_{i=1}^N\\alpha_iy_iK(X_i,X)+b\\lt0$，则 y = − 1 可以发现，最终训练流程和测试流程中完全不需要用到无限维的 ϕ(X)，只需要使用核函数就行了。这也就是 SVM 用有限维手段来处理无限维问题的方法。 03* 补充：优化理论 在优化领域中，在优化理论中，原问题（Prime Problem）和对偶问题（Dual Problem）是一对相关的数学问题。 原问题的定义如下： 目标：最小化 f(ω) 限制条件： gi(ω) ≤ 0 (i=1∼K) hi(ω) = 0 (i=1∼M) 原问题是非常普适化的，虽然上面展示的是最小化问题，但只需要在 f(ω) 前加一个负号，立马就变成了最大化问题；同样地，在 gi(ω) ≤ 0 中加一个负号，也就变成了 − gi(ω) ≥ 0；而在式 2 的左边减去一个常数 C，就立马变成了 hi(ω) − C = 0，这样就可以把等式右边的 0 变成任意常数 C。 对偶问题是从原问题派生出来的一个新问题，对偶问题首先定义了一个函数： $$ \\begin{aligned} L(\\omega,\\alpha,\\beta)&amp;=f(\\omega)+\\sum_{i=1}^{K}\\alpha_ig_i(\\omega)+\\sum_{i=1}^M\\beta_ih_i(\\omega)\\quad&amp;①\\\\ &amp;=f(\\omega)+\\alpha^Tg(\\omega)+\\beta^Th(\\omega)\\quad&amp;② \\end{aligned} $$ 上式中，α 和 β 是两个和 ω 维数一样的向量，并且分别乘上了不等式的限制条件和等式的限制条件。式 ① 是该式的代数形式，式 ② 是该式的矩阵形式。有了这个函数，我们就可以给出对偶问题的定义了： 目标：最大化 $\\theta(\\alpha,\\beta)=\\underset{\\omega}{\\inf}\\{L(\\omega,\\alpha,\\beta)\\}$ 限制条件：αi ≥ 0 (i=1∼K) 解释一下这里的目标函数，inf 就是求最小值的意思，下面的 ω 是指，遍历所有每个 ω 对应的 L(ω,α,β) ，所以 $\\underset{\\omega}{\\inf}\\{L(\\omega,\\alpha,\\beta)\\}$ 就是指，求所有 ω 对应的 L(ω,α,β) 中，L(ω,α,β) 最小的取值。而通过 θ(α,β) 可以看出，α 和 β 是固定的，也就是说，我们每确定一组 α 和 β，就去求一次 L(ω,α,β) 的最小值，所以 θ 是只和 α 和 β 有关的函数。而我们的目标又是最大化 θ(α,β)，所以，实质上我们就是要使 L(ω,α,β) 的最小值最大化。而对偶问题的限制条件很简单，只要求每个 αi 大于 0 即可。 接下来我们就来介绍一下原问题和对偶问题的关系，有一条定理是这样的： 如果 ω* 是原问题的解，而 α* 和 β* 是对偶问题的解，则有 f(ω*) ≥ θ(α*,β*)。 这条定理的证明如下： 由于 α* 和 β* 是对偶问题的解，则下式肯定成立： $$ \\theta(\\alpha^*,\\beta^*)=\\underset{\\omega}{\\inf}\\{L(\\omega,\\alpha^*,\\beta^*\\}\\le L(\\omega^*,\\alpha^*,\\beta^*) $$ 这里的 ω* 是指一个具体的 ω 的值。根据 L(ω,α,β) 的定义，展开不等式右边的式子有： $$ L(\\omega^*,\\alpha^*,\\beta^*)=f(\\omega^*)+\\sum_{i=1}^{K}\\alpha_i^*g_i(\\omega^*)+\\sum_{i=1}^M\\beta_i^*h_i(\\omega^*) $$ 既然 ω* 是原问题的解，那么 ω* 必然满足原问题的两个限制条件，也就是说 gi(ω*) ≤ 0，hi(ω*) = 0；另外，既然 α* 是对偶问题的解，那么 α* 也必然满足 α* ≥ 0。进一步，既然 hi(ω*) = 0，那么上式中 $\\sum_{i=1}^M\\beta_i^*h_i(\\omega^*)=0$；既然 gi(ω*) ≤ 0，α* ≥ 0，那么上式中 $\\sum_{i=1}^{K}\\alpha_i^*g_i(\\omega^*)\\le0$，所以存在： $$ \\theta(\\alpha^*,\\beta^*)=\\underset{\\omega}{\\inf}\\{L(\\omega,\\alpha^*,\\beta^*\\}\\le L(\\omega^*,\\alpha^*,\\beta^*)\\le f(\\omega^*) $$ 证毕。 遂定义： G = f(ω*) − θ(α*,β*) ≥ 0 G 叫做原问题与对偶问题的间距（Duality Gap）。对应某些特定的优化问题，可以证明 G = 0. 这里不再证明，直接给出问题的结论——强对偶定理： 若 f(ω) 为凸函数，且 g(ω) = Aω + b（线性函数），h(ω) = CW + d（一组线性函数），则此优化问题的原问题和对偶问题的间距是 0，即 f(ω*) = θ(α*,β*)。 问题是，强对偶定理的前提成立意味着什么？假设现在原问题和对偶问题满足强对偶定理，即 f(ω*) = θ(α*,β*) 成立，那么就有 $f(\\omega^*)=\\theta(\\alpha^*,\\beta^*)=\\underset{\\omega}{\\inf}\\{L(\\omega,\\alpha^*,\\beta^*\\}$，也就是说，原问题的解 ω*，刚刚就是对偶问题在 α* 和 β* 确定时，取到最小值的那个点。 更加精妙的是，当 G = 0 成立时，$\\sum_{i=1}^{K}\\alpha_i^*g_i(\\omega^*)+\\sum_{i=1}^M\\beta_i^*h_i(\\omega^*)=0$，其中，$\\sum_{i=1}^M\\beta_i^*h_i(\\omega^*)$ 等于 0 不用再说了，但 $\\sum_{i=1}^{K}\\alpha_i^*g_i(\\omega^*)=0$ 意味着，∀i = 1 ∼ K，要么 αi* = 0，要么 gi(ω*) = 0。这个条件叫做 KKT 条件。 隐马尔可夫模型 隐马尔可夫模型 HMM（Hidden Markov Model）是一种统计模型，用来描述一个隐含未知量的马尔可夫过程（马尔可夫过程是一类随机过程，它的原始模型是马尔科夫链），它是结构最简单的动态贝叶斯网，是一种著名的有向图模型，主要用于时序数据建模，在语音识别、自然语言处理等领域有广泛应用，是一种生成式模型。 01 马尔可夫模型 在学习隐马尔可夫模型之前，我们先来了解一下它的前生——马尔可夫模型 MM（Markov Model）。 我们用一个例子进行引入：天气的变化应该具有某种联系。晴天、多云和暴雨这三种天气之间的转换应该存在某种规律，下图中的箭头表示两种天气之间转换的概率： 于是我们能得到一个状态转移概率矩阵： 根据该矩阵，我们就能在知道今天天气的情况下，预测明天的天气。显然，这种预测是建立在未来所处的状态仅与当前状态有关的假设上的，即第二天的天气只取决于前一天的天气。这种假设就是马尔可夫假设，符合这种假设描述的随机过程，就被称为马尔可夫过程，其具有马尔可夫性，即无后效性。 令 qt 表示在时刻 t 系统所处的状态，令 Si 表示某一具体状态，则 qt = Si 表示在某一时刻 t，系统处于状态 Si，令 P(qt + 1=Si|qt=Sj) 表示在前一时刻 t 系统处于状态 Sj 的情况下，下一时刻 t + 1 系统处于状态 Si 的概率。基于马尔可夫假设，则在马尔可夫模型中存在下列关系： P(qt + 1=Si|qt=Sj,qt − 1=Sk,...) = P(qt + 1=Si|qt=Sj) 除了状态转移概率矩阵（用 A 表示）之外，我们还需要知道所有状态的初始状态概率向量 Π，设系统中一共有 N 种状态，则 Π 的长度为 N，Π 中的每一个元素代表系统的初始状态为某一状态的概率，且有 $\\sum_{i=1}^N\\Pi_i=1$。 假设我们想计算一下今天 t = 1 的天气状况，则我们可以得到： $$ P(q_1=S_{\\text{sun}})=P(q_1=S_{\\text{sun}}|q_0=\\sum_{i=1}^{3}S_i)=\\sum_{i=1}^{3}\\Pi_{S_i}\\times A_{S_i\\rightarrow S_{\\text{sun}}} $$ 用文字形式表示就是： 02 隐马尔可夫模型 2.1 概念 而隐马尔可夫模型就比马尔可夫模型要复杂多了。我们还是用上面这个例子进行引入，但是这次我们漂流到了一个岛上，这里没有天气预报，只有一片海藻，海藻具有干燥、较干、较湿和湿润四种状态。现在我们没有直接的天气信息了，但是天气状况跟海藻的状态还是有一定联系的，虽然看不见天气状况，但其决定了海藻的状态，所以我们还是能从海藻的状态推知天气的状态。 在这个例子里，海藻是能看到的，那它就是观测状态；天气信息是看不到的，那它就是隐藏状态。其中，隐藏状态天气时是决定性因素，观测状态是被决定因素，由隐藏状态到观测状态，这就是隐马尔可夫模型。 如上图所示，观测状态（海藻的状态）有 4 个，而隐藏状态（天气）只有 3 个，说明观测状态与隐藏状态的数量并不是一一对应的，可以根据需要定义。我们可以画出更加抽象的隐马尔可夫模型的示意图： 图中，Zi 表示隐藏状态，Xi 表示观测状态，隐藏状态决定了观测状态，所以箭头由 Z 指向 X。并且，隐藏状态之间还可以相互转换，所以 Zi 和 Zj 之间也有箭头。根据马尔可夫假设，下一时刻的状态只取决于当前时刻的状态，所以，对于观测状态和隐藏状态来讲，都存在如下关系： $$ \\begin{aligned} &amp;P=(Z_t|Z_{t-1},X_{t-1},Z_{t-2},X_{t-2},...,Z_1,X_1)=P(Z_t|Z_{t-1})\\\\ &amp;P=(X_t|Z_{t},X_{t},Z_{t-1},X_{t-1},...,Z_1,X_1)=P(X_t|Z_t)\\\\ \\end{aligned} $$ 2.2 组成 马尔可夫模型有两个组成部分——初始状态概率向量 Π 和 状态转移概率矩阵 A。 而隐马尔可夫模型有则有三个组成部分： 初始状态概率向量 Π 状态转移概率矩阵 A 观测状态概率矩阵 B 其中，Π 是针对隐藏状态来说的，因为隐藏状态决定了观测状态；A 矩阵是针对隐藏状态来说的，因为隐马尔可夫模型中进行状态转移的是隐藏状态；而 B 是由隐藏状态到观测状态转移的概率矩阵，在上例中，矩阵 B 可表示如下： 也就是说，由 Zi → Zj 的转换看矩阵 A，由 Zi → Xi 的转换看矩阵 B。 因此，隐马尔可夫模型 λ 可以用三元符号表示： λ(A,B,Π) 2.3 求解目标 HMM 的求解目标有三个： 给定模型 λ(A,B,Π) 及观测序列 O = {o1, o2, ..., ot}，计算该观测序列出现的概率 P(O|λ)； 给定观测序列 O = {o1, o2, ..., ot}，求解参数 (A,B,Π) 使得 P(O|λ) 最大； 已知模型 λ(A,B,Π) 和观测序列 O = {o1, o2, ..., ot}，求状态序列，使得 P(I|O,λ) 最大。 03 暴力求解法 我们要求的是在给定模型下观测序列出现的概率，那如果我们能把所有的隐藏序列都列出来，也就可以知道联合概率分布 P(O,I|λ) 了（其中，I 为 O 对应的隐藏状态序列），再根据 P(O|λ) = ∑IP(O,I|λ)，我们就能求得观测序列出现的概率。 根据联合概率分布的公式：P(X=x,Y=y) = P(X=x)P(Y=y|X=x)，可得 P(O,I|λ) 的求解方法： P(O,I|λ) = P(I|λ)P(O|I,λ) 其中，P(I|λ) 是在给定模型下，一个隐藏序列出现的概率，即 P(I|λ) = P(i1,i2,...,in|λ) = P(i1|λ)P(i2|λ)...P(in|λ)。那么怎么求 P(in|λ)？别忘了状态转移概率矩阵 A 的存在，A 所记录的不就是隐藏状态之间转换的概率吗？所以可得： P(I|λ) = πi1ai1i2ai2i3...ait − 1it 接下来要求的就是 P(O|I,λ)，它的含义是：在给定模型下，当隐藏序列为 I 时，观测序列为 O 的概率。求解 P(O|I,λ) 的方法和求解 P(I|λ) 的方法是一样的，还记得观测状态概率矩阵 B 吗？B 记录的正是从隐藏序列到观测序列转换的概率，所以 P(O|I,λ) 的计算方法如下： P(O|I,λ) = bi1o1bi2o2...bitot 于是，我们只需要将上面两个式子乘在一起，就能得到 P(O,I|λ) 了： P(O,I|λ) = πi1ai1i2bi1o1ai2i3bi2o2...ait − 1itbitot 则观测序列 O 出现的概率为： P(O|λ) = ∑IP(O,I|λ) = ∑i1, i2, ..., iTπi1ai1i2bi1o1ai2i3bi2o2...ait − 1iTbitoT 解释一下上面的公式：我们要求的是在给定模型下，某一观测序列出现的概率。暴力求解的方法找出所有可能的隐藏序列，将由这些隐藏序列得到该观测序列的概率全部加起来，最终得到该观测序列的概率。假设隐藏状态数有 N 个，我们需要遍历每一个隐藏序列，序列的长度为观测状态数 T，所以可能的隐藏序列有 NT 种，而对于每一个序列，都要遍历其 T 个 ai 和 bi，加起来就是 2T，计算时间复杂度时省去系数，则该算法的时间复杂度将达到 O(TNT)。 04 前向算法 4.1 算法解析 暴力求解法告诉我们隐马尔可夫模型的问题看上去是可解，但高昂的时间开销却是不可承受的。对此，有人提出了前向算法，该算法利用动态规划的思想来求解该问题，降低了时间复杂度。 给定 t 时刻的隐藏状态为 qi（注意，这里的 i 是指一种具体的隐藏状态，例如晴天、雨天等，是固定好的），观测序列为 o1, o2, ...on 的概率叫做前向概率，定义为： αt(i) = P(o1,o2,...,ot,St=qi|λ) 换句话来讲，前向概率就是在给定某一观测序列的情况下，某一时刻的状态刚刚好为 qi 的概率。 则，当 t = T 时，αT(i) = P(o1,o2,...,oT,ST=qi|λ) 表示最后一个时刻，隐藏状态为状态 qi 并且得到观察序列为 o1, o2, ..., oT 的概率。现在我们回来思考一下我们要解决的最原始的问题是什么，应该是 P(O|λ) = P(o1,o2,...oT|λ)，而 αT(i) 和它相比，末尾多了个 ST = qi，也就是说 αT(i) 还要求最终的隐藏状态必须为 qi，貌似和原本的问题相比有点画蛇添足，但仔细想想，如果我们能把所有最终可能的隐藏状态都拿过来，求 αT(1) + αT(2) + · · · + αT(n)，那不就大功告成了？所以现在的问题就变成了如何求解 T 时刻的前向概率，这是一个动态规划的问题。 在第一个时刻：α1(i) = P(o1,S1=qi|λ) 表示第一时刻的隐藏状态为 qi，观测序列为 o1 的概率，其结果为（这里的 bi(o1) 就表示由隐藏状态 qi 转换为观测状态 o1 的概率，是矩阵 B 的元素）： α1(i) = πibi(o1) 在第 t 时刻，隐藏状态变成了 qj（这里的 qj 不是具体状态，是任意状态都可以），t + 1 时刻隐藏状态变成了为 qi，此时，隐藏状态由 qj 变成 qi 的概率可由矩阵 A 得到，值为 aji，而 qj 可以是任何一种状态，我们都得考虑进去，所以我们得遍历一遍所有隐藏状态，然后相加，即 ∑jαt(j)，所以有： αt + 1(i) = [∑jαt(j)aij]bi(ot + 1) 如果这个式子看上去还是太麻烦，我们可以拆开来看：αt(j) 表示的是前一时刻隐藏状态为 qj 的概率，aij 表示由隐藏状态 qj 转换为 qi 的概率，相乘就是前一时刻的隐藏状态恰好为 qj，并且由 qj 能转换到 qi 的概率，由于得考虑全部 qj 的情况，所以得遍历求和；后面的 bi(ot + 1) 则是由隐藏状态 qi 转换到观测状态 ot + 1 的概率，将它与前一部分相乘，就得到了前一时刻的隐藏状态恰好为 qj，并且由 qj 能转换到 qi，又由 qi 得到 ot + 1 的概率。 则最终结果就是： P(O|λ) = ∑iαi(T) 计算一下前向算法的时间复杂度：一共要计算 T 次 α，每次计算 α 的时间复杂度为 N2 （原因很简单，自己想），所以前向算法的时间复杂度为 O(TN2)。显然，前向算法将暴力算法的时间复杂度从指数级降到了线性级别，极大提升了执行效率。 4.2 公式推导 由上述过程，我们可以得到前向算法的递推式： 初值： α1(i) = πibi(o1)，i = 1, 2, ..., N 递推： $$ \\alpha_{t+1}(i)=[\\sum_{j=1}^N\\alpha_t(j)a_{ji}]b_i(o_{t+1})，i=1,2,...,N $$ 终止： $$ P(O|\\lambda)=\\sum_{i=1}^N\\alpha_T(i) $$ 接下来，我们对每一个公式进行推导。 首先，我们要求解的目标是 P(O|λ) = ∑IP(I,O|λ)，而 αT(i) = P(O,ST=qi|λ)，所以对于终止公式有： $$ \\begin{aligned} P(O|\\lambda)&amp;=\\sum_{I}P(I,O|\\lambda)\\\\ &amp;=\\sum_{i=1}^NP(o_1,o_2,...,o_T,S_T=q_i|\\lambda)\\\\ &amp;=\\sum_{i=1}^N\\alpha_T(i) \\end{aligned} $$ 对于递推公式则有： $$ \\begin{aligned} \\alpha_{t+1}(i)&amp;=P(o_1,o_2,...,o_{t+1},S_{t+1}=q_i|\\lambda)\\\\ &amp;=P(S_{t+1}=q_i|\\lambda)P(o_1,o_2,...,o_{t+1}|S_{t+1}=q_i,\\lambda)\\\\ &amp;=[\\sum_{j=1}^NP(S_{t+1}=q_i,S_t=q_j|\\lambda)]P(o_1,o_2,...,o_{t+1}|S_{t+1}=q_i,\\lambda)\\\\ &amp;=[\\sum_{j=1}^NP(S_t=q_j|\\lambda)P(S_{t+1}=q_i|S_t=q_j,\\lambda)]P(o_1,o_2,...,o_{t+1}|S_{t+1}=q_i,\\lambda)\\\\ &amp;=[\\sum_{j=1}^NP(o_1,...,o_t,S_t=q_j|\\lambda)P(S_{t+1}=q_i|S_t=q_j,\\lambda)]P(o_1,o_2,...,o_{t+1}|S_{t+1}=q_i,\\lambda)\\\\ &amp;=[\\sum_{j=1}^N\\alpha_t(j)a_{ji}]b_i(o_{t+1}) \\end{aligned} $$ 对于初值有： $$ \\begin{aligned} \\alpha_1(i)&amp;=P(o_1,S_1=q_i|\\lambda)\\\\ &amp;=P(S_1=q_i|\\lambda)P(o_1|S_1=q_i,\\lambda)\\\\ &amp;=\\pi_1b_i(o_1) \\end{aligned} $$ 05 后向算法 后向算法比前向算法稍微复杂一点，这一节着重讲解后向算法初值、递推和终止公式的推导。 给定隐马尔可夫模型，定义在时刻 t 状态为 qi 的条件下，从 t + 1 到 T 的部分观测序列为 ot + 1, ot + 2, ..., oT 的概率称为后向概率，记作： βt(i) = P(ot + 1,ot + 2,...,oT|St=qi,λ) 观察后向概率的公式和定义，我们可以用另一种方法描述后向概率：当前时刻为 T，也就是终止时刻，前 T − t 个时刻的观测序列为 ot + 1, ot + 2, ..., oT，且 t 时刻隐藏状态恰好为 qi 的概率。可以发现，后向概率是以终止时刻为起点，倒退回去考虑的，与前向概率正好相反，所以递推的起点是 βT(i) = P(∅|ST=qi,λ)，可以发现，当 t = T 时，已不存在后续观测序列，所以我们规定 βT(i) = 1。 我们要求解的是 P(O|λ) = P(o1,o2,...oT|λ)，后向算法递推的终点是序列的起始点，也就是 t = 1，而 β1(i) = P(o2,o3,...,oT|St=qi,λ)，这之间又要怎么转换？这就是后向算法比前向算法复杂的点，它并不像前向算法那样容易推导。首先，我们使用全概率公式和条件概率公式对 P(O|λ) 进行变换： $$ \\begin{aligned} P(O|\\lambda)&amp;=\\sum_{i=1}^{N}P(o_1,o_2,...,o_T,S_1=q_i|\\lambda)\\\\ &amp;=\\sum_{i=1}^{N}P(o_1,o_2,...,o_T|S_1=q_i,\\lambda)P(S_1=q_i|\\lambda)\\\\ &amp;=\\sum_{i=1}^{N}P(o_1|o_2,...,o_T,S_1=q_i,\\lambda)P(o_2,...,o_T|S_1=q_i,\\lambda)\\pi_i\\\\ &amp;=\\sum_{i=1}^Nb_i(o_1)\\beta_1(i)\\pi_i \\end{aligned} $$ 经过上面的推导，我们就能发现 P(O|λ) 和 β1(i) 的联系。下一个要解决的就是 βt(i) 的推导了，首先令 βt + 1(j) = P(ot + 2,ot + 3,...oT|St + 1=qj,λ)，其推导过程如下： $$ \\begin{aligned} \\beta_t(i)&amp;=P(o_{t+1},o_{t+2},...,o_T|S_t=q_i,\\lambda)\\\\ &amp;=\\sum_{j=1}^{N}P(o_{t+1},o_{t+2},...,o_T,S_{t+1}=q_j|S_t=q_i,\\lambda)\\\\ &amp;=\\sum_{j=1}^{N}P(o_{t+1},...,o_T|S_t=q_i,S_{t+1}=q_j,\\lambda)P(S_{t+1}=q_j|S_t=q_i,\\lambda)\\\\ &amp;=\\sum_{j=1}^{N}P(o_{t+1},...,o_T|S_{t+1}=q_j,\\lambda)P(S_{t+1}=q_j|S_t=q_i,\\lambda)\\\\ &amp;=\\sum_{j=1}^{N}P(o_{t+1}|o_{t+2},...,o_T,S_{t+1}=q_j,\\lambda)P(o_{t+2},...,o_T|S_{t+1}=q_j,\\lambda)P(S_{t+1}=q_j|S_t=q_i,\\lambda)\\\\ &amp;=\\sum_{j=1}^N\\beta_{t+1}(j)b_j(o_{t+1})a_{ij} \\end{aligned} $$ 至此，我们就得到后向算法中的初值、递推和终止公式： 初值： βT(i) = 1 递推： $$ \\beta_t(i)=\\sum_{j=1}^N\\beta_{t+1}(j)b_j(o_{t+1})a_{ij} $$ 终止： $$ P(O|\\lambda)=\\sum_{i=1}^Nb_i(o_1)\\beta_1(i)\\pi_i $$ 06 Baum-Welch 算法 讨论完了如何求解 P(O|λ)，下一步我们就要考虑最难的一个问题——如何求解 HMM 的参数，即 A，B，Π。 如果是不加任何限制地考虑这个问题，那其实是很简单的。根据大数定理：在试验次数足够多的情况下，频数就等于概率。要想得到 A 和 B，只需要对数据进行统计，计算每种状态出现的频数就行了，于是就有： $$ \\begin{aligned} &amp;\\hat{a}_{ij}=\\frac{A_{ij}}{\\sum_{j=1}^{N}A_{ij}}&amp;,i=1,2,...,N,j=1,2,...,N\\\\ &amp;\\hat{b}_{j}(k)=\\frac{B_{jk}}{\\sum_{k=1}^{M}B_{jk}}&amp;,j=1,2,...,N,k=1,2,...,M\\\\ \\end{aligned} $$ 解释一下取值范围：A 是状态转移概率矩阵，这是隐藏状态和隐藏状态之间转移的概率，所以 i 和 j 的最大值都是隐藏状态的数量 N；而 B 是生成观测状态概率矩阵，这是隐藏状态到观测状态之间转移的概率，令 j 代表隐藏状态，其最大值就是隐藏状态的数量 N，k 代表观测状态，其最大值就是观测状态的数量 M，我们之前讲到过，HMM 中的隐藏状态和观测状态数量不一定要相同，所以 N 不一定等于 M。 至于 Π 也很简单，根据往期数据计算就行了。所以如果只是像这样单纯地求解 HMM 的参数，只要有数据，那就几乎是没有难度的。但我们来考虑一下求解目标中的第二个：给定观测序列 O = {o1, o2, ..., ot}，求解参数 (A,B,Π) 使得 P(O|λ) 最大。这要怎么做？ 之前的讨论是在所有数据均有的情况下进行的，也就是隐藏状态序列 I 和观测状态序列 O 均已知的情况下，但现在只有观测序列 O，要我们求最优的参数 (A,B,Π)，使 P(O|λ) 最大。也就是说 I 被隐藏了，这相当于是一个含隐变量的参数估计问题，需要 EM 算法来解决。 EM 算法应用到 HMM 中时通常被称为 Baum-Welch 算法，Baum-Welch 算法是 EM 算法的一个特例。 归结原理 01 归结推理 反证法：P ⇒ Q，当且仅当 P ∧ ¬Q ⇔ F，即 Q 为 P 的逻辑结论，当且仅当 P ∧ ¬Q 是不可满足的。 定理：Q 为 P1，P2，……，Pn 的逻辑结论，当且仅当 (P1∧P2∧…∧Pn) ∧ ¬Q 是不可满足的。 归结推理就是基于上面两条定理，将原命题转换成反命题，然后证明其反命题是不可满足的，即可得证原命题是真命题。归结推理的整体思路是： 欲证明 P ⇒ Q 化为反命题 P ∧ ¬Q 化成子句集 证明子句集不可满足(鲁滨逊归结原理) 02 子句集 什么是子句？如何将谓词公式化为子句集？ 我们称一个不能再分割的命题为原子谓词公式，将原子谓词公式及其否定形式称为文字，而子句就是任何文字的析取式，任何文字本身也是子句。空子句是一个不包含任何文字的子句，它永远为假，不可满足，通常表示为 NIL，虽然听上去没什么用，但它却是归结推理中最重要的子句，之后你会知道为什么。以上就是子句的概念，而子句集就是由子句构成的集合。 以下面这道题为例讲解如何将一个谓词公式化为子句集： (∀x)((∀y)P(x,y)→¬(∀y)(Q(x,y)→R(x,y))) 第一步：消去谓词公式中的 → 和 ↔︎，得到： (∀x)(¬(∀y)P(x,y)∨¬(∀y)(¬Q(x,y)∨R(x,y))) 第二步：将否定符号 ¬ 移到紧靠谓词的位置上： (∀x)((∃y)¬P(x,y)∨(∃y)(Q(x,y)∧¬R(x,y))) 第三步：变量标准化，将重复的变量名换掉： (∀x)((∃y)¬P(x,y)∨(∃z)(Q(x,z)∧¬R(x,z))) 第四步：消去存在量词，要用到 Skolem 函数，令 y = f(x)，z = g(x)： (∀x)(¬P(x,f(x))∨(Q(x,g(x))∧¬R(x,g(x)))) 第五步：化为前束形，即将所有的全称谓词提到公式最前面，使母式中不存在任何量词，上式已满足前束形。 第六步：化为 Skolem 标准形，即将母式化为合取式： (∀x)((¬P(x,f(x))∨Q(x,g(x))∧(¬P(x,f(x))∨¬R(x,g(x)))) 第七步：略去全称量词： (¬P(x,f(x)) ∨ Q(x,g(x)) ∧ (¬P(x,f(x))∨¬R(x,g(x))) 第八步：把和取词看作分隔符，把整体化为集合： {¬P(x,f(x)) ∨ Q(x,g(x)), ¬P(x,f(x)) ∨ ¬R(x,g(x))} 第九步：子句变量标准化，即将不同的子句中的变量名字区分开，用不同的符号表示： {¬P(x,f(x)) ∨ Q(x,g(x)), ¬P(y,f(y)) ∨ ¬R(y,g(y))} 以上，就得到了谓词公式的子句集。 03 鲁滨逊归结原理 子句集中的各子句是合取关系，所以只要有一个不可满足，则整个子句集不可满足。所以，我们需要去找一个空子句，假如子句集中存在空子句，那就肯定不可满足。但是子句集中直接出现空子句的情况是很少的，那么如何找到空子句？这就是鲁滨逊归结原理要解决的问题，根据鲁滨逊归结原理对子句集进行归结，如果最终归结出一个空子句，则说明该子句集不可满足，进一步说明原命题不可满足。 鲁滨逊归结原理（也称消解原理）的基本思路是：检查子句集 S 中是否包含空子句，若包含，则 S 不可满足；若不包含，在 S 中选择合适的子句进行归结，一旦归结出空子句，就说明 S 是不可满足的。 归结的定义：设 C1 和 C2 是子句集中的任意两个子句，如果 C1 中的文字 L1 与 C2 中的文字 L2 互补，那么从 C1 和 C2 中分别消去 L1 和 L2，并将两个子句中余下的部分析取，构成一个新子句 C12。 例题：设 C1 = ¬P ∨ Q，C2 = ¬Q ∨ R，C3 = P，请对 {C1, C2, C3} 进行归结。 C1 和 C2 中存在互补子句 Q 和 ¬Q，所以消去这两个子句集，并将余下子句析取，得到 C12 = ¬P ∨ R；C12 和 C3 中存在互补子句 ¬P 和 P，所以消去这两个子句集，并将余下子句析取，得到 C123 = R。所以 C123 就是该子句集归结的结果。 定理：归结式 C12 是其亲本子句 C1 和 C2 的逻辑结论，即，如果 C1 和 C2 为真，则 C12 也为真。 上述定理有一条推论：设 C1 和 C2 是子句集 $\\text S$ 中的两个子句集，C12 是它们的归结式，若用 C12 代替 C1 和 C2 后得到新子句集 S1，则由 S1 不可满足性可推出 S 的不可满足性。但是注意，这条推论不能证明若 S 是不可满足的，则 S1 也不可满足，所以还有另一条推论：设 C1 和 C2 是子句集 $\\text S$ 中的两个子句集，C12 是它们的归结式，若 C12 加入原子句集 S，得到新子句集 S2，则 $\\text S$ 和 $\\text S_2$ 在不可满足性上是等价的，即若 S 是不可满足的，则 S2 也不可满足，反之亦然。不过我们的目的只是为了证明原子句集不可满足，也就是归结出一个空子句，所以上述两个推论均可用。 04 归结反演 应用鲁滨逊归结原理证明定理的过程称为归结反演。它总共分为以下四个步骤： 将已知前提表示为谓词公式 F； 将待证明的结论表示为谓词公式 Q，并否定得到 ¬Q； 把谓词公式集 {F, ¬Q} 化为子句集 $\\text S$； 应用归结原理对子句集 $\\text S$ 中的子句进行归结，并把每次归结得到的归结式都并入到 $\\text S$ 中。如此反复进行，若出现了空子句，则停止归结，此时就证明了 Q 为真。 例题：某公司招聘工作人员，A，B，C 三人面试，经面试后公司表示如下想法： 三人中至少录取一人； 如果录取 A 而不录取 B，则一定录取 C； 如果录取 B，则一定录取 C。 求证：公司一定录取 C。 解：第一步，将已知前提表示为谓词公式，先定义谓词：设 P(x) 表示录取 x。于是可得如下前提： P(A) ∨ P(B) ∨ P(C) P(A) ∧ ¬P(B) → P(C) P(B) → P(C) 第二步，将待证明的结论表示为谓词公式，并将其否定：¬P(C)。 第三步，将上述谓词公式化为子句集： P(A) ∨ P(B) ∨ P(C) ¬P(A) ∨ P(B) ∨ P(C) ¬P(B) ∨ P(C) ¬P(C) 第四步，应用归结原理进行归结： P(B) ∨ P(C) 归结(1)和(2) P(C) 归结(3)和(5) NIL 归结(4)和(6) 由于归结出了空子句，所以成功证明了 ¬P(C) 为假，因此原命题 P(C) 为真，公司一定录取 C。 例题：已知： 任何人的兄弟不是女性； 任何人的姐妹必是女性； Mary 是 Bill 的姐妹。 求证：Mary 不是 Tom 的兄弟。 解：第一步，将已知前提表示为谓词公式，先定义谓词：设 brother(x,y) 表示录取 x 是 y 的兄弟，设 sister(x,y) 表示录取 x 是 y 的姐妹，设 woman(x) 表示录取 x 是女性。于是可得如下前提： (∀x)(∀y)(brother(x,y)→¬woman(x)) (∀x)(∀y)(sister(x,y)→woman(x)) sister(Mary,Bill) 第二步，将待证明的结论表示为谓词公式，并将其否定：brother(Mary,Tom)。 第三步，将上述谓词公式化为子句集： C1 = ¬brother(x,y) ∨ ¬woman(x) C2 = ¬sister(x,y) ∨ woman(x) C3 = sister(Mary,Bill) C4 = brother(Mary,Tom) 第四步，应用归结原理进行归结： C23 = woman(Mary) C123 = ¬borther(Mary,y) C1234 = NIL 由于归结出了空子句，所以成功证明了 brother(Mary,Tom) 为假，因此原命题 ¬brother(Mary,Tom) 为真，Mary 不是 Tom 的兄弟。 A* 搜索 01 启发式搜索 能有助于简化搜索过程的信息称为启发信息，利用启发信息的搜索过程称为启发式搜索。 求解问题中能利用的大多是非完备的启发信息，所谓非完备就是指，信息也许对搜索有正面影响的，但是我们无法得知它是否总能提供正面影响，不知道它是否会造成负面影响。就例如极值点导数为 0，这是一条完备的信息，因为它可被证明总是成立。造成这种结果的原因如下： 求解问题系统不可能知道与实际问题有关的全部信息，因而无法知道该问题的全部状态空间，也不可能用一套算法来求解所以问题； 有些问题在理论上虽然存在着求解算法，但是在工程实践中，这些算法不是效率太低，就是根本无法实现(就比如宽度优先搜索，它总能找到最优解，但是无法实现)。 启发式搜索在搜索过程中根据启发信息评估各个节点的重要性，优先搜索重要的节点。估价函数的任务就是估计待搜索节点“有希望”的程度。估价函数 fn 表示从初始节点经过 n 节点到达目的节点的路径的最小代价估计值，其一般形式为： f(n) = g(n) + h(n) 其中，g(n) 是从初始节点到结点 n 的实际代价，h(n) 是从节点 n 到目的节点的最佳路径的估计代价。一般地，在 f(n) 中，g(n) 的比重越大，越倾向于宽度优先搜索方式，而 h(n) 的比重越大，表示启发性能更强。如果 h(n) 的比重降为 0，则搜索过程将变为盲目搜索，因为不再考虑启发信息。 估价函数的设计方法有很多种，并且不同的估价函数对问题有不同的影响。以八数码问题为例，最简单的估价函数可以取一格局与目的格局相比，其位置不同的数码数目；这种估价函数是最简单实现的，但是效果未必好，一种比较好的估价函数的设计是取各数码移到目的位置所需移动的距离的总和，这是最理想的，但是不可能实现；还可以将每一对逆转数码1乘以一个倍数 3；但是这种做法的局限性太大，所以还可以在此基础上再加上位置不符的数码的个数。 02 A 搜索算法 启发式图搜索法的基本特点：寻找并设计一个与问题有关的 h(n) 以构造 f(n) = g(n) + h(n)，然后以 f(n) 的大小来排列待扩展状态的次序，每次选择 f(n) 值最小者进行扩展。这也就是 A 搜索算法的执行流程。 利用 A 搜索算法求解八数码问题，估价函数定义为： f(n) = d(n) + w(n) 其中，d(n) 代表状态的深度，每步为单位代价；w(n) 以与目标格局不符的数码数量作为启发信息的度量。例如： 初始格局处于第 0 层，因此 d(S) = 0，其中，* 代表空格，计算 w(n) 的时候，既可以算上 *，也可以不算，反正不影响节点扩展，如果不算入，那么 w(S) = 4 (算入的话结果为 5)。由初始格局可得到 3 种状态，即分别把空格往上、左、右移动，上图中只展示了 3 种情况。其中，A 格局的 d(A) = 1，w(A) = 5，所以 f(A) = 6；B 格局的 d(B) = 1，w(B) = 3，所以 f(B) = 4；C 格局的 d(C) = 1，w(C) = 5，所以 f(C) = 6。根据 A 搜索算法的原则，估价函数值最小的是 B 格局，因此应该扩展该节点，接下来的过程也是一样。最终经过 5 次搜索，也就是扩展 5 层节点，最终能到达目标格局，该过程的完整搜索树在课本 P143 页，这里就不再展开了。 那么 A 搜索算法能否保证找到最优解？其实仔细想想就知道，我们构造的估价函数中，h(n) 是对待扩展节点到目标节点的代价估计，不一定准确，所以 A 搜索算法不一定能保证找到最优解。 如果有代价一样的结点怎么办：可以随机。 03 A* 搜索算法 A* 搜索算法是对 A 搜索算法的改进。我们说 A 搜索算法无法保证找到最优解，而 A* 搜索算法则保证了一定能搜索到解，并且一定能搜索到最优解。A* 算法给出了 A 算法能得到最优解的条件，我们令 h*(n) 为状态 n 到目标状态的实际最小代价，h(n) 是我们定义的估计代价，则当 ∀n，h(n) ≤ h*(n) 时，我们就称该搜索算法为 A* 搜索算法。 这就好比是有人托我们帮他买衣服，这种品牌的衣服的最低价格是 1000 元(相当于 h*)，如果他希望以不高于 1000 (相当于 h)的价格买到这件衣服(此时 h ≤ h*)，那我们就要搜索很多家店(前提是这家店要存在)，这种情况下，虽然搜索的店较多，但是我们必然能找到一家店能满足要求；但是倘若他希望以 1500 元以内(相当于 h)的价格买到这件衣服(此时 h &gt; h*)，那搜索的范围就大大减少了，因为最低价是 1000，那么价格高于 1000 的店找起来肯定没那么费力气，但缺点就是找到的店铺未必是最便宜的。 在上面我们讨论的八数码问题中，我们选取一格局与目标格局不符的数码数量作为启发信息的度量 w(n)，而八数码问题中的 h*(n) 应该是各数码移到目的位置所需移动的距离的总和，显然 w(n) ≤ h*(n)，满足了 h(n) ≤ h*(n) 的条件，所以也算一种 A* 搜索算法。 逆转数码的概念涉及到逆序数。例如 1 4 2 3，在这个序列中，它并未按照从小到大的顺序排序，2 和 3 比 4 小，但是却排在 4 后面，所以该序列的逆序数就是 2. 逆序数可以用来判断一个八数码问题是否有解，至于原因就不在此赘述了。↩︎","categories":[{"name":"计算机专业基础","slug":"计算机专业基础","permalink":"https://example.com/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E4%B8%93%E4%B8%9A%E5%9F%BA%E7%A1%80/"}],"tags":[{"name":"machine learning","slug":"machine-learning","permalink":"https://example.com/tags/machine-learning/"},{"name":"hmm","slug":"hmm","permalink":"https://example.com/tags/hmm/"},{"name":"a-star","slug":"a-star","permalink":"https://example.com/tags/a-star/"},{"name":"deduction","slug":"deduction","permalink":"https://example.com/tags/deduction/"},{"name":"svm","slug":"svm","permalink":"https://example.com/tags/svm/"},{"name":"heuristic search","slug":"heuristic-search","permalink":"https://example.com/tags/heuristic-search/"},{"name":"ai","slug":"ai","permalink":"https://example.com/tags/ai/"}]},{"title":"计算机图形学","slug":"计算机图形学","date":"2023-12-10T03:38:36.000Z","updated":"2025-04-09T11:50:27.892Z","comments":true,"path":"2023/12/10/计算机图形学/","permalink":"https://example.com/2023/12/10/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9B%BE%E5%BD%A2%E5%AD%A6/","excerpt":"01 画线算法 本节主要掌握 DDA 算法和 Bresenham 算法。DDA 算法主要是由笛卡尔斜率截距方程导出的，而 Bresenham 算法则算是对 DDA 算法的改进。 笛卡尔斜率截距方程为：y = mx + b，笛卡尔斜率截距方程告诉我们，如果已知一条直线上的两个端点 (x0,y0) 和 (xend,yend)，我们就能确定该直线的斜率和截距： $$ \\begin{aligned} &amp;m=\\frac{x_{end}-x_{0}}{y_{end}-y_{0}}\\\\ &amp;b=y_{0}-m·x_{0} \\end{aligned} $$ 如果令直线在 x 轴上让线段延长一段距离 δx，那么对应的，y 轴上就应该延长 δy = m · δx；反过来，如果令直线在 y 轴上延长一段距离 δy，那么在 x 轴上就要延长 $\\delta x=\\frac{\\delta y}{m}$。这两个公式被称为增量公式。","text":"01 画线算法 本节主要掌握 DDA 算法和 Bresenham 算法。DDA 算法主要是由笛卡尔斜率截距方程导出的，而 Bresenham 算法则算是对 DDA 算法的改进。 笛卡尔斜率截距方程为：y = mx + b，笛卡尔斜率截距方程告诉我们，如果已知一条直线上的两个端点 (x0,y0) 和 (xend,yend)，我们就能确定该直线的斜率和截距： $$ \\begin{aligned} &amp;m=\\frac{x_{end}-x_{0}}{y_{end}-y_{0}}\\\\ &amp;b=y_{0}-m·x_{0} \\end{aligned} $$ 如果令直线在 x 轴上让线段延长一段距离 δx，那么对应的，y 轴上就应该延长 δy = m · δx；反过来，如果令直线在 y 轴上延长一段距离 δy，那么在 x 轴上就要延长 $\\delta x=\\frac{\\delta y}{m}$。这两个公式被称为增量公式。 1.1 DDA 算法 计算机是使用一系列离散的像素点来近似非水平或垂直的直线和其他图案的，要画的直线在大部分向前看不可能刚刚好穿过一系列在水平或垂直方向上连续的像素点，所以画线算法的重点就在于确定所画线段要经过的像素点的横纵坐标。DDA 算法的思想是：通过在 x 轴或 y 轴上设置一个微小的步距 step，然后通过上述的增量公式，来得到另一个轴上对应的坐标。 倘若我们选择在 x 轴上设置步距，也就是 x 轴上的更新公式是 xk + 1 = xk + stepx，那么对应 y 轴上的更新公式应该为 yk + 1 = yk + m · stepx；倘若我们选择在 y 轴上设置步距，也就是 y 轴上的更新公式是 yk + 1 = yk + stepy，那么对应 x 轴上的更新公式应该为 $x_{k+1}=x_{k}+\\frac{step_{y}}{m}$. 那么具体应该选择 x 轴还是 y 轴？按照 DDA 算法的规定，当 |m| &lt; 1 时，应该选取 x 轴；而当 |m| &gt; 1 时，应该选取 y 轴。这是因为，当 |m| &lt; 1 时，y 上移动 setpy，x 上要移动的距离 $step_{x}=\\frac{step_{y}}{m}&gt;step_{y}$，如果此时选择 y 轴作为移动的主轴，会导致 x 轴上移动的步距过大，精度下降，所以应该 选择 x 轴作为移动的主轴，反之亦然。 以上就是 DDA 算法的思想，那么所谓的步距 step 具体应该取何值？显然，step 应该取主轴上的像素点的单位间隔，课本上直接取了 1. 如果终止点的位置落在起始点的右边，也就是移动的位置应该相反，则此时的步距应该为负数，也就是 − 1. 另外，由于 m 可能为浮点数，使得计算出的 yk + 1 和 xk + 1 也为浮点数，应该对最终结果予以四舍五入。 总结：DDA 算法的执行步骤是 根据 m 的大小选择移动轴 移动轴每次移动一步 ( + 1)，并将移动轴的坐标带入直线方程求另一条轴的坐标 对求得的坐标进行四舍五入，得到完整坐标 重复步骤 2 和 3，直到到达终止点为止 DDA 算法的优缺点： 优点：消除了算法中的乘法，简单易懂 缺点：有浮点数的计算，并伴随浮点数相加累积误差，对长线段而言容易引起像素点位置与理想位置的较大偏移；四舍五入操作消耗时间 1.2 Bresenham 算法 Bresenham 算法的思想很简单，它直接选取与目标直线垂直距离最近的像素点，例如下图中的两点，很明显应该选择 (xk+1,yk+1)： 但 Bresenham 的数学原理是有点复杂的，因为它使用递推的手段去求解决策参数，避免了大量的浮点数运算和四舍五入的操作。接下来是数学公式的推导，可以不看。 假设我们要画一条线段，起始点为 (x0,y0)，终止点为 (x1,y1)，令 Δx = x1 − x0，Δy = y1 − y0，m ∈ (0,1) 为该线段的斜率且有 $m=\\frac{\\Delta y}{\\Delta x}$，b 为截距，假设现在已经求得了第 k 个点为 (xk,yk)，现在讨论在 xk + 1 处描点的情况。 注意：这里非常容易弄混 xk、xk + 1、xk + 1 和 yk、yk + 1、yk + 1 之间的关系，(xk,yk) 是上一个已知的点，现在要求的是在 xk + 1 处的点，而 xk + 1 = xk + 1，yk + 1 ∈ {yk, yk + 1}。 将 xk + 1 带入直线方程可得该点处 y 轴的精确值为 y = m(xk+1) + b，设 dupper 和 dlower 分别为 yk 和 yk + 1 与 y 的差值，也就是可选的两个点与精确位置的垂直距离，则有： $$ \\begin{aligned} &amp;d_{upper}=y_{k}+1-y\\\\ &amp;d_{lower}=y-y_{k} \\end{aligned} $$ 将 y = m(xk+1) + b 带入就有： $$ \\begin{aligned} &amp;d_{upper}=y_{k}+1-m(x_{k}+1)-b\\\\ &amp;d_{lower}=m(x_{k}+1)+b-y_{k} \\end{aligned} $$ 根据 Bresenham 算法的思想，应该将上面两个值相减求差，根据差的正负号来判断哪个点离线段更近，则有： $$ \\begin{aligned} \\Delta d=d_{lower}-d_{upper}&amp;=m(x_{k}+1)+b-y_{k}-y_{k}-1+m(x_{k}+1)+b\\\\ &amp;=2m(x_{k}+1)-2y_{k}+2b-1\\\\ \\end{aligned} $$ 接下来定义一个决策参数 pk，其值为： $$ \\begin{aligned} p_{k}&amp;=\\Delta x·\\Delta d\\\\ &amp;=\\Delta x(2m(x_{k}+1)-2y_{k}+2b-1)\\\\ &amp;\\Downarrow m=\\frac{\\Delta y}{\\Delta x}\\\\ &amp;=2\\Delta y(x_{k}+1)-2\\Delta x·y_{k}+\\Delta x(2b-1)\\\\ &amp;=2\\Delta y·x_{k}-2\\Delta x·y_{k}+2\\Delta y+\\Delta x(2b-1)\\\\ &amp;=2\\Delta y·x_{k}-2\\Delta x·y_{k}+c \\end{aligned} $$ 由于 Δx &gt; 0，所以乘上它并不会改变 Δd 的正负性，最后 2Δy + Δx(2b−1) 这一项为常数项，在循环计算 pk 时可以消除，所以可以不管；乘上 Δx 是为了后续计算方便，简化最终的递推式形式。 根据 pk 的正负性可以得到： 1. pk &lt; 0 时，即 dlower &lt; dupper 时，应选择 yk 作为 yk + 1 2. pk &gt; 0 时，即 dlower &gt; dupper 时，应选择 yk + 1 作为 yk + 1 从 pk 的式子很容易得出 pk + 1 的情况： pk + 1 = 2Δy · xk + 1 − 2Δx · yk + 1 + c 将上面两式相减： $$ \\begin{aligned} p_{k+1}-p_{k}&amp;=2\\Delta y(x_{k+1}-x_{k})-2\\Delta x(y_{k+1}-y_{k})\\\\ &amp;\\Downarrow x_{k+1}=x_{k}+1\\\\ p_{k+1}&amp;=p_{k}+2\\Delta y-2\\Delta x(y_{k+1}-y_{k}) \\end{aligned} $$ 如果 pk &lt; 0，则 yk + 1 = yk，则 pk + 1 = pk + 2Δy 如果 pk &gt; 0，则 yk + 1 = yk + 1，则 pk + 1 = pk + 2Δy − 2Δx 至此，我们就得到了 pk 的递推式，根据 pk 值的正负，我们就能知道是选择 yk 还是 yk + 1. 现在，我们只需要知道 p0 就可以通过递推的方式直接计算之后的决策参数，p0 是在线段的起始点 (x0,y0) 处的情况，将该点和直线方程带入 pk 的定义式则有： $$ \\begin{aligned} p_{0}&amp;=2\\Delta y·x_{0}-2\\Delta x·y_{0}+2\\Delta y+\\Delta x(2b-1)\\\\ &amp;\\Downarrow y_{0}=mx_{0}+b,m=\\frac{\\Delta y}{\\Delta x}\\\\ &amp;=2\\Delta y·x_{0}-2\\Delta x·(\\frac{\\Delta y}{\\Delta x}x_{0}+b)+2\\Delta y+\\Delta x(2b-1)\\\\ &amp;=2\\Delta y-\\Delta x \\end{aligned} $$ 至此，Bresenham 算法的数学推导就结束了。最后演示一下 p0 → p3 的递推过程： 1. p0 = 2Δy − Δx，若 p0 &gt; 0，则 xk + 1 = xk + 1，yk + 1 = yk + 1；若 p0 ≤ 0，则 xk + 1 = xk + 1，yk + 1 = yk； 2. p1 = p0 + 2Δy − 2Δx(y1−y0)，若 p1 &gt; 0，则 xk + 1 = xk + 1，yk + 1 = yk + 1；若 p1 ≤ 0，则 xk + 1 = xk + 1，yk + 1 = yk； 3. p2 = p1 + 2Δy − 2Δx(y2−y1)，若 p2 &gt; 0，则 xk + 1 = xk + 1，yk + 1 = yk + 1；若 p2 ≤ 0，则 xk + 1 = xk + 1，yk + 1 = yk； 4. p3 = p2 + 2Δy − 2Δx(y3−y2)，若 p3 &gt; 0，则 xk + 1 = xk + 1，yk + 1 = yk + 1；若 p3 ≤ 0，则 xk + 1 = xk + 1，yk + 1 = yk； 5. … 》参考：【简书】Bresenham 画线算法推导、【CSDN】DDA算法和Bresenham算法 1.3 作业 绘制一条直线，端点分别为 (1,1) 和 (7,5)，使用 DDA 和 Bresenham 算法绘制。 先计算斜率和截距：$m=\\frac{5-1}{7-1}=\\frac{2}{3}$，$b=1-\\frac{2}{3}·1=\\frac{1}{3}$，得到直线方程：$y=\\frac{2}{3}x+\\frac{1}{3}$。 DDA 算法 由于 |m| &lt; 1，以单位间隔 δx = 1 在 x 轴上取样，算法执行过程如下： x $\\frac{2}{3}x+\\frac{1}{3}$ (x,y) 1 1 (1,1) 2 $\\frac{5}{3}$ (2,2) 3 $\\frac{7}{3}$ (3,2) 4 3 (4,3) 5 $\\frac{11}{3}$ (5,4) 6 $\\frac{13}{3}$ (6,4) 7 5 (7,5) Bresenham 算法 首先计算 Δx = 7 − 1 = 6，Δy = 5 − 1 = 4，算法执行过程如下： k pk (xk + 1,yk + 1) 0 2 (2,2) 1 − 2 (3,2) 2 6 (4,3) 3 2 (5,4) 4 − 2 (6,4) 5 6 (7,5) 02 填充算法 填充算法中，我们主要学习扫描线算法。 如何在计算机程序中存储几何图形 (多边形)？最容易的方法就是按顺序存储多边形的顶点，这个多边形就唯一确定了。那显示器是如何显示几何图形的呢？显示设备通常提供一个帧缓冲存储器 (俗称显存)，可以把它当做二维数组，该数组存储的值与屏幕上显示的每一像素的颜色一一对应。那么问题来了，如何把程序中的几何图形转换成显存中的几何图形？这就是扫描线算法要解决的问题。 2.1 算法目标 扫描线多边形区域填充算法是按扫描线顺序 (由下到上)，计算扫描线与多边形的相交区间，再用要求的颜色显示这些区间的像素，即完成填充工作。扫描线在对多边形进行扫描时，会与多边形产生多个交点，如下图所示： 直接观察可以看出，在上图所示的扫描线中，x 轴坐标在 [10,14] 和 [18,24] 上的像素点位于多边形内部，在填充多边形时，只需要将这些点渲染成对应的颜色即可。但我们也看到，(14,18) 上的点并不在多边形内，所以它们不是要填充的部分，扫描线算法的核心就在于区分出哪些像素点位于多边形内，哪些位于多边形外。然而，这件事听起来简单，做起来却不容易。 上图展示的情形是最简单的一种，通过总结规律，我们可以发现，在扫描线不经过多边形的端点的情况下，其与多边形的交点数量永远是偶数，且在多边形区域内的线段与在多边形区域外的线段交替出现，利用这一性质，我们可以很容易地完成填充。但是，当扫描线刚刚好经过多边形的端点的时候，情况就变复杂了，如下图： 上图画了两根扫描线 y 和 y′，它们的共同点是都经过了多边形的端点，不同点是扫描线 y′ 已经不满足上面所说的“在多边形区域内的线段与在多边形区域外的线段交替出现”这一性质了，而 y 则仍然遵守，那么，我们如何区分这两种情况，以及如何设计一个完备的算法处理？这是扫描线算法要解决的第一个问题。 此外，算法还需要求解扫描线与多边形的交点，一个直观的解法是：根据多边形的顶点求出各条边的方程，然后将扫描线的纵坐标代入方程求出横坐标。但是显然，这样做会涉及大量的乘法和除法运算，并且需要遍历每条扫描线和每条边，性能开销极大，设计一个合理的算法求解每条扫描线与多边形的交点，是扫描线算法要解决的第二个问题。 2.2 算法原理 对于第一个问题，仔细观察上图可以发现两根扫描线在拓扑结构上的区别：y′ 上的端点所连接的两条边皆在 y′ 的上面，而 y 上的端点所连接的两条边分别位列 y 的两边。根据这一区别，我们可以设计这样一个算法区分这两种情况，并把它们转换为最好处理的情况：对端点两侧 (水平方向) 的两条边的斜率进行比较，假如两条边的斜率符号相反，则属于 y′ 的情况，反之则属于 y 的情况；当出现 y′ 的情况时，我们需要将该端点看作两个，这样就相当于 y′ 共穿过了 4 个交点，每个交点两两配对，就构成了 3 条线段，这 3 条线段满足“在多边形区域内的线段与在多边形区域外的线段交替出现”的性质，这样就巧妙地将情形转换成了一开始所说的最简单的情况；而对于后者，则是将端点看作是一个交点，这样也能满足该性质。 如何将端点看作是一个点？将端点看作是两个点其实是很好理解的，毕竟端点同时属于两条边，只要将一个端点看作是两条边上不同点就行了。但是如何把一个端点看作是一个点呢？一个可行的办法是，将端点所连接的一条边 (一般是在下面的那条) 在垂直方向上缩短 1 个单位，如下图所示： 上图中展示了两种情况：一个是上线段斜率大于 0，而下线段斜率小于 0；另一个则反一下。但不论是哪种，处理的方法都是：将下端点 y 轴坐标小的那条线段按照其梯度下降的方向缩短 1 个单位，即沿 y 轴缩短 1 个单位，这样端点就被拆分开来了，原本扫描线 y 经过一个端点，与两条边相交，现在只是与一条边相交，则不存在端点的问题了，但相应地，由于缩短了下线段的长度，下线段的上端点应当跑到扫描线 y − 1 上。 然后来解决第二个问题，如何快速求解扫描线与多边形的交点？请看下图： 图中展示了两条扫描线 yk 和 yk + 1，两条扫描线相邻，所以有 yk + 1 = yk + 1. 设 yk 与多边形的一个交点为 (xk,yk)，则在下一刻，也就是在 yk + 1 中，它将变动到坐标 (xk + 1,yk + 1) 上。在 画线算法 中我们已经讨论了增量公式，假设 (xk,yk) 和 (xk + 1,yk + 1) 所在的直线的斜率为 m，并且 $m=\\frac{y_{k+1}-y_k}{x_{k+1}-x_k}=\\frac{1}{x_{k+1}-x_k}$，由于扫描线 yk 和 yk + 1 的距离为 1，所以自然有 yk + 1 = yk + 1，根据增量公式，就有 $$ x_{k+1}=x_k+\\frac{1}{m} $$ 只要有上一条扫描线的交点的坐标和直线的斜率，就可以直接计算出下一条扫描线与该直线的交点。 2.3 算法的执行流程 为了实现算法，扫描线算法提出了两种数据结构 —— 边表 (Edge table，简称 ET) 和活动边表 (Active edge table，简称 AET)。边表实质上是一个邻接链表，其作用是记录扫描线所经过的端点所连接的两条边的信息，并且链表中的每个节点按照线段的下端点的 y 轴坐标进行归类 (即下端点的 y 轴坐标相同的放到同一层去)，下图是一个边表的示例： 其中，链表中的成员属性如下： 上端点 y 坐标 下端点 x 坐标 线段斜率的倒数 下一条边的地址 以上图中的扫描线 yA 为例，其经过端点 A，该端点连接了边 AE 和 AB，由于这两条边的下端点都是 A，所以应该将这两条边的信息保存到扫描线 yA 对应的下标位置；在扫描线 yc 中，它经过端点 C，由于 C 连接的两条边分别位于 yC 两边，所以应当将 CD 沿 y 轴缩短一个单位，这样就将 C 拆分成了两个点 —— C 和 C′. 由于 CB 和 C′D 的下端点的 y 轴坐标不同，只有 CB 的下端点 y 轴坐标等于 yC，所以不能将 C′D 的信息保存到 yC 中去，而应该保存到 yD 中。 而活动边表是一个链表，其中的节点所保存的信息如下 (注意比较和边表的区别)： 上端点 y 坐标 交点 x 坐标 线段斜率的倒数 下一条边的地址 它是一个随扫描线 y 轴变化而变化的同态表，而且与边表不同的是，边表只记录端点所在的边，活动边表记录的则是扫描线经过的所有边，并且按照交点的 x 坐标大小从小到大排序。那么活动边表是如何得到每个交点的？方法在之前已经介绍过了，初始的活动边表 (空表不计) 中只有端点所连接的两条边的信息 (水平线例外)，例如上图中的 yA，所以在 yA 时刻，活动边表中的信息与边表中 yA 是一样的，而在 yA + 1 时刻，扫描线会与多边形产生两个交点，如何求这两个交点？首先，交点的 y 轴坐标是已知的，就是 yA + 1，唯一要求解的其实只有 x 轴的坐标，根据公式 $x_{k+1}=x_k+\\frac{1}{m}$，我们可以很轻松地从上一个交点的 x 轴坐标直接计算出当前交点的 x 轴坐标，所以按照此方法更新交点的 x 轴坐标；并且，由于端点的信息在边表中保存了两次，但分别隶属于斜率不同的两条线段，所以原本重叠的端点在下一时刻就分开了，然后根据之前说的“在多边形区域内的线段与在多边形区域外的线段交替出现”的性质，对交点进行两两配对，我们就能得到扫描线 yA + 1 上处于多边形内部的线段范围。当扫描线扫描到 yD 时，又要向活动边表中加入新的端点 (2个)；而当扫描线扫描到 yE 时，有两个交点，即在 E 点重叠的点，即将被丢弃，因为在下一时刻，这个点就不再是交点了，判断的标准就是 $扫描线的y轴坐标\\ge上端点的y轴坐标$，只要这一点不再满足，就说明该交点就不再存在了。 限于文字描述，可能上述过程并不是很清楚，更加详细和具体的例子可以参考【知乎】扫描线填充算法详解这篇文章。 》参考：【知乎】扫描线填充算法详解、【简书】扫描线算法完全解析 2.4 作业 多边形顶点的坐标为 (7,1)、(2,3)、(2,9)、(7,7)、(13,11)、(13,5) 和 (7,1)，请写出该多边形的边表 ET，以及活动边表 AET 中内容的变化。 首先画出该多边形： 先求出各条边的斜率及其倒数： 边 m 1/m AB -2/5 -5/2 BC ∞ 0 CD -2/5 -5/2 DE 2/3 3/2 EF ∞ 0 FA 2/3 3/2 然后开始构造边表，要注意的是，图中的点 B 和点 F 属于特殊端点，需要将其进行拆分，也就是要变成下面这样： 接下来构造边表： 接下来开始构造活动边表，我们从 y1 开始： 我们将 y1 对应的边表中的内容加入到活动边表： 通过该表，我们就能渲染 y 轴坐标为 1，x 轴坐标在 [7,7] 之间的点。接下来我们来看 y2： 可以看到，y2 交分别交 AB′ 和 AF′ 于点 (4.5,2) 和 (8.5,2)，活动边表应及时更新这一信息，也就是更新交点的 x 轴坐标，即用原来的加上斜率的倒数： 通过该表，我们就能渲染 y 轴坐标为 1，x 轴坐标在 [4.5,8.5] 之间的点。接下来看 y3 的情况： 此时出现了新情况，那就是 y3 经过了一个新端点，将该端点从边表中取出，并按 x 轴的大小从小大到大排序插入活动边表，此外，原本的 B′ 由于已低于 y3，所以应当从活动边表中删除： 之后的步骤是一样的，直到扫描完最高的端点就结束了，故不再赘述，下面直接给出活动边表的变化过程： 03 二维变换 3.1 平移 通过将坐标轴加上位移量，可以实现一次平移。对二维图像的平移可以转换为对其端点的平移。设某点的坐标为 (x,y)，将其 x 轴坐标平移 tx 个单位，y 轴坐标平移 ty 个单位，得到新坐标 (x+tx,y+ty)，记为 (x′,y′)，其中，tx 和 ty 称为平移距离，(tx,ty) 称为平移向量，将它们转换为矩阵形式： $$ \\begin{aligned} P=\\begin{bmatrix} x \\\\ y \\end{bmatrix},P^\\prime=\\begin{bmatrix} x^\\prime \\\\ y^\\prime \\end{bmatrix},T=\\begin{bmatrix} t_x \\\\ t_y \\end{bmatrix} \\end{aligned} $$ 这样就可以用矩阵形式来表示二维平移方程： P′ = P + T 3.2 旋转 通过指定旋转轴和旋转角度，可以实现一次旋转。利用三角函数，我们可以得到一个点围绕某个轴旋转后的坐标，以下图为例： 该图中，长度为 r 的线段围绕原点逆时针旋转角度 θ，与 x 轴所成角度由 ϕ 变成 ϕ + θ，上端点 (x,y) 移动至 (x′,y′)，可以得到以下关系： $$ \\begin{aligned} &amp;x^\\prime=r\\cos(\\phi+\\theta)=r\\cos\\phi\\cos\\theta-r\\sin\\phi\\sin\\theta\\\\ &amp;y^\\prime=r\\sin(\\phi+\\theta)=r\\cos\\phi\\sin\\theta+r\\sin\\phi\\cos\\theta \\end{aligned} $$ 原本的坐标满足以下关系： $$ \\begin{aligned} &amp;x=r\\cos\\phi\\\\ &amp;y=r\\sin\\phi \\end{aligned} $$ 将其带入之前的式子就可以得到： $$ \\begin{aligned} &amp;x^\\prime=x\\cos\\theta-y\\sin\\theta\\\\ &amp;y^\\prime=x\\sin\\theta+y\\cos\\theta \\end{aligned} $$ 同样，如果我们把上面的信息转换为矩阵： $$ \\begin{aligned} P=\\begin{bmatrix} x \\\\ y \\end{bmatrix},P^\\prime=\\begin{bmatrix} x^\\prime \\\\ y^\\prime \\end{bmatrix}=\\begin{bmatrix} x\\cos\\theta-y\\sin\\theta \\\\ x\\sin\\theta+y\\cos\\theta \\end{bmatrix},R=\\begin{bmatrix} \\cos\\theta &amp; -\\sin\\theta \\\\ \\sin\\theta &amp; \\cos\\theta \\end{bmatrix} \\end{aligned} $$ 其中，P′ 就被称为旋转矩阵，于是就可以得到矩阵形式的二维旋转方程： P′ = RP 上例展示的是以原点为中心旋转的情况，如果是以任意坐标 (xr,yr) 作为旋转中心，则式子将会发生下面的变化： $$ \\begin{aligned} &amp;x^\\prime-x_r=(x-x_r)\\cos\\theta-(y-y_r)\\sin\\theta\\Rightarrow x^\\prime=x_r+(x-x_r)\\cos\\theta-(y-y_r)\\sin\\theta\\\\ &amp;y^\\prime-y_r=(x-x_r)\\sin\\theta+(y-y_r)\\cos\\theta\\Rightarrow y^\\prime=y_r+(x-x_r)\\sin\\theta+(y-y_r)\\cos\\theta \\end{aligned} $$ 以上就是通用的二维旋转方程。 3.3 缩放 改变一个图形的大小可以使用缩放。缩放的本质其实就是在原本坐标的基础上，给横纵坐标分别乘上一个缩放系数 sx 和 xy，于是有公式： $$ \\begin{aligned} &amp;x^\\prime=x·s_x\\\\ &amp;y^\\prime=y·s_y \\end{aligned} $$ 同样，我们将其化为矩阵形式： $$ \\begin{aligned} \\begin{bmatrix} x^\\prime \\\\ y^\\prime \\end{bmatrix}=\\begin{bmatrix} s_x &amp; 0 \\\\ 0 &amp; s_y \\end{bmatrix}·\\begin{bmatrix} x \\\\ y \\end{bmatrix} \\end{aligned} $$ 也可以表示为： P′ = S · P 其中，S 称为缩放矩阵。可以赋予 sx 和 sy 任意正值，当 sx 或 sy 的值大于 0 的时候则是放大对象；当 sx 或 sy 的值小于 0 的时候则是缩小对象。当 sx = sy 的时候，对象的长和宽都缩放一样的比例，这种缩放称为一致缩放；反之，就是差值缩放。同时，还可以给 sx 和 sy 赋予负值，此时不仅会改变图像的尺寸，还会使其相对于一个或多个轴反射。 一件有意思的事情是，当使用上述公式对图像进行缩放时，如果 sx, sy &lt; 1，图像将会朝靠近原点的方向缩小；如果 sx, sy &gt; 1，图像将会朝原理原点的方向放大。也就是说，缩放操作不仅导致图像被缩放，还会导致图像被重定位。我们可以选择一个缩放后位置不变的点，称为固定点，来控制对象缩放的位置，使其不再向原点缩放。设固定点坐标为 (xf,yf)，得到下面的公式： $$ \\begin{aligned} x^\\prime-x_f=(x-x_f)·s_x,\\quad y^\\prime-y_f=(y-y_f)·s_y \\end{aligned} $$ 重新整理上述公式可得： $$ \\begin{aligned} &amp;x^\\prime=x·s_x+x_f(1-s_x)\\\\ &amp;y^\\prime=y·s_y+y_f(1-s_y) \\end{aligned} $$ 3.4 齐次坐标 我们可以尝试将平移、旋转、缩放的公式写到一起，就有： P′ = M1 · P + M2 M1 是旋转矩阵和缩放矩阵的乘积，是一个 2*2 的矩阵，M2 是平移矩阵，其大小为 1*2。对于纯平移操作，M1 是单位矩阵；对于纯旋转或缩放操作，M2 并不是零矩阵，而是提供了旋转中心和固定点的增量值。 出于提升性能的需要，我们希望尽可能使用矩阵乘法，消除矩阵加法，并且避免复合操作的中间过程，直接通过一个公式导出最终结果，为此，我们需要引入齐次坐标。齐次坐标是一种用 N + 1 个数字来表示 N 维坐标系的方法，对于二维坐标，其一般形式为 (x,y,w)，用于表示二维坐标 $(\\frac xh,\\frac yh)$，其中，h 称为齐次参数，一般为 1，对于不是 1 的齐次坐标，我们需要手动将其化为 1。 齐次坐标对几何的影响十分深远，有了齐次坐标，我们甚至可以表示一个无穷远处的点，只需要将齐次参数设为 0 即可。教材只介绍了齐次坐标的使用，并未介绍齐次坐标的原理，感兴趣可以自行了解。 在引入了齐次坐标之后，我们可以重新表示二维坐标下的平移矩阵： $$ T=\\begin{bmatrix} 1 &amp; 0 &amp; x_t \\\\ 0 &amp; 1 &amp; y_t \\\\ 0 &amp; 0 &amp; 1 \\end{bmatrix} $$ 二维平移方程可改为： $$ \\begin{bmatrix} x^\\prime \\\\ y^\\prime \\\\ 1 \\end{bmatrix}=\\begin{bmatrix} 1 &amp; 0 &amp; x_t \\\\ 0 &amp; 1 &amp; y_t \\\\ 0 &amp; 0 &amp; 1 \\end{bmatrix}·\\begin{bmatrix} x \\\\ y \\\\ 1 \\end{bmatrix} $$ 这样，我们就成功地将平移操作转换为了矩阵乘法。类似地，我们也可以二维旋转公式转换为矩阵形式： $$ \\begin{bmatrix} x^\\prime \\\\ y^\\prime \\\\ 1 \\end{bmatrix}=\\begin{bmatrix} \\cos\\theta &amp; -\\sin\\theta &amp; 0 \\\\ \\sin\\theta &amp; \\cos\\theta &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\end{bmatrix}·\\begin{bmatrix} x \\\\ y \\\\ 1 \\end{bmatrix} $$ 二维缩放公式： $$ \\begin{bmatrix} x^\\prime \\\\ y^\\prime \\\\ 1 \\end{bmatrix}=\\begin{bmatrix} s_x &amp; 0 &amp; 0 \\\\ 0 &amp; s_y &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\end{bmatrix}·\\begin{bmatrix} x \\\\ y \\\\ 1 \\end{bmatrix} $$ 3.5 逆变换 如果我们需要将一个进行了平移后的对象还原回去，那就需要进行逆操作，逆平移矩阵为： $$ T^{-1}=\\begin{bmatrix} 1 &amp; 0 &amp; -t_x \\\\ 0 &amp; 1 &amp; -t_y \\\\ 0 &amp; 0 &amp; 1 \\end{bmatrix} $$ 对于逆旋转操作，其实现方式只是将其旋转角度改为负值，显然这只会对 sin 产生影响： $$ R^{-1}=\\begin{bmatrix} \\cos\\theta &amp; \\sin\\theta &amp; 0 \\\\ -\\sin\\theta &amp; \\cos\\theta &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\end{bmatrix} $$ 而对于逆缩放，只需要对缩放系数取倒数即可： $$ S^{-1}=\\begin{bmatrix} \\frac{1}{s_x} &amp; 0 &amp; 0 \\\\ 0 &amp; \\frac{1}{s_y} &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\end{bmatrix} $$ 3.6 复合变换 $$ P^\\prime=M_1·M_2...M_n·P\\\\ \\Downarrow\\\\ P^\\prime=M·P $$ 在将所有操作都统一为矩阵乘法之后，我们可以单用一个矩阵 M 来直接表示所有操作的复合，关于复合变换其实没什么好讲的，但这里有几条性质值得关注一下。 复合平移变换：T(t1x,t1y) · T(t2x,t2y) = T(t1x+t2x,t1y+t2y) $$ \\begin{bmatrix} 1 &amp; 0 &amp; t_{1x} \\\\ 0 &amp; 1 &amp; t_{1y} \\\\ 0 &amp; 0 &amp; 1 \\end{bmatrix}·\\begin{bmatrix} 1 &amp; 0 &amp; t_{2x} \\\\ 0 &amp; 1 &amp; t_{2y} \\\\ 0 &amp; 0 &amp; 1 \\end{bmatrix}=\\begin{bmatrix} 1 &amp; 0 &amp; t_{1x}+t_{2x} \\\\ 0 &amp; 1 &amp; t_{1y}+t_{2y} \\\\ 0 &amp; 0 &amp; 1 \\end{bmatrix} $$ 复合旋转变换：R(θ1) · R(θ2) = R(θ1+θ2) $$ \\begin{bmatrix} \\cos\\theta_1 &amp; -\\sin\\theta_1 &amp; 0 \\\\ \\sin\\theta_1 &amp; \\cos\\theta_1 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\end{bmatrix}·\\begin{bmatrix} \\cos\\theta_2 &amp; -\\sin\\theta_2 &amp; 0 \\\\ \\sin\\theta_2 &amp; \\cos\\theta_2 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\end{bmatrix}=\\begin{bmatrix} \\cos(\\theta_1+\\theta_2) &amp; -\\sin(\\theta_1+\\theta_2) &amp; 0 \\\\ \\sin(\\theta_1+\\theta_2) &amp; \\cos(\\theta_1+\\theta_2) &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\end{bmatrix} $$ 复合缩放变换：S(s1x,s1y) · S(s2x,s2y) = S(s1x·s2x,s1y·s2y) $$ \\begin{bmatrix} s_{1x} &amp; 0 &amp; 0 \\\\ 0 &amp; s_{1y} &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\end{bmatrix}·\\begin{bmatrix} s_{2x} &amp; 0 &amp; 0 \\\\ 0 &amp; s_{2y} &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\end{bmatrix}=\\begin{bmatrix} s_{1x}·s_{2x} &amp; 0 &amp; 0 \\\\ 0 &amp; s_{1y}·s_{2y} &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\end{bmatrix} $$ 基准点旋转 当工具包只支持围绕原点旋转时，我们可以使用如下方法实现围绕任意基准点 (xr,yr) 旋转： 通过平移使对象的基准点回到原点 对对象进行旋转 通过逆平移使对象的基准点回到原来位置 上述过程的图示如下： 复合变换矩阵表示为： $$ \\begin{bmatrix} 1 &amp; 0 &amp; x_{r} \\\\ 0 &amp; 1 &amp; y_{r} \\\\ 0 &amp; 0 &amp; 1 \\end{bmatrix}·\\begin{bmatrix} \\cos\\theta &amp; -\\sin\\theta &amp; 0 \\\\ \\sin\\theta &amp; \\cos\\theta &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\end{bmatrix}·\\begin{bmatrix} 1 &amp; 0 &amp; -x_{r} \\\\ 0 &amp; 1 &amp; -y_{r} \\\\ 0 &amp; 0 &amp; 1 \\end{bmatrix}=\\begin{bmatrix} \\cos\\theta &amp; -\\sin\\theta &amp; x_{r}(1-\\cos\\theta)+y_{r}\\sin_\\theta \\\\ \\sin\\theta &amp; \\cos\\theta &amp; y_{r}(1-\\cos\\theta)-x_{r}\\sin_\\theta \\\\ 0 &amp; 0 &amp; 1 \\end{bmatrix} $$ 基准点缩放 和围绕基准点旋转的方法一样，都是先将基准点调整至原点，缩放，然后再调整回去。复合变换矩阵表示为： $$ \\begin{bmatrix} 1 &amp; 0 &amp; x_{r} \\\\ 0 &amp; 1 &amp; y_{r} \\\\ 0 &amp; 0 &amp; 1 \\end{bmatrix}·\\begin{bmatrix} s_x &amp; 0 &amp; 0 \\\\ 0 &amp; s_y &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\end{bmatrix}·\\begin{bmatrix} 1 &amp; 0 &amp; -x_{r} \\\\ 0 &amp; 1 &amp; -y_{r} \\\\ 0 &amp; 0 &amp; 1 \\end{bmatrix}=\\begin{bmatrix} s_x &amp; 0 &amp; x_f(1-s_x) \\\\ 0 &amp; s_y &amp; y_f(1-s_y) \\\\ 0 &amp; 0 &amp; 1 \\end{bmatrix} $$ 定向缩放 正常的 sx 和 sy 沿 x 轴和 y 轴缩放对象，但如果希望沿其他轴线缩放，可以使用类似基准点缩放的方法，即先将缩放轴线调整至与 x 轴或 y 轴重合再进行缩放，最后再调整回来。如下图所示： 复合变换矩阵表示为： $$ R^{-1}(\\theta)·S(s_1,s_2)·R(\\theta)=\\begin{bmatrix} s_1\\cos^2\\theta+s_2\\sin^2\\theta &amp; (s_2-s_1)\\cos\\theta\\sin\\theta &amp; 0 \\\\ (s_2-s_1)\\cos\\theta\\sin\\theta &amp; s_1\\sin^2\\theta+s_2\\cos^2\\theta &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\end{bmatrix} $$ 另外，按照矩阵乘法的结合性质，M1 · M2 · M3 = (M1·M2) · M3 = M1 · (M2·M3)；但是，变换积一般不可交换顺序，也就是 M1 · M2 ≠ M2 · M1，除非所进行的复合变换属于同种变换。 3.7 错切 错切是指，将某一点沿 x 轴 (或 y 轴) 方向移动其与 y 轴 (或 x 轴) 距离的某一倍数，定义错切参数 shx、shy，则有： $$ \\begin{aligned} x^\\prime=x+y·\\operatorname{sh}_x\\\\ y^\\prime=y+x·\\operatorname{sh}_y \\end{aligned} $$ 错切的变换矩阵为： $$ \\begin{bmatrix} 1 &amp; \\operatorname{sh}_{x} &amp; 0 \\\\ \\operatorname{sh}_{y} &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 1\\end{bmatrix} $$ 如果想做相对于其他参考线的错切，可以使用下面的变换矩阵： $$ \\begin{bmatrix} 1 &amp; \\operatorname{sh}_{x} &amp; -\\operatorname{sh}_x·y_{ref} \\\\ \\operatorname{sh}_{y} &amp; 1 &amp; -\\operatorname{sh}_y·x_{ref} \\\\ 0 &amp; 0 &amp; 1\\end{bmatrix} $$ 这样，错切方程将会变为： $$ \\begin{aligned} &amp;x^\\prime=x+\\operatorname{sh}_{x}(y-y_{ref})\\\\ &amp;y^\\prime=y+\\operatorname{sh}_y(x-x_{ref}) \\end{aligned} $$ 3.8 作业 将正方形 A(0,0)、B(0,1)、C(1,1)、D(1,0)，放大 2 倍，并保持 C(1,1) 位置不变，请给出变换矩阵以及对应的正方形顶点坐标。 将正方形 ABCD 放大 2 倍且要保持 C 点位置不变，即以 C 点为基准点进行缩放，变换矩阵为： $$ M=\\begin{bmatrix} 1 &amp; 0 &amp; 1 \\\\ 0 &amp; 1 &amp; 1 \\\\ 0 &amp; 0 &amp; 1 \\end{bmatrix}·\\begin{bmatrix} 2 &amp; 0 &amp; 0 \\\\ 0 &amp; 2 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\end{bmatrix}·\\begin{bmatrix} 1 &amp; 0 &amp; -1 \\\\ 0 &amp; 1 &amp; -1 \\\\ 0 &amp; 0 &amp; 1 \\end{bmatrix}=\\begin{bmatrix} 2 &amp; 0 &amp; -1 \\\\ 0 &amp; 2 &amp; -1 \\\\ 0 &amp; 0 &amp; 1 \\end{bmatrix} $$ 对答案进行检验： $$ \\begin{aligned} &amp;A^\\prime=\\begin{bmatrix} 2 &amp; 0 &amp; -1 \\\\ 0 &amp; 2 &amp; -1 \\\\ 0 &amp; 0 &amp; 1 \\end{bmatrix}·\\begin{bmatrix} 0 \\\\ 0 \\\\ 1 \\end{bmatrix}=\\begin{bmatrix} -1 \\\\ -1 \\\\ 1 \\end{bmatrix}\\\\ &amp;B^\\prime=\\begin{bmatrix} 2 &amp; 0 &amp; -1 \\\\ 0 &amp; 2 &amp; -1 \\\\ 0 &amp; 0 &amp; 1 \\end{bmatrix}·\\begin{bmatrix} 0 \\\\ 1 \\\\ 1 \\end{bmatrix}=\\begin{bmatrix} -1 \\\\ 1 \\\\ 1 \\end{bmatrix}\\\\ &amp;C^\\prime=\\begin{bmatrix} 2 &amp; 0 &amp; -1 \\\\ 0 &amp; 2 &amp; -1 \\\\ 0 &amp; 0 &amp; 1 \\end{bmatrix}·\\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\end{bmatrix}=\\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\end{bmatrix}\\\\ &amp;D^\\prime=\\begin{bmatrix} 2 &amp; 0 &amp; -1 \\\\ 0 &amp; 2 &amp; -1 \\\\ 0 &amp; 0 &amp; 1 \\end{bmatrix}·\\begin{bmatrix} 1 \\\\ 0 \\\\ 1 \\end{bmatrix}=\\begin{bmatrix} 1 \\\\ -1 \\\\ 1 \\end{bmatrix} \\end{aligned} $$ 画出图像： 正方形 A(0,0)、B(0,1)、C(1,1)、D(1,0) 经过变换后的坐标分别对应为 (0,0)、(0.5,1.5)、(2,2)、(1.5,0.5)。请给出变换矩阵。 画出变换之后的图像可以发现，是由原图像沿右对角线定向缩放得到的，此时应该设定 θ = 45°，sx = 1，sy = 2，θ 为 45° 是因为对角线想要逆时针旋转到 y 轴需要旋转 45°，而 sx 和 sy 是如何确定的呢？我们将原图像逆时针旋转 45° 后，计算出每个点的坐标，再将变换后的图像也逆时针旋转 45°，也计算出各点的坐标，然后计算每个轴上缩放的比例即可。计算完比例之后，根据定向缩放的公式，得到： $$ M=R^{-1}(45°)·S(1,2)·R(45°)=\\begin{bmatrix} 1.5 &amp; 0.5 &amp; 0 \\\\ 0.5 &amp; 1.5 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\end{bmatrix} $$ 上面演示的是带公式的方法，还有一种更简单的方法是，利用矩阵逆运算： $$ \\begin{aligned} &amp;M·\\begin{bmatrix} 0 &amp; 0 &amp; 1 &amp; 1 \\\\ 0 &amp; 1 &amp; 1 &amp; 0 \\\\ 1 &amp; 1 &amp; 1 &amp; 1 \\end{bmatrix}=\\begin{bmatrix} 0 &amp; 0.5 &amp; 2 &amp; 1.5 \\\\ 0 &amp; 1.5 &amp; 2 &amp; 0.5 \\\\ 1 &amp; 1 &amp; 1 &amp; 1 \\end{bmatrix}\\\\\\\\ &amp;M=\\begin{bmatrix} 0 &amp; 0.5 &amp; 2 &amp; 1.5 \\\\ 0 &amp; 1.5 &amp; 2 &amp; 0.5 \\\\ 1 &amp; 1 &amp; 1 &amp; 1 \\end{bmatrix}·\\begin{bmatrix} 0 &amp; 0 &amp; 1 &amp; 1 \\\\ 0 &amp; 1 &amp; 1 &amp; 0 \\\\ 1 &amp; 1 &amp; 1 &amp; 1 \\end{bmatrix}^{-1} \\end{aligned} $$ 解得变换矩阵仍然为： $$ M=\\begin{bmatrix} 1.5 &amp; 0.5 &amp; 0 \\\\ 0.5 &amp; 1.5 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\end{bmatrix} $$ 04 样条表示 样条是通过一组指定点集而生成平滑曲线的柔性带。通俗来讲，之前学的画线算法解决的是在图形学里画直线的问题，样条表示解决的是在图形学里画曲线的问题。 在计算机图形学中，样条曲线指多项式曲线段连接而成的曲线，在每段的边界处满足特定的连续性条件。样条曲面可以使用两组样条曲线进行描述。 4.1 基本概念 4.1.1 曲线曲面的表示方法 曲线曲面的表示有三种：显式、隐式和参数表示。 显式表示就是我们熟知的 y = f(x) 形式，这种形式虽然直观简单，但是显式方程中，一个 x 只能对应一个 y 值，所以显式方程不能表示封闭或多值曲线。 隐式表示的形式是 f(x,y) = 0，隐式方程的优点是易于判断一个点是否在曲线上，缺点是不直观，作图不方便。并且，不论显示还是隐式，都与坐标轴相关，而且有可能会出现斜率无穷大的情形。 为了避免这些问题，可以选择参数方程来表示。假定用 t 表示参数，平面参数上任一点 P 可表示为： p(t) = [x(t), y(t)] 这是二维的情形，如果是在空间里，三维点 P 可表示为： p(t) = [x(t), y(t), z(t)] 它等价于笛卡尔分量表示： p(t) = x(t)i⃗ + y(t)j⃗ + z(t)k⃗ 其中，i⃗、j⃗、k⃗ 是三个坐标轴上的单位向量。这样，给定一个 t 值，就得到曲线上一点的坐标。 假设曲线段对应的参数区间为 [a,b]，即 a ≤ t ≤ b，为方便将区间 [a,b] 规范化成 [0,1]，参数变换为： $$ t^\\prime=\\frac{t-a}{b-a} $$ 参数曲线一般可写成： p = p(t) t ∈ [0,1] 类似地，可以把曲面表示成为双参数 u 和 v 的矢量函数： p(u,v) = p(x(u,v), y(u,v), z(u,v)) (u,v) ∈ [0,1] × [0,1] 4.1.2 曲线曲面的绘制方法 在计算数学中，逼近通常指用一些性质较好的函数近似表示一些性质不好的函数。在计算机图形学中，逼近继承了这方面的含义。插值和拟合是两种图形学中常用的曲线曲面的绘制方法，它们体现的思想都可以视为逼近。 方法 1：插值 自由曲线和自由曲面一般通过少数分散的点生成，这些点叫做“型值点”、“样本点”或“控制点”。 给定一组有序的数据点 Pi (i=0,1,2,...,n)，要求构造一条曲线，顺序通过这些数据点，这一过程称为对这些数据点进行插值，所构造的曲线称为插值曲线。插值的过程如下所示： 插值的目标就是找一个函数 f(x)，使得该函数图像刚刚好穿过所有的点。如果该函数是一个形如 f(x) = ax + b 的线性函数，则称为线性插值，这种情况下其实是拿一条直线去近似一条曲线；如果该函数是一个形如 f(x) = ax2 + bx + c 的抛物线函数，则称为抛物线插值。 方法 2：拟合 在实际实验中，这些控制点也难免会有误差，也就是说控制点的位置也不一定准确，所以没必要使曲线一定要穿过每一个控制点。 构造一条曲线，使之在某种意义下最接近给定的数据点（但未必通过这些点），所构造的曲线为拟合曲线，这一过程也就称为拟合。其过程如下所示： 可以发现，由这四个控制点所绘制出的曲线并没有完全穿过所有控制点，而是只穿过了起始点和终止点。 一般，我们将连接有一定次序控制点的直线序列称为控制多边形或特征多边形。 4.2 Hermite 曲线 Hermite 曲线是一种插值曲线，通过给定的控制点和切线来定义曲线的形状。 参数曲线有代数形式和几何形式。接下来以三次参数曲线为例，讨论参数曲线的代数和几何形式。参数曲线的代数形式如下： $$ \\left\\{ \\begin{array}{} x(t)=a_{3x}t^3+a_{2x}t^2+a_{1x}t+a_{0x}\\\\ y(t)=a_{3y}t^3+a_{2y}t^2+a_{1y}t+a_{0y}\\\\ z(t)=a_{3z}t^3+a_{2z}t^2+a_{1z}t+a_{0z}\\\\ \\end{array} \\right. \\quad\\quad t\\in[0,1] $$ 将上述代数式写成矢量式就是： P(t) = a3t3 + a2t2 + a1t + a0 t ∈ [0,1] 在上式中，a3、a2、a1、a0 是参数曲线的系数，但不是常数而是向量。代数形式的缺点在于，倘若系数被改变，曲线会如何变化是不清楚的。 参数曲线的几何形式是利用一条曲线端点的几何性质来刻画一条曲线。所谓端点的几何性质，就是指曲线的端点位置、切向量、各阶导数等端点的信息。对于三次参数曲线，可以用其端点的位矢 P(0)、P(1) 和切矢 P′(0)、P′(1) 描述，我们将这四个量简记为 P0、P1、P0′、P1′。我们将这四个量分别代入参数曲线的矢量式和其一阶导，也就是将 t = 0, 1 代入，得到： $$ \\left\\{ \\begin{array}{} \\begin{aligned} &amp;P_0=a_0\\\\ &amp;P_1=a_3+a_2+a_1+a_0\\\\ &amp;P_0^\\prime=a_1\\\\ &amp;P_1^\\prime=3a_3+2a_2+a_1 \\end{aligned} \\end{array} \\right. $$ 于是就能得到： $$ \\left\\{ \\begin{array}{} \\begin{aligned} &amp;a_0=P_0\\\\ &amp;a_1=P^\\prime_0\\\\ &amp;a_2=-3P_0+3P_1-2P_0^\\prime-P_1^\\prime\\\\ &amp;a_3=2P_0-2P_1+P_0^\\prime+P_1^\\prime \\end{aligned} \\end{array} \\right. $$ 将上面的解代会到原式中，就能得到参数曲线的几何形式： P(t) = (2t3−3t2+1)P0 + (−2t3+3t2)P1 + (t3−2t2−t)P0′ + (t3−t2)P1′ 我们令： $$ \\begin{aligned} &amp;F_0(t)=2t^3-3t^2+1\\\\ &amp;F_1(t)=-2t^3+3t^2\\\\ &amp;G_0(t)=t^3-2t^2-t\\\\ &amp;G_1(t)=t^3-t^2\\\\ \\end{aligned} $$ 再代入原来的方程： P(t) = F0P0 + F1P1 + G0P0′ + G1P1′ t ∈ [0,1] 其中，P0、P1、P0′、P1′ 是几何系数，F0、F1、G0、G1 称为调和函数（或混合函数）。上式就是三次 Hermite 曲线的几何形式。 4.3 Bezier 曲线 4.3.1 定义 针对 Bezier 曲线，给定空间 n+1 个点的位置矢量 Pi (i=0,1,2,...,n)，则 Bezier 曲线段的参数方程表示如下： $$ p(t)=\\sum\\limits_{i=0}^{n}P_iB_{i,n}(t)\\quad t\\in[0,1] $$ 其中，Pi(xi,yi,zi), i = 1, 2, ..., n 是控制多边形的 n+1 个顶点，即构成该曲线的特征多边形，Bi, n(t) 是 Bernstein 基函数，有如下形式： $$ B_{i,n}(t)=\\frac{n!}{i!(n-i)!}t^i(1-t)^{n-i}=C_n^it^{i(1-t)^{n-1}}\\quad(i=0,1,...,n) $$ $\\sum_{i=0}^{n}B_{i,n}(t)$ 恰好是二项式 [t+(1−t)]n 的展开式！ 4.3.2 一次 Bezier 曲线 当 n=1 的时候，只有两个控制点 P0 和 P1，Bezier 多项式是一次多项式： $$ p(t)=\\sum_{i=0}^{1}P_iB_{i,1}(t)=P_0B_{0,1}(t)+P_1B_{1,1}(t) $$ 直接代入 Bernstein 方程求 B0, 1 和 B1, 1： $$ \\begin{aligned} &amp;B_{0,1}=\\frac{n!}{i!(n-i)!}t^i(1-t)^{n-i}=\\frac{1!}{0!(1-0)!}t^0(1-t)^{1-0}=1-t\\\\ &amp;B_{1,1}=\\frac{n!}{i!(n-i)!}t^i(1-t)^{n-i}=\\frac{1!}{1!(1-1)!}t^1(1-t)^{1-1}=t\\\\ \\end{aligned} $$ 所以，化简后的一次 Bezier 曲线的方程为： p(t) = (1−t)P0 + tP1 这刚刚好是连接起点 P0 和终点 P1 的直线段。 4.3.3 二次 Bezier 曲线 当 n=2 时，有 3 个控制点 P0、P1 和 P2，Bezier 多项式是二次多项式： $$ p(t)=\\sum_{i=0}^{2}P_iB_{i,2}(t)=P_0B_{0,2}(t)+P_1B_{1,2}(t)+P_2B_{2,2}(t) $$ 求解 B0, 2、B1, 2 和 B2, 2： $$ \\begin{aligned} &amp;B_{0,2}=\\frac{n!}{i!(n-i)!}t^i(1-t)^{n-i}=\\frac{2!}{0!(2-0)!}t^0(1-t)^{2-0}=(1-t)^2\\\\ &amp;B_{1,2}=\\frac{n!}{i!(n-i)!}t^i(1-t)^{n-i}=\\frac{2!}{1!(2-1)!}t^1(1-t)^{2-1}=2t(1-t)\\\\ &amp;B_{2,2}=\\frac{n!}{i!(n-i)!}t^i(1-t)^{n-i}=\\frac{2!}{2!(2-2)!}t^2(1-t)^{2-2}=t^2\\\\ \\end{aligned} $$ 最后化简得到的二次 Bezier 曲线的方程为： p(t) = (1−t)2P0 + 2t(1−t)P1 + t2P2 二次 Bezier 曲线为抛物线，其矩阵形式为： $$ p(t)=\\left[ \\begin{array}{} t^2 &amp; t &amp; 1 \\\\ \\end{array} \\right] \\cdot \\left[ \\begin{array}{} 1 &amp; -2 &amp; 1 \\\\ -2 &amp; 2 &amp; 0 \\\\ 1 &amp; 0 &amp; 0 \\\\ \\end{array} \\right] \\cdot \\left[ \\begin{array}{} P_0 \\\\ P_1 \\\\ P_2 \\end{array} \\right] \\quad t\\in[0,1] $$ 4.3.4 三次 Bezier 曲线 当 n=3 时，有 4 个控制点 P0、P1、P2 和 P3，Bezier 多项式是三次多项式： $$ p(t)=\\sum_{i=0}^{3}P_iB_{i,3}(t)=P_0B_{0,3}(t)+P_1B_{1,3}(t)+P_2B_{2,3}(t)+P_3B_{3,3}(t) $$ 求解 B0, 3、B1, 3、B2, 3 和 B3, 3： $$ \\begin{aligned} &amp;B_{0,3}=\\frac{n!}{i!(n-i)!}t^i(1-t)^{n-i}=\\frac{3!}{0!(3-0)!}t^0(1-t)^{3-0}=(1-t)^3\\\\ &amp;B_{1,3}=\\frac{n!}{i!(n-i)!}t^i(1-t)^{n-i}=\\frac{3!}{1!(3-1)!}t^1(1-t)^{3-1}=3t(1-t)^2\\\\ &amp;B_{2,3}=\\frac{n!}{i!(n-i)!}t^i(1-t)^{n-i}=\\frac{3!}{2!(3-2)!}t^2(1-t)^{3-2}=3t^2(1-t)\\\\ &amp;B_{3,3}=\\frac{n!}{i!(n-i)!}t^i(1-t)^{n-i}=\\frac{3!}{3!(3-3)!}t^3(1-t)^{3-3}=t^3\\\\ \\end{aligned} $$ 最后化简得到的三次 Bezier 曲线的方程为： p(t) = (1−t)3P0 + 3t(1−t)2P1 + 3t2(1−t)P2 + t3P3 将三次 Bezier 曲线写为矩阵形式： $$ \\begin{aligned} p(t)&amp;=\\left[ \\begin{array}{} t^3 &amp; t^2 &amp; t &amp; 1 \\\\ \\end{array} \\right] \\cdot \\left[ \\begin{array}{} -1 &amp; 3 &amp; -3 &amp; 1 \\\\ 3 &amp; -6 &amp; 3 &amp; 0 \\\\ -3 &amp; 3 &amp; 0 &amp; 0\\\\ 1 &amp; 0 &amp; 0 &amp; 0 \\end{array} \\right] \\cdot \\left[ \\begin{array}{} P_0 \\\\ P_1 \\\\ P_2 \\\\ P_3 \\end{array} \\right] \\quad t\\in[0,1]\\\\ &amp;=T\\cdot M_{be}\\cdot G_{be} \\end{aligned} $$ 其中，Mbe 是三次 Bezier 曲线的系数矩阵，为常数；Gbe 是 4 个控制点位置矢量。 下面是一个用 python 实现的绘制三次 Bezier 曲线的示意图：","categories":[{"name":"计算机专业基础","slug":"计算机专业基础","permalink":"https://example.com/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E4%B8%93%E4%B8%9A%E5%9F%BA%E7%A1%80/"}],"tags":[{"name":"graphics","slug":"graphics","permalink":"https://example.com/tags/graphics/"}]},{"title":"Transformer","slug":"Transformer","date":"2023-10-24T05:03:54.000Z","updated":"2025-04-09T16:35:05.574Z","comments":true,"path":"2023/10/24/Transformer/","permalink":"https://example.com/2023/10/24/Transformer/","excerpt":"01 Seq2seq Transformer 是一个 sequence-to-sequence model，我们一般简写作 seq2seq。seq2seq 的意思是指，输入是一个 sequence，输出也是一个 sequence，并且两个 sequence 的长度不一定，这里的长度不一定的意思是指：1. 输入的长度不一定；2. 输出的长度由机器自己决定；3. 输入和输出的长度并不存在必然关系。 比较典型的 seq2seq 的例子有语音识别、文字翻译、语音翻译等，还有例如 text-to-speech (文字转语音) 的 model 也是一种 seq2seq，甚至是聊天机器人 chatbot，也是 seq2seq。 其实，seq2seq 在 NLP 方面的应用是很广泛的，很多你认为可能跟 seq2seq 无关的任务也可以转换成 seq2seq。NLP 的本质其实是 question answering (QA)，而只要能想象成 QA，就基本上都能用 seq2seq 解决。 Seq2seq 还可以用于解决 multi-label classification，multi-label 是将一个东西分到多个类别里的任务，并且一个东西可能属于不止一个类别，而 seq2seq 的输出长度是机器自己决定的，也就是机器觉得有几个输出就几个输出，也就是机器觉得这个东西属于哪几个类别那就是哪几个类别，所以 seq2seq 也可以硬解 multi-label classification。 除此以外，就连图像识别问题也可以用 seq2seq，但是这里就不展开了。至少至此，我们已经知道了 seq2seq 是一个强大的 model，那么它究竟是怎么做到的，接下来我们就要开始研究了。","text":"01 Seq2seq Transformer 是一个 sequence-to-sequence model，我们一般简写作 seq2seq。seq2seq 的意思是指，输入是一个 sequence，输出也是一个 sequence，并且两个 sequence 的长度不一定，这里的长度不一定的意思是指：1. 输入的长度不一定；2. 输出的长度由机器自己决定；3. 输入和输出的长度并不存在必然关系。 比较典型的 seq2seq 的例子有语音识别、文字翻译、语音翻译等，还有例如 text-to-speech (文字转语音) 的 model 也是一种 seq2seq，甚至是聊天机器人 chatbot，也是 seq2seq。 其实，seq2seq 在 NLP 方面的应用是很广泛的，很多你认为可能跟 seq2seq 无关的任务也可以转换成 seq2seq。NLP 的本质其实是 question answering (QA)，而只要能想象成 QA，就基本上都能用 seq2seq 解决。 Seq2seq 还可以用于解决 multi-label classification，multi-label 是将一个东西分到多个类别里的任务，并且一个东西可能属于不止一个类别，而 seq2seq 的输出长度是机器自己决定的，也就是机器觉得有几个输出就几个输出，也就是机器觉得这个东西属于哪几个类别那就是哪几个类别，所以 seq2seq 也可以硬解 multi-label classification。 除此以外，就连图像识别问题也可以用 seq2seq，但是这里就不展开了。至少至此，我们已经知道了 seq2seq 是一个强大的 model，那么它究竟是怎么做到的，接下来我们就要开始研究了。 02 Mechanism Seq2seq 主要由两大部分组成 —— encoder 和 decoder。 Encoder 负责接收和处理数据，然后将处理过的数据交给 decoder，decoder 则根据数据决定输出结果。接下来，我们就来具体来看看 encoder 和 decoder 的结构。 2.1 Encoder 简单来说，encoder 要做的事情就是将接收的一排向量，转换成另一排向量，这一个过程可以用很多种方法来完成，例如 RNN 和 CNN，而 transformer 采用的是 [[Self-attention|self-attention]]，大名鼎鼎的注意力机制也就是从 transformer 中诞生的。 Encoder 内部其实是一个一个 block (块)，每个 block 都接收一排向量，输出一排向量，而且每个 block 的工作也大致都是相同的，其实都是对 input 做 self-attention，然后将 output1 丢进一个全连接网络，得到 output2，这个就是一个 block 的输出。所以 encoder 的结构大致如下： 实际上，transformer 中的 self-attention 是比我们之前讲的 self-attention 更复杂的。在 transformer 中，self-attention 的 output 还要再加上最开始的 input，才算得到最终的 output，这种架构被称为 residual connection (残差连接)，这个技术旨在解决深度神经网络训练过程中的梯度消失和梯度爆炸等问题。然后，还要对得到的 residule 做 layer normalization，方法是：求 residule 整个序列的均值 m 和标准差 σ，然后做标准化： $$ x_i^\\prime=\\frac{x_i-m}{\\sigma} $$ 这样得到的序列才是全连接网络的输入，但是还没完，全连接网络的输出仍要再进行一次 residule connection，即将输入和输出相加，然后同样要对 residule 进行 layer normalization，这样才能得到 block 的输出。 现在我们来看一下 Attention is all you need 这篇论文中所画的 encoder 的结构： 首先，input 进行 embedding 之后，作为输入进行 multi-head attention，考虑到有些时候序列可能是有序的，所以图中还画出了 positional encoding 的环节，这项技术在 self-attention 的笔记中有提到。Attention 之后，得到的输出要先于最初的输入进行相加 (add)，然后进行 layer normalization，这就是上图中淡黄色框的含义。再网上要进行 feed forward，其实就是把上一步得到的结果喂给全连接网络，然后对得到的结果再进行一次 Add &amp; Norm。这就是上图的含义。 2.2 Decoder Decoder 的架构其实分为两种 —— autoregressive (AT) 和 non-autoregressive (NAT)，接下来要将的 autoprogressive 是比较常见的架构，我们将以语音辨识为例进行讲解。 2.2.1 Autoprogressive decoder 的工作流程 假设我们在处理语音辨识的问题，现在，语音已经通过 encoder 转换成了一个序列，decoder 要接收这个序列，然后输出对应的文字。我们暂且不提 decoder 是如何接收 encoder 的输出的，我们假设 decoder 能接收到 encoder 的输出，然后来解释一下 decoder 的工作流程。 首先，我们得给 decoder 一个特殊的 token，我们称之为 BOS (begin of sentence)，接下来简称 BEGIN。当 decoder 接收到这个 token 时，它就开始输出，它的输出应该是一个 one-hot vector，也就是一个独热编码的向量，其大小等同于词汇的大小，以中文为例，可能就是所有中文字的数量。当然你可能会说这有点太大了，那实际上，我们可能只取常见的几千个中文字，生僻字我们不去管。 这个 vector 中的数字就是每个对应位置上的中文字的可能性，它们的总和是 1，由对 decoder 的输出做 softmax 之后得到，最终 decoder 生成汉字就是取其中可能性最大的那个。 总而言之，我们现在得到了第一个汉字，假设这个汉字是“机”，那么下一步，我们要把这个 decoder 生成的汉字加入到 decoder 的输入中，也就是说，现在的 decoder 的输入不只有 BEGIN 了，还有了“机”，于是重复上面的步骤，得到第二个输出“器”，周而复始……最终得到完整的输出“机器学习”。在这个过程中，decoder 当然也有读入 encoder 的输出，但是这一部分我们先不讲。总结上面的过程，我们可以说，其实 decoder 就是将自己前一刻的输出当作输入，进一步得到下一时刻的输出。这里就诞生了一个问题：要是 decoder 自己预测的内容出错了怎么办？会不会造成 error propagation，也就是一步错步步错？当然是有可能的，但是这个问题我们之后再谈，我们先暂且当作没这回事。 decoder 和 encoder 的结构对比 现在我们来看看论文中画的 decoder 的结构，decoder 看上去很复杂，但如果我们将其与 encoder 的结构进行对比，似乎能发现一些相似之处。 你可能会发现，如果我们把 decoder 中间那一块盖起来，encoder 的结构好像就跟 decoder 差不了多少了，无非是 decoder 的输出最后还要经过一个线性层，再经过 softmax 激活，来输出可能性罢了。唯一的不同可能就是 decoder 中，一开始的 attention 是 masked multi-head attention，那这个 masked 是什么意思？ masked self-attention 在我们原来所讲的 self-attention 中，输出的 b1 是考虑了 a1 ∼ an 所有的资讯之后得到的 a1 的资讯，b2 ∼ bn 皆是如此；而在 masked self-attention 中，输出不再能考虑后面的资讯，意思是，b1 只是考虑了 a1 后，a1 的资讯；b2 是考虑了 a1, a2 之后，a2 的资讯；b3 是考虑了 a1, a2, a3 之后，a3 的资讯……只有 bn 是考虑了整个输入后，输出的资讯。这就是 masked self-attention。 至于为什么要用 masked，其实原因很简单。我们之前也解释了，decoder 会拿自己的输出作为输入，输出是从左到右依次产生的，输入肯定也只能从左到右依次输入，所以就出现了这样只能读左边，而不能读右边的 masked self-attention 机制。 何时终止输出 之前说过，transformer 的输出长度是机器自己决定的，也就是由 decoder 决定的，但到目前为止，我们都还没有讨论过，decoder 到底是如何决定输出长度的，它是怎么知道什么时候该停止输出的。就像前面举的例子，在 decoder 输出完“机器学习”之后，万一它又把 BEGIN+“机器学习”作为输入，输出了个“惯”字怎么办？ 对这一点的处理是很巧妙的，要解决这个问题，我们还得再准备一个特殊的 token，叫作 END。就像 BEGIN 是开始的标志，END 就是终止的标志，并且，END 存储在 encoder 的输出 one-hot vector 里，也就是说，one-hot vector 的长度是 vocabulary 的长度再加上 END。如果根据 encoder 的计算，发现输出的 one-hot vector 中，END 的概率是最高的，那就意味着输出该结束了。通过这种方式，encoder 就能自行决定何时终止输出。 2.2.2 Non-autoregressive 接下来我们简短地介绍一下另一种 decoder 的架构 —— non-autoregressive，简称 NAT。 AT vs NAT AT 的运作方式是，先将开始标志 BEGIN 作为输入传入，得到一个输出，随后将自己的输出作为输入再传进来，再得到下一个输出，然后再将下一个输出继续传进来，得到下下个输出，循环往复，直到输出 END；而 NAT 则不一样，它一开始就接收多个 BEGIN，于是得到多个输出，也就是一个完整的句子，然后就结束了。 何时终止输出 那么问题就来了，既然我们一开始都不知道输出的长度，那我们要怎么知道该给 decoder 多少个 BEGIN？ 解决这个问题的方法有很多，一种方法就是准备一个 classifier，将 encoder 的输出先丢给 classifier，由 classifier 决定输出的长度，于是就丢对应长度的 BEGIN。 另一种可能的处理方法是，不管输出的长度，直接丢给 decoder 尽可能大的 BEGIN 数目，然后看看 decoder 的输出中，哪里出现了 END 标志，我们只取 END 前面的内容，END 后面的输出就不管了。 NAT 的优势 NAT 的优势在于，它的运算是平行的，而不像 AT 那样，要预测下一个输出，就必须等前一个输出完成，所以 NAT 的速度应该是比 AT 要快的；另外，它的输出长度是可控的，这就允许我们人为地去控制输出。 NAT 是一个热门的研究话题，它的性能比 AT 要好，也比 AT 的可控性高，但一个严重的问题是，NAT 的准确率是远不及 AT 的。NAT 要想赶上 AT 的准确率，往往需要很多的秘诀才能做到，而 AT 可能只要随便跑跑就能达到比较高的准确率。所以，如何让 NAT 赶上 AT 的准确率，是目前一个大热门。不过这里就不细讲了，毕竟这是一个大坑，有兴趣的话可以自己去了解。 2.3 Encoder-Decoder 接下来，我们就要来看看之前说先暂且遮起来的那一块了。 图中被框起来的那一部分叫作 cross attention，它是连接 encoder 和 decoder 的桥梁。你可以看到，从 encoder 引出了两个箭头连接到了 multi-head attention，除此以外还有一个箭头是从 decoder 的上一层引出的，那这个模组到底是怎么运作的？我们继续以语音辨识的例子进行阐述。 首先，encoder 读进一个向量 a1, a2, a3，并且输出一个等长的向量 b1, b2, b3，用同样的方法，得到对应的 k1, k2, k3 和 v1, v2, v3；同时，decoder 读进一个 BEGIN，进行 masked self-attention，由于是 attention，所以输出也肯定是一个和 BEGIN 等长的向量，接下来，将这个输出乘上一个矩阵，进行 transform，得到 q；然后用 q 与 k1, k2, k3 去分别计算，得到 α1, α2, α3，然后再分别乘上 v1, v2, v3，将加权的结果加起来，得到 v，这个 v 就是接下来的全连接网络的输入。在这个过程中，α, k, v 都来自 encoder，而 q 来自 decoder，所以这个过程就叫做 cross attention。 如果说现在 decoder 已经产生了一个输出“机”，那么接下来的操作也是一样的，将“机”作为输入传进去，进行 masked self-attention，然后得到 q′，去做相同的运算得到 v′，再丢进全连接网络得到下一个输出，如此往复…… 2.4 Conclusion Transformer 的工作流程已经讲完了，为了深入理解这些过程，而不是仅仅停留于表面的数学运算，我们还得从实例中剖析这个过程。Ecoder 所做的工作其实是对 input 进行提炼，方法就是 self-attention；而 decoder 则是将 encoder 提炼后的数据，以及一个开始标志 BEGIN 作为输入，开始生成结果，并且每得到一个 output，就将这个结果加入到自己的 input 中，做 masked self-attention，以此不断得到新的 output，直到 decoder 输出 END 为止。 下面我们以机器翻译为例，假设我们需要机器翻译“我喜欢你”这句话，那么 transfomer 做的第一件事是，将这句话输入到 encoder 当中，通过 self-attention 获取语义编码，这里以 c 标识，这里的 c 是一个向量，其中包括了 c1, c2, ... 等等，每一个汉字就对应一个 c。 c = Encoder (\"我喜欢你\") 然后，decoder 将该语义编码与一个 token，也就是上面说的 BEGIN 作为输入，得到第一个单词的输出： I = Decoder (c1,BEGIN) 然后将这个单词作为输入，再进行一次输出： love = Decoder (c2,BEGIN,I) 重复上面的过程，直到输出 END： $$ \\begin{aligned} &amp;\\text{you} = \\operatorname{Decoder}(c_3,BEGIN,I,love)\\\\\\\\ &amp;\\text{END}=\\operatorname{Decoder}(c_4,BEGIN,I,love,you) \\end{aligned} $$ 于是最后得到的翻译结果：I love you. 03 Training 现在我们已经把 transformer 的内部运作方式给讲完了，下一步，就是讲讲 transformer 的训练。 我们知道，机器学习的目标就是不断降低 loss。而 seq2seq 这件事，似乎就是在做分类，最后的输出是从所有汉字中选择一个最可能的，所以衡量 seq2seq 的 loss，可以使用和 classifier 类似的方法。在 transformer 中，我们继续以语音辨识为例，decoder 输出的每一个汉字都与正确答案之间有一个 cross entropy，模型优化的目标就是，使所有输出与正确答案之间的 cross entropy 的总和最小。 3.1 Copy mechanism 有些时候，输出的序列会和输入的序列有重合的部分，例如下面这个 chatbot 的例子： 再比如说文献摘要，让机器来给一篇文献写摘要，摘要中肯定会有与正文内容重复的部分。我们当然是希望机器能够从输入的内容中，直接把这些重合的部分提取出来，而不是自己合成，那这就需要借助 copy mechanism，让机器能够直接从输入中把部分内容 copy 过来。这样的模型当然是存在的，最早的具有复制能力的模型是 copy network，后来还有论文 Incorporating Copy Mechanism in Sequence-to-Sequence Learning 也做过这个，如果感兴趣可以自己去了解。 3.2 Guided attention 机器观察输入，对输入做 attention 的顺序是不固定的，这是机器自己学习的结果，自己学习就会导致问题。例如在 TTS (Text to Speech) 的任务中，有些时候，机器居然会漏字。如果是一般的 chatbot 或者 summarization 问题，那么漏一两个字可能也没什么关系，但在 TTS 中，漏字是非常严重的问题。这个时候，我们可以用 guided attention 来教给机器一个固定的顺序来处理输入。 例如说，在 TTS 中，读入一段文字之后，机器应该从左到右依次做 attention，这样才是正确解决问题的方法。但如果机器颠三倒四，先看后面，再看前面，最后看中间，那显然有些事情就做错了，需要我们人工纠正。 所以 guided attention 就是要求机器的 attention 遵循一个固定的法则，这个法则肯定是我们事先就知道了这个问题的处理方式才得出的，guided attention 也一般只适用于那些有固定解法的问题。 一些关于 guided attention 的关键词汇：monotonic attention、location-aware attention。如果感兴趣可以自行搜索。 3.3 Beam search 继续以语音辨识为例，假设现在的 vocabulary 中只有 A 和 B 两个字，那么我们可以构建出一棵树，每次 decoder 都只面对 A 和 B 两种选择。那么根据原则，在每一个节点处，decoder 会选择分数最高的那个，一直到叶子节点。这种搜索方法叫作 greedy decoding，因为它每次都挑分数最高的。 但是有时候，选择当前分数最高的，未必能选出一条总分数最高的 path，如下图所示： 虽然一开始 B 的分数比 A 低，但是我们可以看到，之后路径上的节点的分数明显比 A 之后路径上节点分数要高，所以总体而言，一开始选分数较低的那个，反而能得到一条更好的 path。但是我们要怎么做才能选出这条 path 呢？一种方法是 dfs，即全部走一遍，但显然这种方法不显示，毕竟中文里的汉字有几千个，不可能用 dfs 来搜索。 那么还有一种方法就是 beam search，它可以找出一条相对好的，但也不是很精准的 path。那么这个算法到底有没有用呢？有趣的是，它有时候有效，有时候就没什么作用。那什么时候有用呢？根据研究，当一个问题有一个比较明确的解时，beam search 就会比较有用；但当一个问题需要发挥机器自己的想象力来完成的时候，beam search 就比较没用。如果感兴趣，可以自行了解。","categories":[{"name":"深度学习","slug":"深度学习","permalink":"https://example.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"transformer","slug":"transformer","permalink":"https://example.com/tags/transformer/"}]},{"title":"自注意力机制 Self-attention","slug":"自注意力机制-Self-attention","date":"2023-09-09T05:45:15.000Z","updated":"2025-04-09T14:28:45.825Z","comments":true,"path":"2023/09/09/自注意力机制-Self-attention/","permalink":"https://example.com/2023/09/09/%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6-Self-attention/","excerpt":"01 复杂输入 不论是在预测问题还是在影像处理上，我们都会假设输入的向量长度是一定的，但如果是不一定的呢？ 比如说在翻译问题中，输入是句子，我们可以采用 one-hot encoding 或 word embedding 的方式将句子转换成向量，而句子是不定长的，也就是说输入的向量是不定长的，这就与我们之前所谈论的情况不一样了。 或者说更复杂一些，输入的向量不仅不定长，而且数量也不确定，如语音辨识，输入是很长一段语音，这段语音中包含了很多句子，机器需要把这些句子拆分出来，然后进行翻译，这种情况下，句子的数量和长度都不是确定的。 还有一些更复杂的数据结构，比如说图，例如社交网络、分子结构等，这些信息也可以看作是一组向量，所有这些情况就构成了更加复杂的输入 (more sophisticated input)，那么我们要如何处理这些输入呢？","text":"01 复杂输入 不论是在预测问题还是在影像处理上，我们都会假设输入的向量长度是一定的，但如果是不一定的呢？ 比如说在翻译问题中，输入是句子，我们可以采用 one-hot encoding 或 word embedding 的方式将句子转换成向量，而句子是不定长的，也就是说输入的向量是不定长的，这就与我们之前所谈论的情况不一样了。 或者说更复杂一些，输入的向量不仅不定长，而且数量也不确定，如语音辨识，输入是很长一段语音，这段语音中包含了很多句子，机器需要把这些句子拆分出来，然后进行翻译，这种情况下，句子的数量和长度都不是确定的。 还有一些更复杂的数据结构，比如说图，例如社交网络、分子结构等，这些信息也可以看作是一组向量，所有这些情况就构成了更加复杂的输入 (more sophisticated input)，那么我们要如何处理这些输入呢？ 02 输出是什么 除了更加复杂的输入，输出也分很多种情况。 对于词性分析问题，它的目标是将一句话中的每个单词的词性进行分类，这个时候，每一个向量都有一个 label；再比如语音辨识问题，机器需要把一段语音中的很多个 frame 转换成对应词；还有图的例子，例如在一个社交网络中，机器需要对图中的人进行分类，这些都是输入与输出等长的例子，也就是 N → N 的情况。 另一种情况是，对于一整个输入，机器只需要输出一个 label 就好了，例如情感分析，输入是一个句子向量，机器需要对句子中所表露出的情感进行判断，究竟是正面还是负面；再比如语者辨认，根据输入的语音判断是哪个人讲的，这些都是 N → 1 的情况。 还有一种情况则更复杂，那就是输出不确定的情况。例如说翻译，输入的文本长度跟输出的文本长度并不存在必然联系，输出的长度应该由机器自己决定，这种情况下就变成了 N → N′。 03 FCNN的缺陷 在讨论完了所有输入和输出的情况之后，我们来看看 FCNN 有什么缺点。在学过 FCNN 和 CNN 之后我们知道，FCNN 是 bias 最小的神经网络，它几乎可以拟合出任何函数，但是这表明它就是无敌的吗？未必，我们来看看下面这个例子。 Please translate: I saw a saw. 在上面这个翻译任务中出现了两个相同的单词 saw，显然这两个 saw 的意思是不一样的，但对于 FCNN 来讲，它们没有任何差别，也就是说，FCNN 无法考虑上下文的关系，当它处理同一个单词 saw 的时候，它并不会输出两个不同的答案。 那么难道 FCNN 就无法处理这种情况了吗？当然是可以的，不过要改变一下方法。我们可以设置一个 window，这个 window 包含了要翻译的词以及其前后的上下文，window 越大，其包含的上下文信息就越多。如果我们要 FCNN 考虑整个句子的长度，那么我们就必须开一个足够大的 window 把整个句子都盖住。但正如前面所讨论的，输入的长度可能是不定的，如果要这么做，我们就必须提前调查一下所有句子中最长的句子的长度，然后把 window 的大小设置成这个长度才有可能盖住所有句子。然而这样做，不仅会使参数增多，使运算量增大，还容易 overfitting。 那么到底有没有更好一点的做法呢？当然有，这就是接下来要讲的 self-attention。 04 自注意力机制 Self-Attention 针对上面的问题，self-attention 的做法是，将输入的向量转换成另一个向量，这些转换后的向量是考虑了上下文之后的向量，然后再将这些向量送给 FCNN 就行了。 当然，self-attention 不止可以用一次，可能经过第一次 self-attention 之后 FCNN 已经有了输出，然后我们再用一次 self-attention 再转换一次，然后再交给另一个 FCNN 处理，最后得到结果。 现在我们已经知道 self-attention 做的主要工作就是将一个向量转换成一个考虑了所有向量之后的向量，也就是如下图所示的过程： 上图中的 bi 就是由下层的 ai 在经过 self-attention 考虑了 $\\sum_{i=1}^4a^i$ 之后得到的结果，那么这一步具体要怎么做呢？ 我们现在以 b1 为例讲述求解 b 的过程。我们要做的是，在 a1 ∼ a4 整个序列中找出与 a1 有关的，能作为 a1 的 label 的判断依据的向量，为此，我们需要给两个序列之间的关联性一个度量，取名叫 α，那么接下来的问题就是如何计算 α。 05 相关性指数 α α 的计算方式主要有两种：dot product 和 additive。 5.1 Dot-product Dot-product 是最常用的方法。它的做法是： 将两个向量输入，将两个向量分别乘上不同的矩阵 Wq, Wk，得到两个新向量 q 和 k，再将这两个新向量进行点乘，得到的结果就是这两个向量的关联度 α。Dot-product 的公式是： Attention (Q,K,V) = softmax (QKT)V 上面的公式已经是注意力机制的完整公式，只有 QKT 这一部分是 dot-product。 5.2 Scaled dot-product Scaled dot-product 其实就是对 dot-product 进行了放缩，最后除了个常量。这个常量记作 dk，其值等于矩阵的维度。这样做的原因在论文中是这样解释的： We suspect that for large values of dk, the dot products grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients. To counteract this effect, we scale the dot products by $1/\\sqrt{d_k}$. 当运算的矩阵的维度过高时，可能会导致点乘的结果过大或过小，这样的话，后续将结果放进 softmax 的时候会导致其分布到函数的两端 (softmax 的函数图像的两端梯度都很小)，梯度比较小，可能会出现梯度消失的问题。除以 $\\sqrt{d_k}$ 进行放缩，可以使结果分布到函数图像的中间。Scaled dot-product 的公式是： $$ \\operatorname{Attention}(Q,K,V)=\\operatorname{softmax}(\\frac{QK^T}{\\sqrt{d_k}})V $$ 注意：上面的公式已经是注意力机制的完整公式，只有 $\\frac{QK^T}{\\sqrt{d_k}}$ 这一部分是 scaled dot-product。 5.3 Additive Additive 的做法的第一步和 dot-product 是一样的：将两个向量输入，将两个向量分别乘上不同的矩阵 Wq, Wk，得到两个新向量 q 和 k。但是接下来不是要将它们做点乘，而是相加，经过一个 activation function，再经过一个 transform，最终得到 α。 下面是这两种方法的图示： 06 注意力 Attention 在知道了如何计算 α 之后，我们选择使用 dot-product 方法，继续来看如何求解 b1。 首先，根据 dot-product 的方法，这里我们要求 a1 和剩下向量的关系，所以我们应该先去求 q1，这个值我们称为 query。根据公式，q1 = Wqa1。 接下来，我们应该去求 k，也就是用 Wk 去乘上剩下的所有向量。这个 k 被我们称作是 key。 在求出 q1 和 k2, k3, k4 之后，将它们分别进行点乘，就能得到 α1, 2，α1, 3，α1, 4，这个 α 也有一个名字，叫作 attention score。 但其实，我们还会用 q1 去乘上自己得到 α1, 1，这也很好理解，毕竟你要考虑 a1 和其他向量的关系，也不能忘了 a1 与自身的关联。 在算出所有的 α1, i 之后，我们会将它们放进一个 soft-max 里进行转换得到 α1, i′： $$ \\alpha^\\prime_{1,i}=\\frac{\\operatorname{exp}(\\alpha_{1,i})}{\\sum_j\\operatorname{exp}(\\alpha_{1,j})} $$ 这一步其实是 normalization，至于为什么是 soft-max，只能说这是先人尝试之后的结果，你当然也可以使用 ReLu 之类的函数，但只不过说经过前人的尝试，soft-max 的效果最好。 为什么一定要将 α 放进 softmax 或其他类似的函数里?这是因为，α 代表了 key 和 query 之间的相似性，注意力机制的本质是关注相似性高的，而忽略相似性低的。最后对所有信息进行整合时，我们其实是根据 α 的大小进行加权聚合，相似性高的向量权重就大一点，所以我们需要将所有的 α 放进 softmax 中，以此来得到权重。简单来讲，计算 α 的目的就是为了得到权重，而后面计算出的 v 是每个向量的价值。 在做完这些工作之后，我们就终于可以来计算 b1 了。首先，我们需要将 a1 ∼ a4 都乘上一个矩阵 Wv，得到 v1 ∼ v4，然后根据下面的公式就能得到 b1 了： b1 = ∑iα1, i′vi 讲完了全部过程，我们再来回顾一下。很显然，如果 a1 与其中某一个向量 ai 的关联度最大，那么 α1, i′ 就会很大，这就会使得最终的 b1 与 vi 更加接近，也就是对 ai 的 attention 最大，这也就得到了考虑了上下文的向量。 Self-attention 里其实并没有做太多的工作，它需要通过 dataset 学习的其实仅仅只有三个参数：Wq，Wk 和 Wv。除此之外的所有参数都是人为设置好的。 如果你想知道以上所有过程的代码思路 (仅仅是思路) 或者说矩阵运算的技巧，可以参考这个视频。 07 多头自注意力 Multi-head Self-attention Self-attention 其实还有很多变体，其中一个在今天应用非常广泛的模型就是 multi-head self-attention。 Multi-head self-attention 的想法是，事物与事物之间的关联性有时候是多方面的，当考虑不同的方面时，两个事物的关联性可能就是不同的，所以我们需要不止一种 α。考虑多少种 α，就有多少个 head。 如果这样考虑的话，根据正常的 self-attention 的做法，原本只需要对每个 ai 计算 qi, ki, vi，现在则还需要针对每个 qi 计算 qi, 1, qi, 2...，也就是考虑多方面的关联性。那既然 q 现在有多个，那么 k 和 v 也肯定要有多个。那至于怎么进一步得到 qi, 1, qi, 2...ki, 1, ki, 2...vi, 1, vi, 2...，其实是用更多的 Wq, Wk, Wv 来乘上原来的 q, k, v，也就是： $$ \\begin{aligned} &amp; \\boldsymbol{q}^{i, \\mathbf{1}}=W^{q, 1} \\boldsymbol{q}^i \\\\ &amp; \\boldsymbol{k}^{i, 2}=W^{k, 2} \\boldsymbol{k}^i\\\\ &amp; \\boldsymbol{v}^{i, 2}=W^{v, 2} \\boldsymbol{v}^i \\end{aligned} $$ 所以，其实 multi-head self-attention 要做的事情和 self-attention 是一样的，只不过现在有多个 head，所以要每个 head 都做一遍独立的 self-attention 而已，最后你能得到多个 bi, j，那接下来你可能会把这些 b 都连接起来形成一个矩阵，然后将其乘上另一个矩阵 Wo，得到最终的 bi。 08 位置编码 Positional Encoding 不知道你看到这里有没有发现 self-attention 的一个缺陷？Self-attention 似乎只在考虑 attention，也就是向量与向量之间的关联，却漏掉了一个很重要的信息 —— 那就是“位置 (position)”！例如某个向量是排在序列的最前面还是最后，它是完完全全没有考虑的，而位置信息很明显，在很多任务中都是很重要的，尤其是对于文字处理而言，比如说，动词出现在句首的概率比较低，那么如果一个词出现在句首，它可能是动词的概率就比较低。 所以怎么办呢？这就是 positional encoding 这项技术的作用，它可以把向量的位置信息给“塞进去”。它给序列中的每一个位置都设定了一个 vector，称为 ei，不同的位置都有一个专属的 e。我们要做的事情是，将这个 ei 加到 ai 上面去，就结束了。没错，就这么简单。 那么这个 e 是如何确定的呢？在 Attention is All You Need 论文中，e 是人为规定 (hand-crafted) 的，他们是使用 sin、cos 这些神奇的函数来得到 e 的，至于可不可以用其他函数，答案当然是可以，positional encoding 目前还是一个尚待研究的问题，所以你用什么都是没问题的；当然，e 也可以是通过 data 学习出来的。 09 Self-attention v.s. CNN Self-attention 同样是可以用于处理图像的。我们知道图像信息也是向量，使用 self-attention 我们就可以去考虑每个 pixel 之间的关联度，让机器自己去筛选出一张图片中重要的信息。当今，使用 self-attention 处理图片已然不是什么很新鲜的事情，那么就会有一个问题：self-attention 和 CNN 孰优孰劣？ 其实，我们可以将 self-attention 当作是一个复杂版的 CNN，而 CNN 是 self-attention 的简化版。我们知道，CNN 每次都只考虑一个 perceptive filed 的信息；而 self-attention 则是通过 pixel 与每个 pixel 之间的关联度，来自动筛选出值得关注的 filed，这就好像是在说 self-attention 的 perceptive filed 是自己学习出来的。 在 On the Relationship between Self-Attention and Convolutional Layers 这篇论文里，作者用数学方法讨论了 CNN 和 self-attention，并且证明了只要参数设置正确，self-attention 完全可以变成 CNN。也就是说，CNN 只是 self-attention 的一种特例。 那么究竟 CNN 和 self-attention 谁更加好呢？显然通过我们上面的讨论，CNN 的 bias 比 self-attention 更大，也就是 CNN 的弹性更加小，能拟合出的函数更加少。而我们也知道，弹性大的网络在面对较小的数据量时很容易 overfitting；弹性小的网络在面对更大的数据量的时候则很难再学到新的东西。也有学者对此做过专门研究，最后发现，当数据量较少时，CNN 的准确率是能超过 self-attention 的，但当数据量到达 100M 级别的时候，CNN 就被 self-attention 超过了。所以 CNN 和 self-attention 到底谁更好是依你的数据量来定的。 10 图中的自注意力机制 Self-attention 也是可以用在 graph 上的，例如下面这张图： 当我们需要去给节点 1 做 label 的时候，我们就可以去考虑它的邻居节点，然后使用 self-attention 对其与邻居节点之间的关联性进行考量，而对于那些不与节点 1 相连的节点则代表我们已经人为地帮机器排除了彼此的联系，所以就不需要机器再去考虑了。这其实就是 GNN 的一种。 11 注意力和自注意力 我们上面介绍的是 self-attention，注意：self-attention (自注意力机制) 和 attention (注意力机制) 是不一样的！我们可以用一个例子来说明注意力机制是什么。以淘宝搜索商品为例，淘宝要做的是，将我们输入的关键词与其数据库内的商品相关联，其本质其实和 self-attention 很像，都是找关联性强的，忽略关联性弱的，但不同的是，在这个例子中，我们的 q, k, v 来自 source (商品) 和 target (关键词)，它们位于 transfomer 架构的两端；而在上面举的机器翻译的例子中，source 是输入的中文，target 是要求解的英文翻译，它们位于 transfomer 的两端，但 q, v, k 只来自 source。","categories":[{"name":"深度学习","slug":"深度学习","permalink":"https://example.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"attention","slug":"attention","permalink":"https://example.com/tags/attention/"}]},{"title":"Flask入门","slug":"Flask","date":"2023-03-14T12:15:18.000Z","updated":"2025-04-13T06:16:20.785Z","comments":true,"path":"2023/03/14/Flask/","permalink":"https://example.com/2023/03/14/Flask/","excerpt":"","text":"01 初识FLASK 先来解析一下FLASK的基本框架，新建一个FLASK项目，打开文件下的app.py，都会出现下面这样的骨架： 12345678910from flask import Flaskapp = Flask(__name__)@app.route(&#x27;/&#x27;)def hello_world(): return &quot;Hello Flask!&quot; if __name__ == &#x27;__main__&#x27;: app.run() 我们来逐行解释一下： from flask import Flask，这句话引入了flask包中Flask这个类 app = Flask(__name__)则是创建了一个Flask类的实例化对象，这个对象就表示一个实例化程序 @app.route('/')是设置URL，其中'/'是根路由 02 模板渲染 我们可以看到，只是单纯返回一个字符串是不能做出正式的网页效果的，我们需要的是返回一整个html，这要借助模板渲染工具render_template。 在我们的文件目录下，有一个文件叫templates，我们应该把所有的html放在这个目录下，然后如下使用render_template进行渲染。 1234567891011from flask import Flask, render_templateapp = Flask(__name__)@app.route(&#x27;/&#x27;)def hello_world(): return render_template(&quot;index.html&quot;)if __name__ == &#x27;__main__&#x27;: app.run() render_template还可以传参。有时候，我们模板中需要用到用户传过来的参数，这个时候就需要用到render_template的传参功能。 1234567891011from flask import Flask, render_templateapp = Flask(__name__)@app.route(&#x27;/&lt;int:blog_id&gt;&#x27;)def hello_world(blog_id): return render_template(&quot;index.html&quot;, blog_id=blog_id)if __name__ == &#x27;__main__&#x27;: app.run() 1234567891011&lt;!DOCTYPE html&gt;&lt;html lang=&#x27;en&#x27;&gt; &lt;head&gt; &lt;meta charset=&quot;UTF-8&quot;&gt; &lt;title&gt;blog_detail&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;!--在html中，输入两队花括号就可以使用参数--&gt; &lt;h1&gt;You are viewing blog &#123;&#123;blog_id&#125;&#125;&lt;/h1&gt; &lt;/body&gt;&lt;/html&gt; 03 模板访问对象属性 上节中我们讲到了模板可以传参，如果参数是对象呢？ 如下为例，我们定义了一个User类和一个person字典，并创建了User类的一个对象user，将user和person传给了index模板 12345678910111213141516171819202122232425from flask import Flask, render_templateapp = Flask(__name__)class User: def __init__(self, name, email): self.name = name self.email = email@app.route(&#x27;/&#x27;)def hello_world(): ## put application&#x27;s code here user = User(name=&#x27;coda&#x27;, email=&#x27;xxx@outlook.com&#x27;) person = &#123; &#x27;username&#x27;: &#x27;jonas&#x27;, &#x27;qq&#x27;: 21321321 &#125; return render_template(&quot;index.html&quot;, user=user, person=person)if __name__ == &#x27;__main__&#x27;: app.run() 于是我们就能在index.html里访问user和person了 123456789101112131415&lt;!DOCTYPE html&gt;&lt;html lang=&quot;en&quot;&gt;&lt;head&gt; &lt;meta charset=&quot;UTF-8&quot;&gt; &lt;title&gt;Title&lt;/title&gt;&lt;/head&gt;&lt;body&gt; &lt;h1&gt;这是一个博客!&lt;/h1&gt; &lt;h2&gt;姓名：&#123;&#123; user.name &#125;&#125;&lt;/h2&gt; &lt;h2&gt;邮箱：&#123;&#123; user.email &#125;&#125;&lt;/h2&gt; &lt;!--字典的访问方式有两种：1. 使用[]下标访问 2. 跟类对象一样使用.访问--&gt; &lt;h3&gt;别名：&#123;&#123; person.username &#125;&#125;&lt;/h3&gt; &lt;h3&gt;QQ：&#123;&#123; person[&#x27;qq&#x27;] &#125;&#125;&lt;/h3&gt;&lt;/body&gt;&lt;/html&gt; 效果： 04 过滤器的使用 过滤器的本质是Python的函数，他会把被过滤的值当作第一个参数传给这个函数，函数经过一些逻辑处理后，再返回新的值。 jinja2已经内置了三十几个过滤器供使用，详见https://www.osgeo.cn/jinja/templates.html#list-of-builtin-filters。 内置过滤器的使用示例下示代码以内置的length过滤器为例： 1234567891011121314151617181920from flask import Flask, render_template ## 从flask这个包中导入Flask这个类app = Flask(__name__)class User: def __init__(self, name, email): self.name = name self.email = email@app.route(&#x27;/filter&#x27;)def filter_demo(): user = User(name=&#x27;coda&#x27;, email=&#x27;xxx@outlook.com&#x27;) return render_template(&#x27;index.html&#x27;, user=user)if __name__ == &#x27;__main__&#x27;: app.run() 12345678910111213&lt;!DOCTYPE html&gt;&lt;html lang=&quot;en&quot;&gt;&lt;head&gt; &lt;meta charset=&quot;UTF-8&quot;&gt; &lt;title&gt;Title&lt;/title&gt;&lt;/head&gt;&lt;body&gt; &lt;h1&gt;这是一个博客!&lt;/h1&gt; &lt;h2&gt;姓名：&#123;&#123; user.name &#125;&#125;&lt;/h2&gt; &lt;!--length可以求字符长度--&gt; &lt;h2&gt;姓名长度：&#123;&#123; user.name|length &#125;&#125;&lt;/h2&gt;&lt;/body&gt;&lt;/html&gt; 效果： 自定义过滤器下面，我们来看一些如何自定义自己的过滤器。 首先，我们要定义一个函数，这个函数当然会有一个参数，还要有返回值，这里以一个简单的反转字符串函数为例： 1234567891011121314151617181920212223from flask import Flask, render_template ## 从flask这个包中导入Flask这个类app = Flask(__name__)class User: def __init__(self, name, email): self.name = name self.email = emaildef reverse_str(s): return s[::-1]app.add_template_filter(reverse_str, &#x27;reverse&#x27;)@app.route(&#x27;/filter&#x27;)def filter_demo(): user = User(name=&#x27;coda&#x27;, email=&#x27;xxx@outlook.com&#x27;) return render_template(&#x27;index.html&#x27;, user=user)if __name__ == &#x27;__main__&#x27;: app.run() 定义好之后，要使用app.add_template_filter来注册过滤器，第一个参数是函数名,第二个参数是过滤器名。 然后就可以在html模板里使用这个过滤器了。 123456789101112&lt;!DOCTYPE html&gt;&lt;html lang=&quot;en&quot;&gt;&lt;head&gt; &lt;meta charset=&quot;UTF-8&quot;&gt; &lt;title&gt;Title&lt;/title&gt;&lt;/head&gt;&lt;body&gt; &lt;h1&gt;这是一个博客!&lt;/h1&gt; &lt;h2&gt;姓名：&#123;&#123; user.name &#125;&#125;&lt;/h2&gt; &lt;h2&gt;姓名反转：&#123;&#123; user.name|reverse &#125;&#125;&lt;/h2&gt;&lt;/body&gt;&lt;/html&gt; 效果： 05 控制语句 jinja2中提供了和python一样的控制语句，使能在前端使用分支和循环。在html中使用控制语句要使用 &#123;% %&#125;结构，且和python不一样的是，jinja2中的if和for必须要以&#123;% endif %&#125;和&#123;% endfor %&#125;结束。 示例： 123456789101112131415161718192021222324252627282930from flask import Flask, render_template ## 从flask这个包中导入Flask这个类app = Flask(__name__)@app.route(&#x27;/control/&lt;int:age&gt;&#x27;)def control_demo(age): books = [ &#123; &#x27;title&#x27;: &#x27;百年孤独&#x27;, &#x27;author&#x27;: &#x27;马尔克斯&#x27;, &#x27;genre&#x27;: &#x27;小说&#x27; &#125;, &#123; &#x27;title&#x27;: &#x27;红楼梦&#x27;, &#x27;author&#x27;: &#x27;曹雪芹&#x27;, &#x27;genre&#x27;: &#x27;小说&#x27; &#125;, &#123; &#x27;title&#x27;: &#x27;电锯人&#x27;, &#x27;author&#x27;: &#x27;藤本树&#x27;, &#x27;genre&#x27;: &#x27;漫画&#x27; &#125; ] return render_template(&#x27;control.html&#x27;, age=age, books=books)if __name__ == &#x27;__main__&#x27;: app.run() 1234567891011121314151617181920212223&lt;!DOCTYPE html&gt;&lt;html lang=&quot;en&quot;&gt;&lt;head&gt; &lt;meta charset=&quot;UTF-8&quot;&gt; &lt;title&gt;Title&lt;/title&gt;&lt;/head&gt;&lt;body&gt; &#123;% if age &gt; 18 %&#125; &lt;div&gt;您的年龄已大于18岁，允许进入网吧！&lt;/div&gt; &#123;% elif age &lt; 18 %&#125; &lt;div&gt;您尚未成年，禁止进入网吧！&lt;/div&gt; &#123;% else %&#125; &lt;div&gt;您刚满18岁，请在父母的陪同下进入网吧！&lt;/div&gt; &#123;% endif %&#125; &lt;hr&gt; &#123;% for book in books %&#125; &lt;h3&gt;书名: &#123;&#123; book.title &#125;&#125;&lt;/h3&gt; &lt;h3&gt;作者: &#123;&#123; book.author &#125;&#125;&lt;/h3&gt; &lt;h3&gt;类型: &#123;&#123; book.genre &#125;&#125;&lt;/h3&gt; &lt;hr&gt; &#123;% endfor %&#125;&lt;/body&gt;&lt;/html&gt; 效果： 注意：在jinja2的循环控制语句中，不存在break这种语句，无法提前跳出循环！ 06 模板继承 引入static文件夹中的图片、css、js文件可以使用url_for，引入图片的语句是&lt;img src=\"&#123;&#123; url_for('static', filename='img/wall1.jpg') &#125;&#125;\" alt=\"\" width=\"400px\"&gt;，filename是文件路径，默认就是从static文件找，所以不需要加static/；引入css的语句是&lt;link rel=\"stylesheet\" href=\"&#123;&#123; url_for('static', filename='css/style.css') &#125;&#125;\"&gt;；引入js的语句是&lt;script src=\"&#123;&#123; url_for('static', filename='js/test.js') &#125;&#125;\"&gt;&lt;/script&gt;。 示例： 12345678910111213141516171819from flask import Flask, render_template ## 从flask这个包中导入Flask这个类app = Flask(__name__)@app.route(&#x27;/&#x27;)def hello_world(): ## put application&#x27;s code here user = User(name=&#x27;coda&#x27;, email=&#x27;xxx@outlook.com&#x27;) person = &#123; &#x27;username&#x27;: &#x27;jonas&#x27;, &#x27;qq&#x27;: 21321321 &#125; return render_template(&quot;index.html&quot;, user=user, person=person)@app.route(&#x27;/static&#x27;)def static_demo(): return render_template(&#x27;static.html&#x27;)if __name__ == &#x27;__main__&#x27;: app.run() 123456789101112&lt;!DOCTYPE html&gt;&lt;html lang=&quot;en&quot;&gt;&lt;head&gt; &lt;meta charset=&quot;UTF-8&quot;&gt; &lt;title&gt;Title&lt;/title&gt; &lt;link rel=&quot;stylesheet&quot; href=&quot;&#123;&#123; url_for(&#x27;static&#x27;, filename=&#x27;css/style.css&#x27;) &#125;&#125;&quot;&gt; &lt;script src=&quot;&#123;&#123; url_for(&#x27;static&#x27;, filename=&#x27;js/test.js&#x27;) &#125;&#125;&quot;&gt;&lt;/script&gt;&lt;/head&gt;&lt;body&gt; &lt;img src=&quot;&#123;&#123; url_for(&#x27;static&#x27;, filename=&#x27;img/wall1.jpg&#x27;) &#125;&#125;&quot; alt=&quot;&quot; width=&quot;400px&quot;&gt;&lt;/body&gt;&lt;/html&gt; 效果： 07 加载静态文件 引入static文件夹中的图片、css、js文件可以使用url_for，引入图片的语句是&lt;img src=\"&#123;&#123; url_for('static', filename='img/wall1.jpg') &#125;&#125;\" alt=\"\" width=\"400px\"&gt;，filename是文件路径，默认就是从static文件找，所以不需要加static/；引入css的语句是&lt;link rel=\"stylesheet\" href=\"&#123;&#123; url_for('static', filename='css/style.css') &#125;&#125;\"&gt;；引入js的语句是&lt;script src=\"&#123;&#123; url_for('static', filename='js/test.js') &#125;&#125;\"&gt;&lt;/script&gt;。 示例： 1234567891011121314151617181920212223from flask import Flask, render_template ## 从flask这个包中导入Flask这个类app = Flask(__name__)@app.route(&#x27;/&#x27;)def hello_world(): ## put application&#x27;s code here user = User(name=&#x27;coda&#x27;, email=&#x27;xxx@outlook.com&#x27;) person = &#123; &#x27;username&#x27;: &#x27;jonas&#x27;, &#x27;qq&#x27;: 21321321 &#125; return render_template(&quot;index.html&quot;, user=user, person=person)@app.route(&#x27;/static&#x27;)def static_demo(): return render_template(&#x27;static.html&#x27;)if __name__ == &#x27;__main__&#x27;: app.run() 123456789101112&lt;!DOCTYPE html&gt;&lt;html lang=&quot;en&quot;&gt;&lt;head&gt; &lt;meta charset=&quot;UTF-8&quot;&gt; &lt;title&gt;Title&lt;/title&gt; &lt;link rel=&quot;stylesheet&quot; href=&quot;&#123;&#123; url_for(&#x27;static&#x27;, filename=&#x27;css/style.css&#x27;) &#125;&#125;&quot;&gt; &lt;script src=&quot;&#123;&#123; url_for(&#x27;static&#x27;, filename=&#x27;js/test.js&#x27;) &#125;&#125;&quot;&gt;&lt;/script&gt;&lt;/head&gt;&lt;body&gt; &lt;img src=&quot;&#123;&#123; url_for(&#x27;static&#x27;, filename=&#x27;img/wall1.jpg&#x27;) &#125;&#125;&quot; alt=&quot;&quot; width=&quot;400px&quot;&gt;&lt;/body&gt;&lt;/html&gt; 效果：","categories":[{"name":"技术开发","slug":"技术开发","permalink":"https://example.com/categories/%E6%8A%80%E6%9C%AF%E5%BC%80%E5%8F%91/"},{"name":"后端","slug":"技术开发/后端","permalink":"https://example.com/categories/%E6%8A%80%E6%9C%AF%E5%BC%80%E5%8F%91/%E5%90%8E%E7%AB%AF/"}],"tags":[{"name":"web","slug":"web","permalink":"https://example.com/tags/web/"},{"name":"flask","slug":"flask","permalink":"https://example.com/tags/flask/"},{"name":"后端","slug":"后端","permalink":"https://example.com/tags/%E5%90%8E%E7%AB%AF/"}]},{"title":"机器学习入门 吴恩达","slug":"机器学习入门-吴恩达","date":"2022-07-21T08:26:16.000Z","updated":"2025-04-12T04:50:57.875Z","comments":true,"path":"2022/07/21/机器学习入门-吴恩达/","permalink":"https://example.com/2022/07/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8-%E5%90%B4%E6%81%A9%E8%BE%BE/","excerpt":"§ Introduction to Machine Learning 01 Definition Machine Learning: Field of study that gives computers the ability to learn without being explicitly programmed. — Arthur Samuel Well-posed Learning Problem: A compouter program is said to learn from experience E with respect to some task T and some performance measure P, if its performance on T, as measured by P, imporoves with experience E. — Tom Mitchell Machine Learning algorithms: Supervised learning Unsupervised learning Others: Reinforcement learning, recommender systems","text":"§ Introduction to Machine Learning 01 Definition Machine Learning: Field of study that gives computers the ability to learn without being explicitly programmed. — Arthur Samuel Well-posed Learning Problem: A compouter program is said to learn from experience E with respect to some task T and some performance measure P, if its performance on T, as measured by P, imporoves with experience E. — Tom Mitchell Machine Learning algorithms: Supervised learning Unsupervised learning Others: Reinforcement learning, recommender systems 02 Two Types of Learning There are two major types of learning —— supervised learning and unsupervised learning. 2.1 Supervised Learning Definition: The term supervised learning refers to the fact that we give the algorithm a dataset in which the “right answer” are given. Category Regression: Predict continuous valued output Classification: Give discrete valued outout 2.2 Unsupervised learning Definition: Give the algorithm a bunch of data without any explicit instruciton or imformation, but require it to categorize the data into different clusters automatically. 03 Linear Regression Definition Regression analysis is a statistical analysis method to determine the interdependent quantitative relationship between two or more variables. According to the number of variables involved, regression analysis can be divided into univariate regression and multivariate regression analysis. The following example is a classic linear regression problem —— Portland housing price prediction. The exact figures are examplified like: We are going to analyse the above data of housing prices from the city of Portland, Oregan to predict the posibble price of a house according to its size based on the analysis result. It’s a clear supervised learning (regression) problem. So the algorithm is aimed to find a hypothesis function through quantities of data, and the function will map x (size of house) to y (estimated price) to get the correct answer. The whole process can be illustrated by the graph below. For there is only one variate (size of house) in this model, we call it univariate linear regression. 04 Cost Function Let’s continue with the example of housing price prediction. Assume the m equals 47, which represents the number of training examples. And the hypothesis function is as below (a univariate linear function): hθ(x) = θ0 + θ1x These θ0 and θ1 are called the parameters of the model. So the idea is to choose the appropriate θ0 and θ1 so that hθ(x) can be approximated to the y for our examples (x,y), which is to say that we want the difference between h(x) and y to be small, i.e. minimize $$ J(\\theta_0,\\theta_1)=\\frac{1}{2m}\\sum_{i=1}^m(h_\\theta(x^{(i)})-y^{(i)})^2 $$ The above function J(θ0,θ1) is what we called Cost Function. And our goal is to minimize its result by finding the best values for θ0 and θ1. The relation between the hypothesis function and the cost function: The hypothesis function hθ(x) is a function of x for fixed θ, while the cost function J(θ0,θ1) is of the parameter θ0 and θ1. By J(θ0,θ1) we obtain the accurate θ we want so that we can figure out hθ(x), which is our final goal. 05 Gradient Descent Let’s visualize the funtction J(θ0,θ1) to better understand it: Imagine that the figure is a huge mountain and you are standing on one point of it. Then think: which direction should I step in if I want to physically walk down this mountain as quickly as possible? Once you have determined the direction, you take your step and then stop to think again: what direction should I take that step in next? Keep walking and keep asking until you converge to this local minimum as the following figure shows. The whole process is what we called Gradient Descent. The following algorithm gives the mathematical form of gradient descent, which is not difficult to understand so I skip its explaination. 06 Gradient Descent for Linear Regression What if we want to apply gradient descent to linear regression? It is pretty easy actually. All we need to do is to take the derivatives of the θ0 and θ1 in the cost function separately and substitute the derivatives into the gradient descent algorithm. The algorithm below shows the gradient descent for linear regression. When we implement the algorithm, the cost function is approaching the optimum and the hypothesis function is getting more and more in line with our dataset. The illustration below shows the visualization of the process. If we accumulate the gradients of a small batch of data samples to update the parameters at once, then we get Batch Gradient Descent. § Linear Regression with Multiple Variables 01 Multiple Features 1.1 Introduction In the previous sections, we’ve talked about linear regression with one variate. What if we got more variates? Multiple variates mean we will have multiple features. In the previous section, we only have one feature and that is the size of the house. Now we let’s discuss the multi-feature situation. 1.2 Multiple Features(variables) In the above case, we have the data of the house’s bedrooms, floors and age and hopefully to predict the possible price of the house. All of this four numbers are called features. For there are four features, the input x(i) will be a four-dimensional vector. We use n to represent the number of features here, x(i) as the input of ith training example and xj(i) as the value of feature j in ith training example. Given that we have multiple features now, the previous hypothesis function is no longer appropriate. Here is the rewritten one: hθ(x) = θ0 + θ1x1 + θ2x2 + · · · + θnxn The above function is clearly too complicated. May we simplify it? 1.3 Simplication of hθ(x) For convenience of notation, we define x0 = 1, which is the equivalent to an additional feature. So previously we have n features but now we’ve got n + 1 whereas we are defining an additional sort of zero feature vector that always takes on the value of 1. So now we have a (n+1)-dimensional indexed from 0 to n. And we are also going to think of our parameters as a vector. They will be like below: $$ X=\\begin{bmatrix} x_{0}\\\\ x_{1}\\\\ x_{2}\\\\ ···\\\\ x_{n}\\\\ \\end{bmatrix} \\;\\;\\;\\;\\;\\;\\; \\theta=\\begin{bmatrix} \\theta_{0}\\\\ \\theta_{1}\\\\ \\theta_{2}\\\\ ···\\\\ \\theta_{n}\\\\ \\end{bmatrix} $$ So the hypothesis function can be rewritten like: hθ(x) = θ0x0 + θ1x1 + θ2x2 + · · · + θnxn (x0=1) If you are familiar with the vector multiplication, you will know it is also equal to this: hθ(x) = θTX 02 Feature Scaling 2.1 Introduction Imagine you have two features — x1 and x2. The difference btween this two numbers are really really large, for example, x1 ranges from 0 to 2000 while x2 ranges from 1 to 5. That way the contour of the cost function can take on this sort of very skewed elliptical shape: Thus your gradients may oscillate back and forth and end up taking a long time before it can finally find its way to the global minimum: In this setting, a useful thing to do is to scale the features. For example, the feature x1 is devided by 2000 and feature x2 is devided by 5, so that the contour will be much less skewed and your gradients will find a direct way to the global minimun instead of following a much more complicated trajectory. This process is what we called Feature Scaling, i.e. get every feature into approximately a − 1 ≤ xi ≤ i range. 2.2 Mean Normalization Replace xi with xi − μi，(μ is the average) to make features have approximately zero mean (Do not apply to x0 = 1) For example, if you know the average size of a house is equal to 1000, you might use this formula set the feature x1 to be size minus the average value 1000 divided by 2000. Similarily, if every house has 1 to 5 bedrooms and on average a house has 2 bedrooms, you might use this formula set the feature x2 to be size minus the average value 2 divided by 5. 03 Learning Rate 3.1 Debugging Making sure gradient descent is working correctly. 3.1.1 Method 1 What we usually do is to plot the cost function J(θ) as gradient descent runs. Hopefully J(θ) will decrease after every iteration of gradient descent and its plot will flatten eventually, which means gradient descent has more or less converged because your cost function isn’t going down much more. So looking at this figure can help you judge whether or not gradient descent has converged. If the plot is increasing like above, that means you gradient descent is not working normally. Sometimes it means you should be using a tinier learning rate. 3.1.2 Method 2 It’s also possible to come up with automatic convergence test, namely to have an algorithm to try to tell you if gradient descent has converged. Example automatic convergence test: Declare convergence if J(θ) decrease by less than 10−3 in one iteration. Here 10−3 is a threshold. But this threshold is not fixed and sometimes it’s hard to choose the appropriate threshold. So method 1 may be more practical. 3.2 Learning Rate In the situation we’ve meantioned above, we said that if the value of the cost function keep increasing, it’s often because the learning rate is too big. Now let me explain why. If your learning rate is too big, gradient descent may overshoot the minimum over and over again and deviate from the minimum, so that you will end up getting the higher value of the cost function. If learning rate is too small, the convergence can be very slow. 04 Normal Equation 4.1 Introduction In the previous sections, we’ve introduced an iterative algorithm that takes many steps, multiple iterations of gradient descent to converge to the global minimum. In contrast, normal equation would give us a method to solve for θ analytically. So rather than run the iterative algorithm, we can instead just solve for the optimal value for θ all at one go. Mathematically, to work out the minimum of a multivariate funtion is to take its partial deviratives and set them equal to zero, which can be somewhat involved. 4.2 Explaination Assume we have a dataset: We take all the number x0 to x4 to construct a m * (n+1) matrix like: $$ X=\\begin{bmatrix} 1 &amp; 2104 &amp; 5 &amp; 1 &amp; 45\\\\ 1 &amp; 1416 &amp; 3 &amp; 2 &amp; 40\\\\ 1 &amp; 1534 &amp; 3 &amp; 2 &amp; 30\\\\ 1 &amp; 852 &amp; 2 &amp; 1 &amp; 36\\\\ \\end{bmatrix} $$ And then do the similar thing to y to construct a m-dimensional vector like: $$ y=\\begin{bmatrix} 460\\\\ 232\\\\ 315\\\\ 178\\\\ \\end{bmatrix} $$ And we can get the values of θ by the formula: θ = (XTX)−1XTy We use x(1) to represent the ith features and y(i) to represent the ith price; then (x(i),y(i)) forms an example. x(i) will be like: $$ x^{(i)}=\\begin{bmatrix} x^{(i)}_0\\\\ x^{(i)}_1\\\\ x^{(i)}_2\\\\ ...\\\\ x^{(i)}_i\\\\ \\end{bmatrix} $$ Here we are going to construct a matrix X called Design Matrix: $$ X_{m*(n+1)}=\\begin{bmatrix} (x^{(1)})^T\\\\ (x^{(2)})^T\\\\ (x^{(3)})^T\\\\ ...\\\\ (x^{(m)})^T\\\\ \\end{bmatrix} $$ For instance, if x(i) is: $$ x^{(i)}=\\begin{bmatrix} 1\\\\ x^{(i)}_1\\\\ \\end{bmatrix} $$ Then the design matrix will be like: $$ X=\\begin{bmatrix} (x^{(1)})^T\\\\ (x^{(2)})^T\\\\ (x^{(3)})^T\\\\ ...\\\\ (x^{(m)})^T\\\\ \\end{bmatrix} =\\begin{bmatrix} 1 &amp; x^{(1)}_1\\\\ 1 &amp; x^{(2)}_1\\\\ 1 &amp; x^{(3)}_1\\\\ ... &amp; ...\\\\ 1 &amp; x^{(m)}_1\\\\ \\end{bmatrix} $$ The by the formula above: θ = (XTX)−1XTy We can obtain all the values of θ. 4.3 Gradient Descent v.s. Normal Equation Gradient Descent: Needs to choose α Needs many iterations Works well even when n is large Normal Equation No need to choose α Don’t need to iterate Need to compute (XTX)−1 Slow if n is very large § Logistic Regression 01 Introduction Linear Regression is a good tool to help solve the regression problem, which predicts continuous valued output. However, it won’t be a good idea if we apply it to the classification problem, which gives discrete valued output. That’s because linear regression can be susceptible to outlier data, rendering the result inaccurate. So here we introduce a new method to cope with the classification problem — Logistic Regression. 02 Hypotheses Representation 2.1 Logistic Regression Model The value of the hypothesis function in classification problem should be between 0 ~ 1: 0 ≤ hθ(x) ≤ 1 As the previous sections have shown, hθ(x) = θTx. Now we are going to make the hypothesis equal g(θTx), where we define the function g as follows: $$ g(z)=\\frac{1}{1+e^{(-z)}} $$ This function is what we called Sigmoid Funtion or Logistic Function, where Logistic Regression gains its name. Let’s plug the parameter back into it. Then there is an alternative way writing out the form of our hypothesis: $$ h_\\theta(x)=\\frac{1}{1+e^{-\\theta^Tx}} $$ Its curve looks like: As you can see, the sigmoid function approaches 1 as its parameter z approaches infinity, and approaches 0 as its parameter approaches infinitesimal. 2.2 Interpretation of Hypothesis Output $$ h_\\theta(x)=\\text{estimated\\;probability\\;of\\;}y=1\\;\\text{on\\;input}\\;x $$ Example: If $x= = $, hθ(x) = 0.7 Tell patient that 70% chance of tumor being malignant. 03 Decision Boundary 3.1 Recap: Logistic Regression Viewing the plot of our hypothesis function, we can easily find that its value is greater than or equal to 0.5 whenever the parameter is positive, while it’s less than 0.5 whenever the parameter is negative. 3.2 Decision Boundary Suppose we have a dataset and hypothesis function like: Concretely, we define the parameters θ to be: $$ \\theta=\\begin{bmatrix} -3\\\\ 1\\\\ 1\\\\ \\end{bmatrix} $$ Now let’s figure out where a hypothesis will end up predicting y = 1 and where it will end up predicting y = 0. Using the conclusion we’ve drawn in the recap, we know that y is more likely equal to 1, that is the probability that y equals 1 is greater than or equal to 0.5, whenever θTx is greater then zero: predict y = 1 if − 3 + x1 + x2 &gt; = 0 That’s equivalent to x1 + x2 ≥ 3. So if we draw a plot of x1 + x2 = 3, it’s basically like: And we call that line Decision Boundary. And the x1, x2 above the line corrspond with the y equal to 1, while in contrast, the x1, x2 fall below the line correspond with the y euqal to 0. Just to be clear, the decision boundary is a property of the hypothesis including the parameters θ rather than the dataset, which means even if we change all the data, as long as the hypothesis remains the same, the boundary remains the same. 3.3 Non-linear decision boundaries In the above case, we define the parameters θ to be [−1,0,0,1,1]. And still we can draw a line to separate thoses x but clearly it’s more complicated than just a simple line. The decision boundary can be much more complex especially when it comes to higher order polynomial terms. But no matter how complex it is, it’s still a property of the hypothesis instead of the training set. 04 Cost Function 4.1 Why the Former Won’t Work Back to when we were developing a linear regression model, we use the following cost function: $$ J(\\theta)=\\frac{1}{m}∑^m_{i=1}\\frac{1}{2}(h_θ(x^{(i)})-y^{(i)})^2 $$ This time, we are going so change it a little bit. Let’s define $$ Cost(h_θ(x^{(i)}),y^{(i)})=\\frac{1}{2}(h_θ(x^{(i)})-y^{(i)})^2 $$ It seems like this cost function that works for linear regression can still be applied to logistic regression, but however, it’s not quite appropriate, because if we plug $h_\\theta(x)=\\frac{1}{1+e^{-\\theta^Tx}}$in the function, you will find that the function is not convex. Usually, when a function is convex, its plot is bowl-shaped, which is easy to spot the global minimum. However when it’s non-convex, there will be many local optimums, making it difficult for gradient descent to converge to the global minimum. So we are going to choose another function better suitable for logistic regression. 4.2 Logistic Regression Cost Function So we give the cost function for logistic regression: $$ Cost(h_θ(x),y)=\\begin{cases} -log(h_θ(x)) &amp; if\\;y=1\\\\ -log(1-h_θ(x)) &amp; if\\;y=0\\\\ \\end{cases} $$ Its curve be like: 05 Simplified Cost Function and Gradient Descent 5.1 Simplified Cost Function Because y is either 0 or 1, we’ll be able to come up with a simpler way to write the cost function. Rather than write this function in separated two lines, we are going to take these two lines and compress them into one equation, which will make it convenient for us to write the cost function and derive gradient descent. Concretely, we can write out the cost function as follows: Cost(hθ(x),y) = − ylog(hθ(x)) − (1−y)log(1−hθ(x)) It’s easy to figure out why does this equation equal the original one. As we know, y is either 1 or 0. If y = 1, this equation will be − log(hθ(x)). If y = 0, this equation will be − log(1−hθ(x)). This is exactly the same as the original equation. The cost function: $$ J(θ)=\\frac{1}{m}∑^m_{i=1}Cost(h_θ(x^{(i)}),y^{(i)})\\\\=-\\frac{1}{m}[∑^m_{(i=1)}y^{(i)}logh_θ(x^{(i)})+(1-y^{(i)})log(1-h_θ(x^{(i)}))] $$ 5.2 Gradient Descent Now we have got the new cost function. The next step is to fit parameters θ and what we are going to fit parameters θ is try to find the parameters θ that minimizes J(θ). 06 Advance Optimization Given θ, we have code that can compute J(θ) and $\\frac{∂}{∂\\theta_j}J{\\theta}$ (for j=0,1,...,n). One is Gradient Descent, while there is other more advanced, more sophisticated optimization algorithms to implement the computation, such as Conjugate Gradient, BFGS, L-BFGS. Choosing the latter three algorithms has advantages as follows: No need to manually pick α Often faster than gradient descent but also disadvantages as follows: More complex 07 Multi-class Classification: One vs all 7.1 Multi-class Classification Email foldering/tagging: Work, Friends, Family, Hobby Here we have a classification problem with 4 classes, to which we might assign the numbers (y=1,2,3,4). Of course there can be more examples like: Medical diagrams: Not ill, Cold, Flue Weather: Sunny, Cloudy, Rain, Snow … And so in all of these examples, y can take on a small number of discrete values. These are multi-class classification problems. We already know how to do binary classification. Using logistic regression, we know how to plot a straight line to separate the positive and negative classes. Using an idea called one-versus-all classification, we can then take this and make it work for multi-class classification as well. 7.2 One vs all Train a logistic regression classifier hθ(i)(x) for each class i to predict the probability that y = i. On a new input x, to make prediction, pick the class i that maximizes maxihθ(i)(x). § Regularization 01 The Problem of Overfitting 1.1 Overfitting What is overfitting? Let’s take some examples: In the first figure, the straight line does not fit the training data very well. We call it “Underfitting” or “has a high bias”. In the second plot, we could fit a quadratic functions to the data and we could see that it fits the data pretty well. At the other extreme would be if we were to fit a fourth order polynomial to the data and with, we can actually fill a curve that passes through all five of our training examples. But this is a very wiggly curve and we don’t think that’s such a good model for predicting housing prices. We call this problem “Overfitting”, or say this algorithm has high variance. Another example: 1.2 Addressing Overfitting Options: Reduce number of features - Manually select which features to keep. - Model selection algorithm (later in course). Regularization - Keep all the features, but reduce magnitude/values of parameters θj. - Works well when we have a lot of features, each of which contributes a bit to predicting y. 02 Cost Function As you can see, the first curve fits the data very well but the second is overfitting and not generalize well. How can we address that? Suppose we penalize and make θ3, θ4 really small: $$ min_\\theta\\frac{1}{2m}∑^m_{i=1}(h_\\theta(x^{(i)})-y^{(i)})^2+n\\theta^2_3+n\\theta_4^2 $$ Our goal is to minimize the cost function and to do so, we must minimize the θ3 and θ4, supposing the n1 is pretty big. If we are to minimize the θ3 and θ4, they will end up close to 0. That’s as if we were getting rid of θ3x3 and θ4x4 these two terms. And if we get rid of these two terms, it’ll end up still a quadratic function maybe plus tiny contributions from small terms. The idea of regularization is assigning small values to parameters θ0, θ1, ..., θn. Thereupon we can obtain a simpler hypothesis less prone to overfitting. But the question is, sometimes it’s hard to distinguish which features are less likely to be relevant. We don’t know which parameters $ $ to pick to try to shrink. So what we’re going to do is take our cost function and modify this cost function by adding a new term to shrink all of our parameters. $$ J(\\theta)=\\frac{1}{2m}∑^m_{i=1}(h_\\theta(x^{(i)})-y^{(i)})^2+λ∑^n_{j=1}\\theta^2_j $$ Just to remind: m is the number of the training data; n is the number of the features; the $$ is rgularization parameter. The above function is equal to the following since λ is a constant: $$ J(\\theta)=\\frac{1}{2m}[∑^m_{i=1}(h_\\theta(x^{(i)})-y^{(i)})^2+λ∑^n_{j=1}\\theta^2_j] $$ 03 Regularized Linear Regression $$ J(\\theta)=\\frac{1}{2m}[∑^m_{i=1}(h_\\theta(x^{(i)})-y^{(i)})^2+λ∑^n_{j=1}\\theta^2_j] $$ In the last section, we have introduced regularization to the cost function and get a new cost function above. In this section, we are going to generalize regularization to gradient descent and normal equation. 3.1 Gradient Descent The original gradient descent is as follows: Here, as you can see, we separate the θ0 from the overall function since the objects we penalize only include θ1, θ2, ..., θ3. To apply regularization to gradient descent is simple. We just need to modify the cost function and take its derivate: $$ \\theta_j:=\\theta_j(1-\\alpha\\frac{\\lambda}{m})-\\alpha\\frac{1}{m}∑^m_{i=1}(h_\\theta(x^{(i)})-y^{(i)})x_j^{(i)} $$ 3.2 Normal Equation The original idea to use normal equation to minimize the cost function is through the equation below: θ = (XTX)−1XTy Concretely, if we are to use regularization, then this formula is to change as follows: $$ \\theta=(X^TX+\\lambda\\begin{bmatrix} 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\\\ 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0\\\\ 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0\\\\ 0 &amp; 0 &amp; 0 &amp; ... &amp; 0\\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 \\end{bmatrix})^{-1}X^Ty $$ The shape of the matrix is (n+1) * (n+1). All of the elements in the matrix is zero except for the elements on the diagonal starting from line 2. 04 Regularized Logistic Regression The gradient descent in logistic regression is to repeat the sentences below until find all the parameters θ: $$ \\begin{aligned} &amp;J(θ)=-\\frac{1}{m}[∑^m_{(i=1)}y^{(i)}logh_θ(x^{(i)})+(1-y^{(i)})log(1-h_θ(x^{(i)}))]\\\\ &amp;θ_0:=θ_0-α\\frac{1}{m}∑^m_{i=1}(h_θ(x^{(i)})-y^{(i)})x_0^{(i)}\\\\ &amp;θ_j:=θ_j-α\\frac{1}{m}∑^m_{i=1}(h_θ(x^{(i)})-y^{(i)})x_j^{(i)}\\\\ \\end{aligned} $$ Regularization for logistic regression: $$ \\begin{aligned} &amp;θ_0:=θ_0-α[\\frac{1}{m}∑^m_{i=1}(h_θ(x^{(i)})-y^{(i)})x_0^{(i)}+\\frac{λ}{m}θ_0]\\\\ &amp;θ_j:=θ_j-α[\\frac{1}{m}∑^m_{i=1}(h_θ(x^{(i)})-y^{(i)})x_j^{(i)}+\\frac{λ}{m}θ_j] \\end{aligned} $$","categories":[{"name":"计算机专业基础","slug":"计算机专业基础","permalink":"https://example.com/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E4%B8%93%E4%B8%9A%E5%9F%BA%E7%A1%80/"}],"tags":[{"name":"machine learning","slug":"machine-learning","permalink":"https://example.com/tags/machine-learning/"}]},{"title":"✨Welcome!✨","slug":"Welcome","date":"2021-04-04T16:00:00.000Z","updated":"2025-04-13T06:34:53.598Z","comments":true,"path":"2021/04/05/Welcome/","permalink":"https://example.com/2021/04/05/Welcome/","excerpt":"","text":"你好！这是我的个人博客，用于记录我的学习笔记以及生活感悟。建议使用电脑端进行访问，手机端体验不佳。","categories":[],"tags":[]}],"categories":[{"name":"Debugging","slug":"Debugging","permalink":"https://example.com/categories/Debugging/"},{"name":"【Lecture】Software Analysis Testing and Verification","slug":"【Lecture】Software-Analysis-Testing-and-Verification","permalink":"https://example.com/categories/%E3%80%90Lecture%E3%80%91Software-Analysis-Testing-and-Verification/"},{"name":"人生总结","slug":"人生总结","permalink":"https://example.com/categories/%E4%BA%BA%E7%94%9F%E6%80%BB%E7%BB%93/"},{"name":"计算机专业基础","slug":"计算机专业基础","permalink":"https://example.com/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E4%B8%93%E4%B8%9A%E5%9F%BA%E7%A1%80/"},{"name":"深度学习","slug":"深度学习","permalink":"https://example.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"},{"name":"技术开发","slug":"技术开发","permalink":"https://example.com/categories/%E6%8A%80%E6%9C%AF%E5%BC%80%E5%8F%91/"},{"name":"前端","slug":"技术开发/前端","permalink":"https://example.com/categories/%E6%8A%80%E6%9C%AF%E5%BC%80%E5%8F%91/%E5%89%8D%E7%AB%AF/"},{"name":"后端","slug":"技术开发/后端","permalink":"https://example.com/categories/%E6%8A%80%E6%9C%AF%E5%BC%80%E5%8F%91/%E5%90%8E%E7%AB%AF/"}],"tags":[{"name":"docker","slug":"docker","permalink":"https://example.com/tags/docker/"},{"name":"error","slug":"error","permalink":"https://example.com/tags/error/"},{"name":"vscode","slug":"vscode","permalink":"https://example.com/tags/vscode/"},{"name":"program analysis","slug":"program-analysis","permalink":"https://example.com/tags/program-analysis/"},{"name":"生活","slug":"生活","permalink":"https://example.com/tags/%E7%94%9F%E6%B4%BB/"},{"name":"discrete math","slug":"discrete-math","permalink":"https://example.com/tags/discrete-math/"},{"name":"gnn","slug":"gnn","permalink":"https://example.com/tags/gnn/"},{"name":"attention","slug":"attention","permalink":"https://example.com/tags/attention/"},{"name":"web","slug":"web","permalink":"https://example.com/tags/web/"},{"name":"vue3","slug":"vue3","permalink":"https://example.com/tags/vue3/"},{"name":"前端","slug":"前端","permalink":"https://example.com/tags/%E5%89%8D%E7%AB%AF/"},{"name":"machine learning","slug":"machine-learning","permalink":"https://example.com/tags/machine-learning/"},{"name":"hmm","slug":"hmm","permalink":"https://example.com/tags/hmm/"},{"name":"a-star","slug":"a-star","permalink":"https://example.com/tags/a-star/"},{"name":"deduction","slug":"deduction","permalink":"https://example.com/tags/deduction/"},{"name":"svm","slug":"svm","permalink":"https://example.com/tags/svm/"},{"name":"heuristic search","slug":"heuristic-search","permalink":"https://example.com/tags/heuristic-search/"},{"name":"ai","slug":"ai","permalink":"https://example.com/tags/ai/"},{"name":"graphics","slug":"graphics","permalink":"https://example.com/tags/graphics/"},{"name":"transformer","slug":"transformer","permalink":"https://example.com/tags/transformer/"},{"name":"flask","slug":"flask","permalink":"https://example.com/tags/flask/"},{"name":"后端","slug":"后端","permalink":"https://example.com/tags/%E5%90%8E%E7%AB%AF/"}]}