<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>自注意力机制 Self-attention</title>
      <link href="/2025/04/08/%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6-Self-attention/"/>
      <url>/2025/04/08/%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6-Self-attention/</url>
      
        <content type="html"><![CDATA[<h2 id="复杂输入">01 复杂输入</h2><p>不论是在预测问题还是在影像处理上，我们都会假设输入的向量长度是一定的，但如果是不一定的呢？</p><p>比如说在翻译问题中，输入是句子，我们可以采用 one-hot encoding 或 wordembedding的方式将句子转换成向量，而句子是不定长的，也就是说输入的向量是不定长的，这就与我们之前所谈论的情况不一样了。</p><p>或者说更复杂一些，输入的向量不仅不定长，而且数量也不确定，如语音辨识，输入是很长一段语音，这段语音中包含了很多句子，机器需要把这些句子拆分出来，然后进行翻译，这种情况下，句子的数量和长度都不是确定的。</p><p>还有一些更复杂的数据结构，比如说图，例如社交网络、分子结构等，这些信息也可以看作是一组向量，所有这些情况就构成了更加复杂的输入(more sophisticated input)，那么我们要如何处理这些输入呢？</p><span id="more"></span><h2 id="输出是什么">02 输出是什么</h2><p>除了更加复杂的输入，输出也分很多种情况。</p><p>对于词性分析问题，它的目标是将一句话中的每个单词的词性进行分类，这个时候，每一个向量都有一个label；再比如语音辨识问题，机器需要把一段语音中的很多个 frame转换成对应词；还有图的例子，例如在一个社交网络中，机器需要对图中的人进行分类，这些都是输入与输出等长的例子，也就是<span class="math inline"><em>N</em> → <em>N</em></span> 的情况。</p><p>另一种情况是，对于一整个输入，机器只需要输出一个 label就好了，例如情感分析，输入是一个句子向量，机器需要对句子中所表露出的情感进行判断，究竟是正面还是负面；再比如语者辨认，根据输入的语音判断是哪个人讲的，这些都是<span class="math inline"><em>N</em> → 1</span> 的情况。</p><p>还有一种情况则更复杂，那就是输出不确定的情况。例如说翻译，输入的文本长度跟输出的文本长度并不存在必然联系，输出的长度应该由机器自己决定，这种情况下就变成了<spanclass="math inline"><em>N</em> → <em>N</em><sup>′</sup></span>。</p><h2 id="fcnn的缺陷">03 FCNN的缺陷</h2><p>在讨论完了所有输入和输出的情况之后，我们来看看 FCNN有什么缺点。在学过 FCNN 和 CNN 之后我们知道，FCNN 是 bias最小的神经网络，它几乎可以拟合出任何函数，但是这表明它就是无敌的吗？未必，我们来看看下面这个例子。</p><p><span class="math display">Please translate: I saw a saw.</span></p><p>在上面这个翻译任务中出现了两个相同的单词 <spanclass="math inline"><em>s</em><em>a</em><em>w</em></span>，显然这两个<span class="math inline"><em>s</em><em>a</em><em>w</em></span>的意思是不一样的，但对于 FCNN 来讲，它们没有任何差别，也就是说，FCNN无法考虑上下文的关系，当它处理同一个单词 <spanclass="math inline"><em>s</em><em>a</em><em>w</em></span>的时候，它并不会输出两个不同的答案。</p><p>那么难道 FCNN就无法处理这种情况了吗？当然是可以的，不过要改变一下方法。我们可以设置一个window，这个 window 包含了要翻译的词以及其前后的上下文，window越大，其包含的上下文信息就越多。如果我们要 FCNN考虑整个句子的长度，那么我们就必须开一个足够大的 window把整个句子都盖住。但正如前面所讨论的，输入的长度可能是不定的，如果要这么做，我们就必须提前调查一下所有句子中最长的句子的长度，然后把window的大小设置成这个长度才有可能盖住所有句子。然而这样做，不仅会使参数增多，使运算量增大，还容易overfitting。</p><p>那么到底有没有更好一点的做法呢？当然有，这就是接下来要讲的self-attention。</p><h2 id="自注意力机制-self-attention">04 自注意力机制 Self-Attention</h2><p>针对上面的问题，self-attention的做法是，将输入的向量转换成另一个向量，这些转换后的向量是考虑了上下文之后的向量，然后再将这些向量送给FCNN 就行了。</p><p><img src="/img/Self-attention-01.png" /></p><p>当然，self-attention 不止可以用一次，可能经过第一次 self-attention之后 FCNN 已经有了输出，然后我们再用一次 self-attention再转换一次，然后再交给另一个 FCNN 处理，最后得到结果。</p><p>现在我们已经知道 self-attention做的主要工作就是将一个向量转换成一个考虑了所有向量之后的向量，也就是如下图所示的过程：</p><p><img src="/img/Self-attention-02.png" /></p><p>上图中的 <spanclass="math inline"><em>b</em><sup><em>i</em></sup></span> 就是由下层的<span class="math inline"><em>a</em><sup><em>i</em></sup></span> 在经过self-attention 考虑了 <span class="math inline">$\sum_{i=1}^4a^i$</span>之后得到的结果，那么这一步具体要怎么做呢？</p><p>我们现在以 <span class="math inline"><em>b</em><sup>1</sup></span>为例讲述求解 <span class="math inline"><em>b</em></span>的过程。我们要做的是，在 <spanclass="math inline"><em>a</em><sup>1</sup> ∼ <em>a</em><sup>4</sup></span>整个序列中找出与 <span class="math inline"><em>a</em><sup>1</sup></span>有关的，能作为 <span class="math inline"><em>a</em><sup>1</sup></span>的 label的判断依据的向量，为此，我们需要给两个序列之间的关联性一个度量，取名叫<spanclass="math inline"><em>α</em></span>，那么接下来的问题就是如何计算<span class="math inline"><em>α</em></span>。</p><h2 id="相关性指数-α">05 相关性指数 α</h2><p><span class="math inline"><em>α</em></span> 的计算方式主要有两种：dotproduct 和 additive。</p><h3 id="dot-product">5.1 Dot-product</h3><p>Dot-product 是最常用的方法。它的做法是：</p><p>将两个向量输入，将两个向量分别乘上不同的矩阵 <spanclass="math inline"><em>W</em><sup><em>q</em></sup>, <em>W</em><sup><em>k</em></sup></span>，得到两个新向量<span class="math inline"><em>q</em></span> 和 <spanclass="math inline"><em>k</em></span>，再将这两个新向量进行点乘，得到的结果就是这两个向量的关联度<span class="math inline"><em>α</em></span>。Dot-product 的公式是：</p><p><spanclass="math display">Attention (<em>Q</em>,<em>K</em>,<em>V</em>) = softmax (<em>Q</em><em>K</em><sup><em>T</em></sup>)<em>V</em></span></p><blockquote><p>上面的公式已经是注意力机制的完整公式，只有 <spanclass="math inline"><em>Q</em><em>K</em><sup><em>T</em></sup></span>这一部分是 dot-product。</p></blockquote><h3 id="scaled-dot-product">5.2 Scaled dot-product</h3><p>Scaled dot-product 其实就是对 dot-product进行了放缩，最后除了个常量。这个常量记作 <spanclass="math inline"><em>d</em><sub><em>k</em></sub></span>，其值等于矩阵的维度。这样做的原因在论文中是这样解释的：</p><blockquote><p>We suspect that for large values of <spanclass="math inline"><em>d</em><sub><em>k</em></sub></span>, the dotproducts grow large in magnitude, pushing the softmax function intoregions where it has extremely small gradients. To counteract thiseffect, we scale the dot products by <spanclass="math inline">$1/\sqrt{d_k}$</span>.</p></blockquote><p>当运算的矩阵的维度过高时，可能会导致点乘的结果过大或过小，这样的话，后续将结果放进softmax 的时候会导致其分布到函数的两端 (softmax的函数图像的两端梯度都很小)，梯度比较小，可能会出现梯度消失的问题。除以<span class="math inline">$\sqrt{d_k}$</span>进行放缩，可以使结果分布到函数图像的中间。Scaled dot-product的公式是：</p><p><span class="math display">$$\operatorname{Attention}(Q,K,V)=\operatorname{softmax}(\frac{QK^T}{\sqrt{d_k}})V$$</span></p><blockquote><p>注意：上面的公式已经是注意力机制的完整公式，只有 <spanclass="math inline">$\frac{QK^T}{\sqrt{d_k}}$</span> 这一部分是 scaleddot-product。</p></blockquote><h3 id="additive">5.3 Additive</h3><p>Additive 的做法的第一步和 dot-product是一样的：将两个向量输入，将两个向量分别乘上不同的矩阵 <spanclass="math inline"><em>W</em><sup><em>q</em></sup>, <em>W</em><sup><em>k</em></sup></span>，得到两个新向量<span class="math inline"><em>q</em></span> 和 <spanclass="math inline"><em>k</em></span>。但是接下来不是要将它们做点乘，而是相加，经过一个activation function，再经过一个 transform，最终得到 <spanclass="math inline"><em>α</em></span>。</p><p>下面是这两种方法的图示：</p><p><img src="/img/Self-attention-03.png" /></p><h2 id="注意力-attention">06 注意力 Attention</h2><p>在知道了如何计算 <span class="math inline"><em>α</em></span>之后，我们选择使用 dot-product 方法，继续来看如何求解 <spanclass="math inline"><em>b</em><sup>1</sup></span>。</p><p>首先，根据 dot-product 的方法，这里我们要求 <spanclass="math inline"><em>a</em><sup>1</sup></span>和剩下向量的关系，所以我们应该先去求 <spanclass="math inline"><em>q</em><sup>1</sup></span>，这个值我们称为query。根据公式，<spanclass="math inline"><em>q</em><sup>1</sup> = <em>W</em><sup><em>q</em></sup><em>a</em><sup>1</sup></span>。</p><p>接下来，我们应该去求 <spanclass="math inline"><em>k</em></span>，也就是用 <spanclass="math inline"><em>W</em><sup><em>k</em></sup></span>去乘上剩下的所有向量。这个 <span class="math inline"><em>k</em></span>被我们称作是 key。</p><p>在求出 <span class="math inline"><em>q</em><sup>1</sup></span> 和<spanclass="math inline"><em>k</em><sup>2</sup>, <em>k</em><sup>3</sup>, <em>k</em><sup>4</sup></span>之后，将它们分别进行点乘，就能得到 <spanclass="math inline"><em>α</em><sub>1, 2</sub></span>，<spanclass="math inline"><em>α</em><sub>1, 3</sub></span>，<spanclass="math inline"><em>α</em><sub>1, 4</sub></span>，这个 <spanclass="math inline"><em>α</em></span> 也有一个名字，叫作 attentionscore。</p><p>但其实，我们还会用 <spanclass="math inline"><em>q</em><sup>1</sup></span> 去乘上自己得到 <spanclass="math inline"><em>α</em><sub>1, 1</sub></span>，这也很好理解，毕竟你要考虑<span class="math inline"><em>a</em><sup>1</sup></span>和其他向量的关系，也不能忘了 <spanclass="math inline"><em>a</em><sup>1</sup></span> 与自身的关联。</p><p>在算出所有的 <spanclass="math inline"><em>α</em><sub>1, <em>i</em></sub></span>之后，我们会将它们放进一个 soft-max 里进行转换得到 <spanclass="math inline"><em>α</em><sub>1, <em>i</em></sub><sup>′</sup></span>：</p><p><span class="math display">$$\alpha^\prime_{1,i}=\frac{\operatorname{exp}(\alpha_{1,i})}{\sum_j\operatorname{exp}(\alpha_{1,j})}$$</span></p><p>这一步其实是 normalization，至于为什么是soft-max，只能说这是先人尝试之后的结果，你当然也可以使用 ReLu之类的函数，但只不过说经过前人的尝试，soft-max 的效果最好。</p><blockquote><p>为什么一定要将 α 放进 softmax 或其他类似的函数里?这是因为，α 代表了key 和 query之间的相似性，注意力机制的本质是关注相似性高的，而忽略相似性低的。最后对所有信息进行整合时，我们其实是根据α 的大小进行加权聚合，相似性高的向量权重就大一点，所以我们需要将所有的 α放进 softmax 中，以此来得到权重。简单来讲，计算 α的目的就是为了得到权重，而后面计算出的 v 是每个向量的价值。</p></blockquote><p>在做完这些工作之后，我们就终于可以来计算 <spanclass="math inline"><em>b</em><sup>1</sup></span> 了。首先，我们需要将<spanclass="math inline"><em>a</em><sup>1</sup> ∼ <em>a</em><sup>4</sup></span>都乘上一个矩阵 <spanclass="math inline"><em>W</em><sup><em>v</em></sup></span>，得到 <spanclass="math inline"><em>v</em><sup>1</sup> ∼ <em>v</em><sup>4</sup></span>，然后根据下面的公式就能得到<span class="math inline"><em>b</em><sup>1</sup></span> 了：</p><p><spanclass="math display"><em>b</em><sup>1</sup> = ∑<sub><em>i</em></sub><em>α</em><sub>1, <em>i</em></sub><sup>′</sup><em>v</em><sup><em>i</em></sup></span></p><p>讲完了全部过程，我们再来回顾一下。很显然，如果 <spanclass="math inline"><em>a</em><sup>1</sup></span> 与其中某一个向量 <spanclass="math inline"><em>a</em><sup><em>i</em></sup></span>的关联度最大，那么 <spanclass="math inline"><em>α</em><sub>1, <em>i</em></sub><sup>′</sup></span>就会很大，这就会使得最终的 <spanclass="math inline"><em>b</em><sup>1</sup></span> 与 <spanclass="math inline"><em>v</em><sup><em>i</em></sup></span>更加接近，也就是对 <spanclass="math inline"><em>a</em><sup><em>i</em></sup></span> 的 attention最大，这也就得到了考虑了上下文的向量。</p><p>Self-attention 里其实并没有做太多的工作，它需要通过 dataset学习的其实仅仅只有三个参数：<spanclass="math inline"><em>W</em><sup><em>q</em></sup></span>，<spanclass="math inline"><em>W</em><sup><em>k</em></sup></span> 和 <spanclass="math inline"><em>W</em><sup><em>v</em></sup></span>。除此之外的所有参数都是人为设置好的。</p><p>如果你想知道以上所有过程的代码思路 (仅仅是思路)或者说矩阵运算的技巧，可以参考<ahref="https://www.bilibili.com/video/BV1Wv411h7kN?p=39">这个视频</a>。</p><h2 id="多头自注意力-multi-head-self-attention">07 多头自注意力Multi-head Self-attention</h2><p>Self-attention 其实还有很多变体，其中一个在今天应用非常广泛的模型就是multi-head self-attention。</p><p>Multi-head self-attention的想法是，事物与事物之间的关联性有时候是多方面的，当考虑不同的方面时，两个事物的关联性可能就是不同的，所以我们需要不止一种<span class="math inline"><em>α</em></span>。考虑多少种 <spanclass="math inline"><em>α</em></span>，就有多少个 head。</p><p>如果这样考虑的话，根据正常的 self-attention 的做法，原本只需要对每个<span class="math inline"><em>a</em><sup><em>i</em></sup></span> 计算<spanclass="math inline"><em>q</em><sup><em>i</em></sup>, <em>k</em><sup><em>i</em></sup>, <em>v</em><sup><em>i</em></sup></span>，现在则还需要针对每个<span class="math inline"><em>q</em><sup><em>i</em></sup></span> 计算<spanclass="math inline"><em>q</em><sup><em>i</em>, 1</sup>, <em>q</em><sup><em>i</em>, 2</sup>...</span>，也就是考虑多方面的关联性。那既然<span class="math inline"><em>q</em></span> 现在有多个，那么 <spanclass="math inline"><em>k</em></span> 和 <spanclass="math inline"><em>v</em></span>也肯定要有多个。那至于怎么进一步得到 <spanclass="math inline"><em>q</em><sup><em>i</em>, 1</sup>, <em>q</em><sup><em>i</em>, 2</sup>...<em>k</em><sup><em>i</em>, 1</sup>, <em>k</em><sup><em>i</em>, 2</sup>...<em>v</em><sup><em>i</em>, 1</sup>, <em>v</em><sup><em>i</em>, 2</sup>...</span>，其实是用更多的<spanclass="math inline"><em>W</em><sup><em>q</em></sup>, <em>W</em><sup><em>k</em></sup>, <em>W</em><sup><em>v</em></sup></span>来乘上原来的 <spanclass="math inline"><em>q</em>, <em>k</em>, <em>v</em></span>，也就是：</p><p><span class="math display">$$\begin{aligned}&amp; \boldsymbol{q}^{i, \mathbf{1}}=W^{q, 1} \boldsymbol{q}^i \\&amp; \boldsymbol{k}^{i, 2}=W^{k, 2} \boldsymbol{k}^i\\&amp; \boldsymbol{v}^{i, 2}=W^{v, 2} \boldsymbol{v}^i\end{aligned}$$</span></p><p>所以，其实 multi-head self-attention 要做的事情和 self-attention是一样的，只不过现在有多个 head，所以要每个 head 都做一遍独立的self-attention 而已，最后你能得到多个 <spanclass="math inline"><em>b</em><sup><em>i</em>, <em>j</em></sup></span>，那接下来你可能会把这些<span class="math inline"><em>b</em></span>都连接起来形成一个矩阵，然后将其乘上另一个矩阵 <spanclass="math inline"><em>W</em><sup><em>o</em></sup></span>，得到最终的<span class="math inline"><em>b</em><sup><em>i</em></sup></span>。</p><h2 id="位置编码-positional-encoding">08 位置编码 PositionalEncoding</h2><p>不知道你看到这里有没有发现 self-attention 的一个缺陷？Self-attention似乎只在考虑attention，也就是向量与向量之间的关联，却漏掉了一个很重要的信息 ——那就是“位置(position)”！例如某个向量是排在序列的最前面还是最后，它是完完全全没有考虑的，而位置信息很明显，在很多任务中都是很重要的，尤其是对于文字处理而言，比如说，动词出现在句首的概率比较低，那么如果一个词出现在句首，它可能是动词的概率就比较低。</p><p>所以怎么办呢？这就是 positional encoding这项技术的作用，它可以把向量的位置信息给“塞进去”。它给序列中的每一个位置都设定了一个vector，称为 <spanclass="math inline"><em>e</em><sup><em>i</em></sup></span>，不同的位置都有一个专属的<span class="math inline"><em>e</em></span>。我们要做的事情是，将这个<span class="math inline"><em>e</em><sup><em>i</em></sup></span> 加到<span class="math inline"><em>a</em><sup><em>i</em></sup></span>上面去，就结束了。没错，就这么简单。</p><p>那么这个 <span class="math inline"><em>e</em></span>是如何确定的呢？在 <em>Attention is All You Need</em> 论文中，<spanclass="math inline"><em>e</em></span> 是人为规定 (hand-crafted)的，他们是使用 sin、cos 这些神奇的函数来得到 <spanclass="math inline"><em>e</em></span>的，至于可不可以用其他函数，答案当然是可以，positional encoding目前还是一个尚待研究的问题，所以你用什么都是没问题的；当然，<spanclass="math inline"><em>e</em></span> 也可以是通过 data 学习出来的。</p><h2 id="self-attention-v.s.-cnn">09 Self-attention v.s. CNN</h2><p>Self-attention同样是可以用于处理图像的。我们知道图像信息也是向量，使用 self-attention我们就可以去考虑每个 pixel之间的关联度，让机器自己去筛选出一张图片中重要的信息。当今，使用self-attention处理图片已然不是什么很新鲜的事情，那么就会有一个问题：self-attention 和CNN 孰优孰劣？</p><p>其实，我们可以将 self-attention 当作是一个复杂版的 CNN，而 CNN 是self-attention 的简化版。我们知道，CNN 每次都只考虑一个 perceptive filed的信息；而 self-attention 则是通过 pixel 与每个 pixel之间的关联度，来自动筛选出值得关注的 filed，这就好像是在说self-attention 的 perceptive filed 是自己学习出来的。</p><p>在 <a href="https://arxiv.org/abs/1911.03584">On the Relationshipbetween Self-Attention and Convolutional Layers</a>这篇论文里，作者用数学方法讨论了 CNN 和self-attention，并且证明了只要参数设置正确，self-attention 完全可以变成CNN。也就是说，CNN 只是 self-attention 的一种特例。</p><p>那么究竟 CNN 和 self-attention谁更加好呢？显然通过我们上面的讨论，CNN 的 bias 比 self-attention更大，也就是 CNN的弹性更加小，能拟合出的函数更加少。而我们也知道，弹性大的网络在面对较小的数据量时很容易overfitting；弹性小的网络在面对更大的数据量的时候则很难再学到新的东西。也有学者对此做过专门研究，最后发现，当数据量较少时，CNN的准确率是能超过 self-attention 的，但当数据量到达 100M 级别的时候，CNN就被 self-attention 超过了。所以 CNN 和 self-attention到底谁更好是依你的数据量来定的。</p><h2 id="图中的自注意力机制">10 图中的自注意力机制</h2><p>Self-attention 也是可以用在 graph 上的，例如下面这张图：</p><p><img src="/img/Self-attention-04.png" /></p><p>当我们需要去给节点 1 做 label的时候，我们就可以去考虑它的邻居节点，然后使用 self-attention对其与邻居节点之间的关联性进行考量，而对于那些不与节点 1相连的节点则代表我们已经人为地帮机器排除了彼此的联系，所以就不需要机器再去考虑了。这其实就是GNN 的一种。</p><h2 id="注意力和自注意力">11 注意力和自注意力</h2><p>我们上面介绍的是 self-attention，注意：self-attention (自注意力机制)和 attention (注意力机制)是不一样的！我们可以用一个例子来说明注意力机制是什么。以淘宝搜索商品为例，淘宝要做的是，将我们输入的关键词与其数据库内的商品相关联，其本质其实和self-attention很像，都是找关联性强的，忽略关联性弱的，但不同的是，在这个例子中，我们的<span class="math inline"><em>q</em>, <em>k</em>, <em>v</em></span> 来自source (商品) 和 target (关键词)，它们位于 transfomer架构的两端；而在上面举的机器翻译的例子中，source 是输入的中文，target是要求解的英文翻译，它们位于 transfomer 的两端，但 <spanclass="math inline"><em>q</em>, <em>v</em>, <em>k</em></span> 只来自source。</p>]]></content>
      
      
      <categories>
          
          <category> deep learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> technique </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>A WSL Error Encountered When Installing Docker</title>
      <link href="/2025/04/07/A-WSL-Error-Encountered-When-installing-Docker/"/>
      <url>/2025/04/07/A-WSL-Error-Encountered-When-installing-Docker/</url>
      
        <content type="html"><![CDATA[<p>This document explains the problems the author had installing Dockerand how to solve them. You can find and download the installationpackage for Docker <a href="https://www.docker.com/">here</a>.</p><p>After installing Docker, an <code>Unexpected WSL error</code> wasencountered, and the process was terminated. After conducting someonline research, I discovered that this error message was indicatingthat I needed to enable the <strong>Hyper-V</strong>, <strong>WindowsSubsystem for Linux</strong> and <strong>Virtual MachinePlatform</strong> functions on my system. I opened the configurationpanel and found that the latter two functions had been properly enabled,but the option for the first one was missing. It took me a some time tofind the solution.</p><span id="more"></span><p>To address the issue, it was necessary to create and run a file named<code>Hyper-V.bat</code> as administrator, which contains the followingcontent:</p><figure class="highlight bat"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">pushd</span> &quot;%~dp0&quot;</span><br><span class="line"><span class="built_in">dir</span> /b <span class="variable">%SystemRoot%</span>\servicing\Packages\*Hyper-V*.mum &gt;hyper-v.txt</span><br><span class="line"><span class="keyword">for</span> /f <span class="variable">%%i</span> <span class="keyword">in</span> (&#x27;<span class="built_in">findstr</span> /i . hyper-v.txt <span class="number">2</span>^&gt;<span class="built_in">nul</span>&#x27;) <span class="keyword">do</span> dism /online /norestart /add-package:&quot;<span class="variable">%SystemRoot%</span>\servicing\Packages\<span class="variable">%%i</span>&quot;</span><br><span class="line"><span class="built_in">del</span> hyper-v.txt</span><br><span class="line">Dism /online /enable-feature /featurename:Microsoft-Hyper-V-All /LimitAccess /ALL</span><br></pre></td></tr></table></figure><p>The program will then require you to restart your machine, at whichpoint you will see that Hyper-V is properly enabled. But Docker stillcannot run normally.</p><p>After that, you need to install WSL2. To do this, run the followingcommand:</p><figure class="highlight cmd"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">wsl --install# may require VPN</span><br><span class="line">wsl --<span class="built_in">set</span>-default-version <span class="number">2</span># <span class="built_in">set</span> the default wsl version to wsl2</span><br><span class="line">wsl --update# may <span class="keyword">not</span> be necessary</span><br></pre></td></tr></table></figure><p>Finally, Docker can run without any problems after all this work.</p><hr /><p>Off topic:</p><p>When using the VSCode extension <code>Remote Containers</code> to setup containers, you might encounter the message “the container does notmeet all the requirements of the VS Code Server”. This happens becauseVSCode has increased the minimum requirements for remote server buildtoolchain since version 1.86. To solve the problem, just downgrade yourVSCode to a version below 1.86. You can download version 1.85.2 <ahref="https://update.code.visualstudio.com/1.85.2/win32-x64-archive/stable">here</a>.Besides, it is necessary to downgrade your extensions as well.</p>]]></content>
      
      
      <categories>
          
          <category> solutions </category>
          
      </categories>
      
      
        <tags>
            
            <tag> technique </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Introduction to Program Analysis</title>
      <link href="/2025/04/07/Introduction-to-Program-Analysis/"/>
      <url>/2025/04/07/Introduction-to-Program-Analysis/</url>
      
        <content type="html"><![CDATA[<h2 id="what-is-program-analysis">01 What Is Program Analysis</h2><p>Program analysis is to discover useful facts about programs. Youprobably have known some manual or automated testing tools like:</p><ul><li><p>Manual testing or semi-automated testing: JUnit, Selenium,etc.</p></li><li><p>Manual “analysis” of programs: Code inspection, debugging,etc.</p></li></ul><p>The focus of this course is <strong>automated</strong> programanalysis.</p><span id="more"></span><p>Program analysis can be broadly classified into three kinds:</p><ul><li>Static (compile-time)<ul><li>Infer facts by inspecting source or binary code</li><li>Typically:<ul><li>Consider all inputs</li><li>Overapproximate possible behavior</li></ul></li><li>E.g. compilers, lint-like tools</li></ul></li><li>Dynamic (execution-time)<ul><li>Infer facts by monitoring program executions</li><li>Typically:<ul><li>Consider current input</li><li>Underapproximate possible behavior</li></ul></li><li>E.g. automated testing tools, profilers</li></ul></li><li>Hybrid (combining dynamic and static)</li></ul><h2 id="terminology">02 Terminology</h2><p>The following is a snippet of JavaScript code.</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">var</span> r = <span class="title class_">Math</span>.<span class="title function_">random</span>();<span class="comment">//value in [0,1)</span></span><br><span class="line"><span class="keyword">var</span> out = <span class="string">&quot;yes&quot;</span>;</span><br><span class="line"><span class="keyword">if</span>(r &lt; <span class="number">0.5</span>)</span><br><span class="line">out = <span class="string">&quot;no&quot;</span>;</span><br><span class="line"><span class="keyword">if</span>(r == <span class="number">1</span>)</span><br><span class="line">out = <span class="string">&quot;maybe&quot;</span>;</span><br><span class="line"><span class="variable language_">console</span>.<span class="title function_">log</span>(out);</span><br></pre></td></tr></table></figure><p>Q: What are the possible outputs?</p><h3 id="overapproximation-v.s.-underapproximation">2.1 Overapproximationv.s. Underapproximation</h3><p>Judging from the static code, it seems that there are three possibleoutputs: “yes”, “no” or “maybe”. (Overapproximation)</p><p>If we consider the case of only one execution like<code>r=0.7</code>, its output is “yes”. (Underapproximation)</p><p>However, both responses are erroneous. The first option yields theimplausible output “maybe”, while the second excludes the feasibleoutput “no”. These erroneous responses serve as quintessentialillustrations of over- and under-approximation, respectively.</p><ul><li>Overapproximation: Consider all paths</li><li>Underapproximation: Execute the program once</li></ul><h3 id="soundness-completeness">2.2 Soundness &amp; Completeness</h3><p>It is easy for us humans to give the right answer —— “yes” or “no”.We think these answers are <strong>sound</strong> and<strong>complete</strong>.</p><p>“Soundness” means it contains all the possible outputs we want (mightgive <strong>false positives</strong>).</p><p>“Completeness” means it excludes all the impossible outputs we do notwant (might give <strong>false negtives</strong>).</p><p>When we put these two ideas together, we get a definition thatincludes exactly all possible outputs.</p><h3 id="false-positives-false-negatives">2.3 False Positives &amp; FalseNegatives</h3><p>The definitions of false positives and false negatives:</p><ul><li>False positives: impossible outputs that are indicated possible</li><li>False negatives: possible outputs that are indicated impossible</li></ul><p>Let <span class="math inline"><em>P</em></span> be <em>Program</em>,<span class="math inline"><em>i</em></span> be <em>Input</em>, <spanclass="math inline"><em>P</em>(<em>i</em>)</span> be <em>Behavior</em>.The following graph shows the relations between the above ideas.</p><p><img src="/img/Introduction-to-Program-Analysis-01.png" /></p><h3 id="precision-recall">2.4 Precision &amp; Recall</h3><p>Differentiate precision and recall:</p><ul><li><p>Precision: how many retrieved items are relevant</p></li><li><p>Recall: how many relevant items are retrieved</p></li></ul><p>Take the overapproximated answer aforementioned for instance, theprecision and the recall are:</p><p><span class="math display">$$\mathrm{precision}=\frac{2}{3}=0.67$$</span></p><p><span class="math display">$$\mathrm{recall}=\frac{1}{2}=0.5$$</span></p><h3 id="program-invariants">2.5 Program Invariants</h3><p>Program Invariants are logical assertions whose certain conditions orproperties remain true throughout the execution of a program. Theseinvariants are key to program correctness. They help verify that theprogram behaves as expected and play an important role in softwaredevelopment.</p><p>See the below code snippet:</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">int</span> <span class="title function_">p</span><span class="params">(<span class="type">int</span> x)</span> &#123; <span class="keyword">return</span> x * x; &#125;</span><br><span class="line"><span class="type">void</span> <span class="title function_">main</span><span class="params">()</span> &#123;</span><br><span class="line">    <span class="type">int</span> z;</span><br><span class="line">    <span class="keyword">if</span> (getc() == <span class="string">&#x27;a&#x27;</span>)</span><br><span class="line">    z = p(<span class="number">6</span>) + <span class="number">6</span>;</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">    z = p(<span class="number">-7</span>) - <span class="number">7</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Q: An invariant at the end of the program is <code>(z == c)</code>for some constant <code>c</code>. What is <code>c</code>?</p><p>Clearly, the <code>z</code> will yield <code>42</code> regardless ofany inputs. Therefore, <code>(z == 42)</code> is definitely aninvariant, while <code>(z == 30)</code> is definitely not aninvariant.</p><p>Using the invariant to avert disaster:</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">int</span> <span class="title function_">p</span><span class="params">(<span class="type">int</span> x)</span> &#123; <span class="keyword">return</span> x * x; &#125;</span><br><span class="line"><span class="type">void</span> <span class="title function_">main</span><span class="params">()</span> &#123;</span><br><span class="line">    <span class="type">int</span> z;</span><br><span class="line">    <span class="keyword">if</span> (getc() == <span class="string">&#x27;a&#x27;</span>)</span><br><span class="line">    z = p(<span class="number">6</span>) + <span class="number">6</span>;</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">    z = p(<span class="number">-7</span>) - <span class="number">7</span>;</span><br><span class="line">    <span class="keyword">if</span> (z != <span class="number">42</span>) &#123;</span><br><span class="line">        disaster();<span class="comment">// disaster averted</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="others">03 Others</h2><h3 id="undecidability-of-program-properties">3.1 Undecidability ofProgram Properties</h3><ul><li>Q: Can program analysis be sound and complete? A: Not if we want itto terminate!</li><li>Questions like “is a program point reachable on some input?” are<strong>undecidable</strong>.</li><li>Designing a program analysis is an art —— tradeoffs dictated byconsumer.</li></ul><h3 id="why-take-this-course">3.2 Why Take This Course?</h3><ul><li><p>Learn methods to improve software quality, reliability, security,performance, etc.</p></li><li><p>Become a better software developer/tester</p></li><li><p>Build specialized tools for software analysis, testing andverification</p></li><li><p>Finding Jobs &amp; Do research</p></li></ul><h3 id="who-needs-program-analysis">3.3 Who Needs Program Analysis?</h3><p>Three primary consumers of program analysis:</p><ul><li>Compilers</li><li><strong>Software Quality Tools (Primary focus of thiscourse)</strong></li><li>Integrated Development Environments (IDEs)</li></ul><h4 id="compilers">3.3.1 Compilers</h4><p>Program analysis serves as the bridge between high-level languagesand architectures.</p><p>For example, we use program analysis to generate efficient code.</p><p>Before:</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">int</span> <span class="title function_">p</span><span class="params">(<span class="type">int</span> x)</span> &#123; <span class="keyword">return</span> x * x; &#125;</span><br><span class="line"><span class="type">void</span> <span class="title function_">main</span><span class="params">(<span class="type">int</span> arg)</span> &#123;</span><br><span class="line">    <span class="type">int</span> z;</span><br><span class="line">    <span class="keyword">if</span> (arg != <span class="number">0</span>)</span><br><span class="line">    z = p(<span class="number">6</span>) + <span class="number">6</span>;</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">    z = p(<span class="number">-7</span>) - <span class="number">7</span>;</span><br><span class="line">    print (z);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>After:</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">int</span> <span class="title function_">p</span><span class="params">(<span class="type">int</span> x)</span> &#123; <span class="keyword">return</span> x * x; &#125;</span><br><span class="line"><span class="type">void</span> <span class="title function_">main</span><span class="params">()</span> &#123;</span><br><span class="line">print (<span class="number">42</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="software-quality-tools">3.3.2 Software Quality Tools</h4><p>Software quality tools are tools for testing, debugging, andverification.</p><p>Software quality tools use program analysis for:</p><ul><li>Finding programming errors</li><li>Proving program invariants</li><li>Generating test cases</li><li>Localizing causes of errors</li><li>…</li></ul><p>Some software quality tools:</p><ul><li>Static Program Analysis<ul><li>Suspicious error patterns: <em>Lint</em>, <em>SpotBugs</em>,<em>Coverity</em></li><li>Memory leak detection: <em>Facebook Infer</em></li><li>Checking API usage rules: <em>Microsoft SLAM</em></li><li>Verifying invariants: <em>ESC/Java</em></li></ul></li><li>Dynamic Program Analysis<ul><li>Array bound checking: <em>Purify</em></li><li>Datarace detection: <em>Eraser</em></li><li>Memory leak detection: <em>Valgrind</em></li><li>Finding likely invariants: <em>Daikon</em></li></ul></li></ul><h4 id="integrated-development-environments">3.3.3 IntegratedDevelopment Environments</h4><p>Examples: <em>Eclipse</em> and <em>VS Code</em></p><p>Use program analysis to help programmers:</p><ul><li>Understand programs</li><li>Refactor programs<ul><li>Restructuring a program without changing its behavior</li></ul></li></ul><p>Useful in dealing with large, complex programs</p><h2 id="quiz">04 Quiz</h2><ul><li>Dynamic vs. Static Analysis:</li></ul><table><colgroup><col style="width: 12%" /><col style="width: 44%" /><col style="width: 43%" /></colgroup><thead><tr class="header"><th style="text-align: center;"></th><th style="text-align: center;">Dynamic</th><th style="text-align: center;">Static</th></tr></thead><tbody><tr class="odd"><td style="text-align: center;">Cost</td><td style="text-align: center;"><u>Proportional to program’s executiontime</u></td><td style="text-align: center;"><u>Proportional to program’ssize</u></td></tr><tr class="even"><td style="text-align: center;">Effectiveness</td><td style="text-align: center;"><u>Unsound (may miss errors)</u></td><td style="text-align: center;"><u>Incomplete (may report spuriouserrors)</u></td></tr></tbody></table><ul><li>Unsoundness yields <u>false negatives</u>; incompleteness yields<u>false positives</u>.</li></ul>]]></content>
      
      
      <categories>
          
          <category> Software Analysis, Testing and Verification </category>
          
      </categories>
      
      
        <tags>
            
            <tag> lecture notes </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>离散数学</title>
      <link href="/2025/03/18/%E7%A6%BB%E6%95%A3%E6%95%B0%E5%AD%A6/"/>
      <url>/2025/03/18/%E7%A6%BB%E6%95%A3%E6%95%B0%E5%AD%A6/</url>
      
        <content type="html"><![CDATA[<p>本篇笔记的参考书目为KennethH.Rosen版《离散数学及其应用》，主要内容包括逻辑和证明、集合、计数、递推关系式、容斥原理、关系、图论。</p><h2 id="逻辑和证明">01 逻辑和证明</h2><h3 id="命题逻辑">1.1 命题逻辑</h3><blockquote><p>概念：命题、原子命题、命题变元、命题真值、逻辑运算符（否定、合取、析取、条件、双条件）、真值表、永真式、矛盾式、可能式</p></blockquote><p>逻辑等价式：1.证明逻辑表达式等价：真值表相同；构造逻辑等价式。2.若<span class="math inline"><em>p</em> ↔︎ <em>q</em></span> 是永真式，则<spanclass="math inline"><em>p</em> ≡ <em>q</em></span>。3.德·摩根律、分配律、结合律。</p><p>重要逻辑等价式：<spanclass="math inline"><em>p</em> → <em>q</em> ≡ ¬<em>p</em> ∨ <em>q</em></span>；<spanclass="math inline"><em>p</em> ↔︎ <em>q</em> ≡ <em>p</em> → <em>q</em> ∧ <em>q</em> → <em>p</em> ≡ (<em>p</em>∧<em>q</em>) ∨ (¬<em>p</em>∧¬<em>q</em>)</span>。</p><p>对偶（dual）：将命题公式 <span class="math inline"><em>A</em></span>中的全部 <span class="math inline">∨</span> 换成 <spanclass="math inline">∧</span>，<span class="math inline">∧</span> 换成<span class="math inline">∨</span>，<spanclass="math inline"><em>F</em></span> 换成 <spanclass="math inline"><em>T</em></span>，<spanclass="math inline"><em>T</em></span> 换成 <spanclass="math inline"><em>F</em></span>，得到其对偶命题 <spanclass="math inline"><em>A</em><sup>*</sup></span>。</p><span id="more"></span><p>对偶定理：</p><ol type="1"><li><spanclass="math inline">¬<em>A</em>(<em>P</em><sub>1</sub>,<em>P</em><sub>2</sub>,…,<em>P</em><sub><em>n</em></sub>) ⇔ <em>A</em><sup>*</sup>(¬<em>P</em><sub>1</sub>,¬<em>P</em><sub>2</sub>,…,¬<em>P</em><sub><em>n</em></sub>)</span>；<spanclass="math inline"><em>A</em>(¬<em>P</em><sub>1</sub>,¬<em>P</em><sub>2</sub>,…,¬<em>P</em><sub><em>n</em></sub>) ⇔ ¬<em>A</em><sup>*</sup>(<em>P</em><sub>1</sub>,<em>P</em><sub>2</sub>,…,<em>P</em><sub><em>n</em></sub>)</span></li><li>若 <span class="math inline"><em>A</em> ⇔ <em>B</em></span>，则<spanclass="math inline"><em>A</em><sup>*</sup> ⇔ <em>B</em><sup>*</sup></span></li></ol><p>范式：析取范式、合取范式、主析取范式、主合取范式</p><p>命题的可满足性：如果一个命题存在一个赋值为真，一个复合命题是可满足的（相容），不存在就是不可满足的（不相容）。</p><h3 id="谓词逻辑">1.2 谓词逻辑</h3><blockquote><p>命题函数、谓词、量词（全称、存在、唯一性）、论域、约束论域、量化表达式的否定</p></blockquote><p>量词的德·摩根律： <spanclass="math display">¬∀<em>x</em><em>P</em>(<em>x</em>) ≡ ∃<em>x</em>¬<em>P</em>(<em>x</em>)</span></p><p><spanclass="math display">¬∃<em>x</em><em>Q</em>(<em>x</em>) ≡ ∀<em>x</em>¬<em>Q</em>(<em>x</em>)</span></p><p>语句到逻辑表达式的翻译：定义命题函数、定义论域、给出逻辑表达式</p><h3 id="嵌套量词">1.4 嵌套量词</h3><p>嵌套量词即出现在其他量词的作用域中的量词，比如：<spanclass="math inline">∀<em>x</em>∃<em>y</em>(<em>x</em>+<em>y</em>=0)</span>。</p><p>量词的顺序会对量化式的含义造成影响，但如果全是全称或存在量词，那就没有影响。</p><p><spanclass="math inline">∃<em>y</em>∀<em>x</em><em>Q</em>(<em>x</em>,<em>y</em>)</span>为真，则 <spanclass="math inline">∀<em>x</em>∃<em>y</em><em>Q</em>(<em>x</em>,<em>y</em>)</span>为真，但反过来不一定。</p><p>会翻译和使用含有嵌套量词的量化式。</p><p>嵌套量词的否定：可以通过将否定词按否定规则（量词的德摩根律）依次移入所有量词里来得到。</p><h3 id="推理规则">1.5 推理规则</h3><h4 id="命题逻辑的推理规则">1.5.1 命题逻辑的推理规则</h4><p>证明一个命题正确的方法：</p><ol type="1"><li>真值表法</li><li>等价演算法</li><li>演绎推理法：P 规则、E 规则、T 规则</li><li>附加前提法（有两种，第二种也叫 CP 规则）：<ol type="1"><li>当要证明 <span class="math inline"><em>A</em> ⇒ <em>B</em></span>时，即证明 <spanclass="math inline"><em>A</em>, ¬<em>B</em> ⇒ 0</span>，也就是将结论否定，加入前提，证明其为矛盾式。</li><li>要证明当 <span class="math inline"><em>A</em></span> 成立时，从<span class="math inline"><em>B</em></span> 能推出 <spanclass="math inline"><em>C</em></span>，即 <spanclass="math inline"><em>A</em> ⇒ <em>B</em> → <em>C</em></span>，可以转换为证明当<span class="math inline"><em>A</em>、<em>B</em></span> 都成立时，<spanclass="math inline"><em>C</em></span> 成立，即 <spanclass="math inline"><em>A</em>, <em>B</em> ⇒ <em>C</em></span>，话句话说，就是将结论中的一部分拿出来当作前提。</li></ol></li></ol><p>假言推理（分离规则）：<spanclass="math inline">(<em>p</em>∧(<em>p</em>→<em>q</em>)) → <em>q</em></span>。这是一个永真式，它是假言推理的基础。</p><p>假言三段论（递推）、附加律（<spanclass="math inline"><em>p</em> → <em>p</em> ∨ <em>q</em></span>）、化简律（<spanclass="math inline"><em>p</em> ∧ <em>q</em> → <em>p</em></span>）、取拒式（若<span class="math inline"><em>p</em> → <em>q</em></span> 且 <spanclass="math inline">¬<em>q</em></span>，则 <spanclass="math inline">¬<em>p</em></span>）、析取三段论（若 <spanclass="math inline"><em>p</em> ∨ <em>q</em></span> 且 <spanclass="math inline">¬<em>p</em></span>，则 <spanclass="math inline"><em>q</em></span>）、消解律（p发生或者q发生，p不发生或者r发生，这两个前提恒成立，则q和r中至少有一个发生）</p><h4 id="量化命题的推理规则">1.5.2 量化命题的推理规则</h4><ol type="1"><li>全称实例（UI规则）：若 <spanclass="math inline">∀<em>x</em><em>P</em>(<em>x</em>)</span> 成立且<span class="math inline"><em>c</em></span> 是该论域内的一个元素，则<span class="math inline"><em>P</em>(<em>c</em>)</span> 成立。</li><li>全称引入（UG规则）：若对某一论域内的任意 <spanclass="math inline"><em>c</em></span>，<spanclass="math inline"><em>P</em>(<em>c</em>)</span> 均成立，则 <spanclass="math inline">∀<em>x</em><em>P</em>(<em>x</em>)</span> 成立。</li><li>存在实例（EI规则）：若 <spanclass="math inline">∃<em>x</em><em>P</em>(<em>x</em>)</span> 成立且<span class="math inline"><em>c</em></span>是该论域内满足要求的一个元素，则 <spanclass="math inline"><em>P</em>(<em>c</em>)</span> 成立。</li><li>存在引入（EG规则）：若某一论域内的某个 <spanclass="math inline"><em>c</em></span> 使 <spanclass="math inline"><em>P</em>(<em>c</em>)</span> 成立，则 <spanclass="math inline">∃<em>x</em><em>P</em>(<em>x</em>)</span> 成立。</li></ol><h4 id="间接证明">1.5.3 间接证明</h4><p>反证法是一种重要的间接证明方式：要证明 <spanclass="math inline"><em>s</em> → <em>c</em></span>是永真式，即证明其逆否命题 <spanclass="math inline">¬<em>c</em> → ¬<em>s</em></span>是永真式，其逆否命题也可以写做 <spanclass="math inline"><em>c</em> ∨ ¬<em>s</em></span>，其否定形式就是<span class="math inline">¬<em>c</em> ∧ <em>s</em></span>，若能证明<span class="math inline">¬<em>c</em> ∧ <em>s</em></span>为矛盾式，则证明 <spanclass="math inline"><em>s</em> → <em>c</em></span> 为永真式。</p><h2 id="集合函数基数矩阵">02 集合、函数、基数、矩阵</h2><h3 id="集合">2.1 集合</h3><p>定义集合的方法：</p><ol type="1"><li>花名册法：列出集合中全部的元素</li><li>集合构造器：通过描述成员的性质来构造集合</li><li>文氏图</li></ol><blockquote><p>注意：空集表示为 <span class="math inline">{}</span> 或 <spanclass="math inline">∅</span>，但 <span class="math inline">{∅}</span>不是空集！</p></blockquote><p>两个集合相等：当且仅当两个集合 <spanclass="math inline"><em>A</em></span> 和 <spanclass="math inline"><em>B</em></span> 中的元素都相等时，<spanclass="math inline"><em>A</em></span> 和 <spanclass="math inline"><em>B</em></span> 相等，即 <spanclass="math inline">∀<em>x</em>(<em>x</em>∈<em>A</em>↔︎<em>x</em>∈<em>B</em>)</span>。</p><h4 id="子集">2.1.1 子集</h4><ol type="1"><li>A 是 B 的子集写做 <spanclass="math inline"><em>A</em> ⊆ <em>B</em></span>，当且仅当 <spanclass="math inline">∀<em>x</em>(<em>x</em>∈<em>A</em>→<em>x</em>∈<em>B</em>)</span>成立</li><li>空集是任何集合的子集，一个集合本身是子集的子集</li><li>A 是 B 的真子集写做 <spanclass="math inline"><em>A</em> ⊊ <em>B</em></span>，当且仅当 <spanclass="math inline">∀<em>x</em>(<em>x</em>∈<em>A</em>→<em>x</em>∈<em>B</em>) ∧ ∃<em>x</em>(<em>x</em>∈<em>B</em>∧<em>x</em>∉<em>A</em>)</span>成立</li><li>基数：集合 <span class="math inline"><em>S</em></span>中不重复元素的个数称为 <span class="math inline"><em>S</em></span>的基数，记作 <span class="math inline">|<em>S</em>|</span></li><li><strong>幂集</strong>：<span class="math inline"><em>S</em></span>的幂集是 <span class="math inline"><em>S</em></span>所有子集构成的集合，记作 <spanclass="math inline">𝒫(<em>S</em>)</span>，一个有 <spanclass="math inline"><em>n</em></span> 个元素的集合的幂集的基数为 <spanclass="math inline">2<sup><em>n</em></sup></span>（注意：幂集中包括一个空集<span class="math inline">∅</span>）</li></ol><h4 id="笛卡尔积">2.1.2 笛卡尔积</h4><ol type="1"><li>有序 n 元组</li><li>笛卡尔积：集合 <span class="math inline"><em>A</em></span> 和 <spanclass="math inline"><em>B</em></span> 的笛卡尔积用 <spanclass="math inline"><em>A</em> × <em>B</em></span> 来表示，定义为 <spanclass="math inline"><em>A</em> × <em>B</em> = {(<em>a</em>,<em>b</em>)|<em>a</em> ∈ <em>A</em> ∧ <em>b</em> ∈ <em>B</em>}</span>，其中(a,b) 称为序偶</li><li><span class="math inline"><em>n</em></span> 个集合的笛卡尔积：<spanclass="math inline"><em>A</em><sub>1</sub> × <em>A</em><sub>2</sub> × … × <em>A</em><sub><em>n</em></sub> = {(<em>a</em><sub>1</sub>,<em>a</em><sub>2</sub>,…,<em>a</em><sub><em>n</em></sub>)|<em>a</em><sub><em>i</em></sub> ∈ <em>A</em><sub><em>i</em></sub>, <em>f</em><em>o</em><em>r</em> <em>i</em> = 1, 2, …, <em>n</em>}</span></li></ol><h4 id="真值集">2.1.3 真值集</h4><p>给定谓词 <span class="math inline"><em>P</em></span> 和论域 <spanclass="math inline"><em>D</em></span>，定义 <spanclass="math inline"><em>P</em></span> 的真值集为 <spanclass="math inline"><em>D</em></span> 中使 <spanclass="math inline"><em>P</em>(<em>x</em>)</span> 为真的元素 <spanclass="math inline"><em>x</em></span> 组成的集合。</p><p><span class="math inline"><em>P</em>(<em>x</em>)</span> 的真值集记为<spanclass="math inline">{<em>x</em> ∈ <em>D</em>|<em>P</em>(<em>x</em>)}</span>。</p><p>当且仅当 <span class="math inline"><em>P</em></span> 的真值集为 <spanclass="math inline"><em>U</em></span> 时，<spanclass="math inline">∀<em>x</em><em>P</em>(<em>x</em>)</span> 在论域<span class="math inline"><em>U</em></span> 上为真。</p><p>当且仅当 <span class="math inline"><em>P</em></span>的真值集非空时，<spanclass="math inline">∃<em>x</em><em>P</em>(<em>x</em>)</span> 在论域<span class="math inline"><em>U</em></span> 上为真。</p><h3 id="集合运算">2.2 集合运算</h3><h4 id="运算">2.2.1 运算</h4><ol type="1"><li>并</li><li>交</li><li>不相交（交集为空）</li><li>容斥原理：<spanclass="math inline">|<em>A</em>∪<em>B</em>| = |<em>A</em>| + |<em>B</em>| − |<em>A</em>∩<em>B</em>|</span></li><li>差</li><li>补：令 <span class="math inline"><em>A</em></span> 的全集为 <spanclass="math inline"><em>U</em></span>，则 <spanclass="math inline"><em>A</em></span> 关于 <spanclass="math inline"><em>U</em></span> 的补集为 <spanclass="math inline">$\overline A=U-A$</span>，也可记作 <spanclass="math inline"> ∼ <em>A</em></span></li><li>对称差：<span class="math inline"><em>A</em></span> 和 <spanclass="math inline"><em>B</em></span> 的对称差是指 <spanclass="math inline"><em>A</em></span> 和 <spanclass="math inline"><em>B</em></span>含有的但并不共同含有的元素构成的集合，记作 <spanclass="math inline"><em>A</em> ⊕ <em>B</em> = (<em>A</em>−<em>B</em>) ∪ (<em>B</em>−<em>A</em>)</span></li></ol><h4 id="集合恒等式">2.2.2 集合恒等式</h4><p>重点关注分配律、德摩根律、吸收律、互补律</p><p>证明集合相等：</p><ol type="1"><li>证明集合互为对方的子集；</li><li>使用集合构造器和逻辑等价式；</li><li>使用成员表（可以把 <span class="math inline">∪</span> 和 <spanclass="math inline">∩</span> 分别看作 <span class="math inline">∨</span>和 <span class="math inline">∧</span>，然后就直接构造真值表）；</li><li>使用集合恒等式推导。</li></ol><h3 id="函数">2.3 函数</h3><p>定义域、陪域、值域、像、原像</p><p>定义：<spanclass="math inline">(<em>f</em><sub>1</sub>+<em>f</em><sub>2</sub>)(<em>x</em>) = <em>f</em><sub>1</sub>(<em>x</em>) + <em>f</em><sub>2</sub>(<em>x</em>)</span>、<spanclass="math inline"><em>f</em><sub>1</sub><em>f</em><sub>2</sub>(<em>x</em>) = <em>f</em><sub>1</sub>(<em>x</em>)<em>f</em><sub>2</sub>(<em>x</em>)</span></p><p>单射(一对一)函数：满足一对一关系的函数，严格单调递增和严格单调递减的函数一定是单射函数。</p><p>映上(满射)函数：陪域中的每一个元素都能对应到定义域中的一个元素，称为映上函数（也可以理解为陪域和值域相等）。</p><p>双射(一一对应)函数：一个函数既是单射函数又是满射函数，则称为双射函数。</p><p>恒等函数：<spanclass="math inline"><em>f</em>(<em>x</em>) = <em>x</em></span></p><p>反函数：只有单射函数和映上函数才有反函数。</p><p>复合函数：<spanclass="math inline">(<em>f</em>◦<em>g</em>)(<em>a</em>) = <em>f</em>(<em>g</em>(<em>a</em>))</span></p><h3 id="基数">2.4 基数</h3><p>当且仅当存在一个<strong>单射函数</strong> <spanclass="math inline"><em>f</em></span> 使得 <spanclass="math inline"><em>f</em>(<em>A</em>) = <em>B</em></span> 时，集合<span class="math inline"><em>A</em></span> 和集合 <spanclass="math inline"><em>B</em></span> 有相同的基数。</p><p>如果一个无限集 <span class="math inline"><em>S</em></span>是可数（可数是指能枚举）的，那我们就称 <spanclass="math inline"><em>S</em></span> 有基数 <spanclass="math inline">ℵ<sub>0</sub></span>（阿里夫零）。</p><p>可数集的子集（无论是有限子集还是无限子集）当然也是可数集。</p><p>一个集合的子集不可数，则该集合也不可数。</p><p>如果存在单射函数 <span class="math inline"><em>f</em></span> 将 <spanclass="math inline"><em>A</em></span> 映射到 <spanclass="math inline"><em>B</em></span>，还存在单射函数 <spanclass="math inline"><em>g</em></span> 将 <spanclass="math inline"><em>B</em></span> 映射到 <spanclass="math inline"><em>A</em></span>，则 <spanclass="math inline">|<em>A</em>| = |<em>B</em>|</span>，即 <spanclass="math inline"><em>A</em></span> 与 <spanclass="math inline"><em>B</em></span>之间存在一一对应关系（双射函数）。</p><h3 id="矩阵">2.5 矩阵</h3><p>唯一陌生的点：布尔积（⊙）、布尔积的幂（<spanclass="math inline"><em>A</em><sup>[<em>p</em>]</sup></span>）</p><h2 id="计数">03 计数</h2><h3 id="鸽巢原理">3.1 鸽巢原理</h3><p>陈述：k + 1 只鸽子要飞往 k 个鸽巢，则有一个鸽巢至少有 2 只鸽子。</p><p>推论 1：一个从 <span class="math inline"><em>k</em> + 1</span>个甚至更多元素到 <span class="math inline"><em>k</em></span>个元素的集合的映射 <span class="math inline"><em>f</em></span>一定不是单射函数。</p><p>广义鸽巢原理：如果 N 个物体放入 k个盒子，那么至少有一个盒子包含了至少 <spanclass="math inline">⌈<em>N</em>/<em>k</em>⌉</span> 个物体。</p><p>定理：每个由 <spanclass="math inline"><em>n</em><sup>2</sup> + 1</span>个不同实数构成的序列都包含一个长为 <spanclass="math inline"><em>n</em> + 1</span>的严格递增子序列或严格递减子序列。</p><p>拉姆齐数：<spanclass="math inline"><em>R</em>(<em>m</em>,<em>n</em>)</span>，表示一个舞会上，使得或者<span class="math inline"><em>m</em></span> 个人两两是朋友，或者 <spanclass="math inline"><em>n</em></span> 个人两两是敌人的最少人数。</p><h3 id="排列组合">3.2 排列组合</h3><p><span class="math inline"><em>r</em></span> 排列： 对一个集合中 <spanclass="math inline"><em>r</em></span> 个元素的有序排列称为 <spanclass="math inline"><em>r</em></span> 排列。</p><p>具有 <span class="math inline"><em>n</em></span> 个不同元素的集合的<span class="math inline"><em>r</em></span> 排列数是 <spanclass="math inline">$P(n,r)=n(n-1)(n-2)\dots(n-r+1)=\frac{n!}{(n-r)!}$</span>。</p><p><span class="math inline"><em>r</em></span> 组合： 对一个集合中 <spanclass="math inline"><em>r</em></span> 个元素的无序排列称为 <spanclass="math inline"><em>r</em></span> 组合。</p><p>具有 <span class="math inline"><em>n</em></span> 个不同元素的集合的<span class="math inline"><em>r</em></span> 组合数是 <spanclass="math inline">$C(n,r)=\frac{n!}{r!(n-r)!}$</span>，组合数的性质<spanclass="math inline"><em>C</em>(<em>n</em>,<em>r</em>) = <em>C</em>(<em>n</em>,<em>n</em>−<em>r</em>)</span>。</p><p>二项式定理： <span class="math display">$$(x+y)^n=\sum_{j=0}^{n}C(n,j)x^{n-j}y^j$$</span> 推论：设 <span class="math inline"><em>n</em></span>为非负整数，令 <span class="math inline"><em>x</em> = 1</span> 和 <spanclass="math inline"><em>y</em> = 1</span>，我们有 <spanclass="math display">$$\sum_{k=0}^{n}C(n,k)=2^n$$</span></p><p><span class="math display">$$\sum_{k=0}^{n}(-1)^kC(n,k)=0$$</span></p><p><span class="math display">$$\sum_{k=0}^{n}2^kC(n,k)=3^n$$</span></p><p>帕斯卡恒等式： <spanclass="math display"><em>C</em><sub><em>k</em></sub><sup><em>n</em> + 1</sup> = <em>C</em><sub><em>k</em> − 1</sub><sup><em>n</em></sup> + <em>C</em><sub><em>k</em></sub><sup><em>n</em></sup></span>范德蒙德恒等式： <span class="math display">$$C_{r}^{m+n}=\sum_{k=0}^{r}C_{r-k}^mC^n_k$$</span> 推论4：如果 n 是一个非负整数，那么 <spanclass="math display">$$C_n^{2n}=\sum_{k=0}^{n}(C_{k}^{n})^2$$</span> 定理4：设 n 和 r 是非负整数，且有 <spanclass="math inline"><em>r</em> ≤ <em>n</em></span>，那么 <spanclass="math display">$$C_{r+1}^{n+1}=\sum_{j=r}^{n}C_{r}^{j}$$</span></p><h3 id="排列组合的推广">3.3 排列组合的推广</h3><p>定理1：具有n个对象的集合允许重复的r排列数为<spanclass="math inline"><em>n</em><sup><em>r</em></sup></span>。</p><p>定理2：具有n个对象的集合允许重复的r组合数为<spanclass="math inline"><em>C</em>(<em>n</em>+<em>r</em>−1,<em>r</em>)</span>。</p><p>具有不可区别物体的集合的排列：类型1的相同的物体有<spanclass="math inline"><em>n</em><sub>1</sub></span>个，类型2的相同的物体有<spanclass="math inline"><em>n</em><sub>2</sub></span>个，……，类型k的相同的物体有<spanclass="math inline"><em>n</em><sub><em>k</em></sub></span>个，那么<spanclass="math inline"><em>n</em></span>个物体的不同排列数是<spanclass="math inline">$\frac{n!}{n_1!n_2!\dots n_k!}$</span>。</p><p>把物体放入盒子：将<spanclass="math inline"><em>n</em></span>个不同的物体分配到<spanclass="math inline"><em>k</em></span>个不同的盒子使得<spanclass="math inline"><em>n</em><sub><em>i</em></sub></span>个物体放入第<spanclass="math inline"><em>i</em></span>（<spanclass="math inline"><em>i</em> = 1, 2, ..., <em>k</em></span>）个盒子的方式数为<spanclass="math inline">$\frac{n!}{n_1!n_2!\dots n_k!}$</span>。</p><h2 id="求解线性递推关系式">04 求解线性递推关系式</h2><p>所谓求解线性递推关系式（linear recurrencerelation），其实就是高中数列的通项公式求解。</p><h3 id="线性常系数齐次递推关系式">4.1 线性常系数齐次递推关系式</h3><p>一个常系数 <span class="math inline"><em>k</em></span>阶线性齐次递推关系是形如 <spanclass="math inline"><em>a</em><sub><em>n</em></sub> = <em>c</em><sub>1</sub><em>a</em><sub><em>n</em> − 1</sub> + <em>c</em><sub>2</sub><em>a</em><sub><em>n</em> − 2</sub> + … + <em>c</em><sub><em>k</em></sub><em>a</em><sub><em>n</em> − <em>k</em></sub></span>的递推关系，其中 <spanclass="math inline"><em>c</em><sub>1</sub>, <em>c</em><sub>2</sub>, …, <em>c</em><sub><em>k</em></sub></span>为实数，且 <spanclass="math inline"><em>c</em><sub><em>k</em></sub> ≠ 0</span>（若 <spanclass="math inline"><em>c</em><sub><em>k</em></sub></span> 为 <spanclass="math inline">0</span> 的话就不是 <spanclass="math inline"><em>k</em></span> 阶了）。</p><p>求解线性常系数齐次递推关系式：</p><p><strong>基本方法</strong>是寻找形如 <spanclass="math inline"><em>a</em><sub><em>n</em></sub> = <em>r</em><sup><em>n</em></sup></span>的解，其中 <span class="math inline"><em>r</em></span>是常数，代入到递推关系式就有： <spanclass="math display"><em>r</em><sup><em>n</em></sup> = <em>c</em><sub>1</sub><em>r</em><sup><em>n</em> − 1</sup> + <em>c</em><sub>2</sub><em>r</em><sup><em>n</em> − 2</sup> + … + <em>c</em><sub><em>k</em></sub><em>r</em><sup><em>n</em> − <em>k</em></sup></span>等式两边同时除以 <spanclass="math inline"><em>r</em><sup><em>n</em> − <em>k</em></sup></span>，移项得到<strong>特征方程</strong>：<spanclass="math display"><em>r</em><sup><em>k</em></sup> − <em>c</em><sub>1</sub><em>r</em><sup><em>k</em> − 1</sup> − <em>c</em><sub>2</sub><em>r</em><sup><em>k</em> − 2</sup> − … − <em>c</em><sub><em>k</em> − 1</sub><em>r</em> − <em>c</em><sub><em>k</em></sub> = 0</span>当且仅当 <span class="math inline"><em>r</em></span>是特征方程的解时，具有 <spanclass="math inline"><em>a</em><sub><em>n</em></sub> = <em>r</em><sup><em>n</em></sup></span>的序列 <spanclass="math inline">{<em>a</em><sub><em>n</em></sub>}</span>是一个解。特征方程的解称为递推关系的特征根，可用特根给出递推关系的所有解的显式表达式。</p><blockquote><p>特征方程其实就是一个一元 <span class="math inline"><em>k</em></span>次方程，可证明的是，当 <span class="math inline"><em>k</em> ≥ 5</span>时，方程没有解析解。</p></blockquote><h4 id="单根">4.1.1 单根</h4><p>定理 1：若特征方程 <spanclass="math inline"><em>r</em><sup>2</sup> − <em>c</em><sub>1</sub><em>r</em> − <em>c</em><sub>2</sub> = 0</span>有两个不相等的根 <spanclass="math inline"><em>r</em><sub>1</sub></span>、<spanclass="math inline"><em>r</em><sub>2</sub></span>，当且仅当 <spanclass="math inline"><em>a</em><sub><em>n</em></sub> = <em>α</em><sub>1</sub><em>r</em><sub>1</sub><sup><em>n</em></sup> + <em>α</em><sub>2</sub><em>r</em><sub>2</sub><sup><em>n</em></sup></span>时，<span class="math inline">{<em>a</em><sub><em>n</em></sub>}</span>就是递推关系式 <spanclass="math inline"><em>a</em><sub><em>n</em></sub> = <em>c</em><sub>1</sub><em>a</em><sub><em>n</em> − 1</sub> + <em>c</em><sub>2</sub><em>a</em><sub><em>n</em> − 2</sub></span>的解。（其中 <span class="math inline"><em>α</em><sub>1</sub></span> 和<span class="math inline"><em>α</em><sub>2</sub></span>可以靠初始条件得到）</p><h4 id="重根">4.1.2 重根</h4><p>定理 2：若特征方程 <spanclass="math inline"><em>r</em><sup>2</sup> − <em>c</em><sub>1</sub><em>r</em> − <em>c</em><sub>2</sub> = 0</span>有一个重根 <spanclass="math inline"><em>r</em><sub>0</sub></span>，当且仅当 <spanclass="math inline"><em>a</em><sub><em>n</em></sub> = <em>α</em><sub>1</sub><em>r</em><sub>0</sub><sup><em>n</em></sup> + <em>α</em><sub>2</sub><em>n</em><em>r</em><sub>0</sub><sup><em>n</em></sup></span>时，<span class="math inline">{<em>a</em><sub><em>n</em></sub>}</span>就是递推关系式 <spanclass="math inline"><em>a</em><sub><em>n</em></sub> = <em>c</em><sub>1</sub><em>a</em><sub><em>n</em> − 1</sub> + <em>c</em><sub>2</sub><em>a</em><sub><em>n</em> − 2</sub></span>的解。（其中 <span class="math inline"><em>α</em><sub>1</sub></span> 和<span class="math inline"><em>α</em><sub>2</sub></span>可以靠初始条件得到）</p><h4 id="任意阶">4.1.3 任意阶</h4><p>定理 3：若特征方程 <spanclass="math inline"><em>r</em><sup><em>k</em></sup> − <em>c</em><sub>1</sub><em>r</em><sup><em>k</em> − 1</sup> − … − <em>c</em><sub><em>k</em></sub> = 0</span>有 <span class="math inline"><em>k</em></span> 个不相等的根 <spanclass="math inline"><em>r</em><sub>1</sub>、<em>r</em><sub>2</sub>、…、<em>r</em><sub><em>k</em></sub></span>，当且仅当<spanclass="math inline"><em>a</em><sub><em>n</em></sub> = <em>α</em><sub>1</sub><em>r</em><sub>1</sub><sup><em>n</em></sup> + <em>α</em><sub>2</sub><em>r</em><sub>2</sub><sup><em>n</em></sup> + … + <em>α</em><sub><em>k</em></sub><em>r</em><sub><em>k</em></sub><sup><em>n</em></sup></span>时，<span class="math inline">{<em>a</em><sub><em>n</em></sub>}</span>就是递推关系式 <spanclass="math inline"><em>a</em><sub><em>n</em></sub> = <em>c</em><sub>1</sub><em>a</em><sub><em>n</em> − 1</sub> + <em>c</em><sub>2</sub><em>a</em><sub><em>n</em> − 2</sub> + … + <em>c</em><sub><em>k</em></sub><em>a</em><sub><em>n</em> − <em>k</em></sub></span>的解。</p><h4 id="部分重根">4.1.4 部分重根</h4><p>定理 4：假设特征方程 <spanclass="math inline"><em>r</em><sup><em>k</em></sup> − <em>c</em><sub>1</sub><em>r</em><sup><em>k</em> − 1</sup> − … − <em>c</em><sub><em>k</em></sub> = 0</span>有 <span class="math inline"><em>t</em></span>个不同的根，每个根的重数分别为 <spanclass="math inline"><em>m</em><sub>1</sub>, <em>m</em><sub>2</sub>, …, <em>m</em><sub><em>t</em></sub></span>，则解为<span class="math display">$$a_n=\sum_{i=1}^{t}(\sum_{j=1}^{m_t}\alpha_{t,j}n^{j-1})r_t^n$$</span></p><h3 id="线性常系数非齐次递推关系式">4.2 线性常系数非齐次递推关系式</h3><p>形如 <spanclass="math inline"><em>a</em><sub><em>n</em></sub> = <em>c</em><sub>1</sub><em>a</em><sub><em>n</em> − 1</sub> + <em>c</em><sub>2</sub><em>a</em><sub><em>n</em> − 2</sub> + … + <em>c</em><sub><em>k</em></sub><em>a</em><sub><em>n</em> − <em>k</em></sub> + <em>F</em>(<em>n</em>)</span>的递推式就是线性常系数非齐次递推关系式，其中 <spanclass="math inline"><em>F</em>(<em>n</em>)</span> 是一个只依赖与 <spanclass="math inline"><em>n</em></span> 且不等于零的函数。其中，<spanclass="math inline"><em>a</em><sub><em>n</em></sub> = <em>c</em><sub>1</sub><em>a</em><sub><em>n</em> − 1</sub> + <em>c</em><sub>2</sub><em>a</em><sub><em>n</em> − 2</sub> + … + <em>c</em><sub><em>k</em></sub><em>a</em><sub><em>n</em> − <em>k</em></sub></span>叫做相伴的线性齐次递推关系。</p><p>类似于线性代数中的特解+通解，线性常系数非齐次递推关系式的每个解，都是一个特解和相伴线性齐次递推关系的一个解的和。</p><p>求线性常系数非齐次递推关系式的通解：</p><ol type="1"><li>先求相伴的线性齐次递推关系的解</li><li>根据 <span class="math inline"><em>F</em>(<em>n</em>)</span>的形式，设出一个特解形式，将特解代回通项公式中解出该特解</li><li>将两个解加起来就是通解</li></ol><p>将初始条件带入通解，得到一个特解。</p><p>下面的定理给出了特解形式的设法：</p><blockquote><p>若一个非齐次递推关系式中的非齐次项 <spanclass="math inline"><em>F</em>(<em>n</em>)</span> 有如下形式： <spanclass="math display"><em>F</em>(<em>n</em>) = (<em>b</em><sub><em>t</em></sub><em>n</em><sup><em>t</em></sup>+<em>b</em><sub><em>t</em> − 1</sub><em>n</em><sup><em>t</em> − 1</sup>+…+<em>b</em><sub>0</sub>)<em>s</em><sup><em>n</em></sup></span>若其中 <span class="math inline"><em>s</em></span>不是相伴齐次递推关系的特征方程的根，存在一个下述形式的特解： <spanclass="math display">(<em>p</em><sub><em>t</em></sub><em>n</em><sup><em>t</em></sup>+<em>p</em><sub><em>t</em> − 1</sub><em>n</em><sup><em>t</em> − 1</sup>+…+<em>p</em><sub>0</sub>)<em>s</em><sup><em>n</em></sup></span>若其中 <span class="math inline"><em>s</em></span>是相伴齐次递推关系的特征方程的 <spanclass="math inline"><em>m</em></span> 重根，存在一个下述形式的特解：<spanclass="math display"><em>n</em><sup><em>m</em></sup>(<em>p</em><sub><em>t</em></sub><em>n</em><sup><em>t</em></sup>+<em>p</em><sub><em>t</em> − 1</sub><em>n</em><sup><em>t</em> − 1</sup>+…+<em>p</em><sub>0</sub>)<em>s</em><sup><em>n</em></sup></span></p></blockquote><h2 id="容斥原理">05 容斥原理</h2><p>基本的容斥： <spanclass="math display">|<em>A</em>∪<em>B</em>| = |<em>A</em>| + |<em>B</em>| − |<em>A</em>∩<em>B</em>|</span>三个有限集： <spanclass="math display">|<em>A</em>∪<em>B</em>∪<em>C</em>| = |<em>A</em>| + |<em>B</em>| + |<em>C</em>| − |<em>A</em>∩<em>B</em>| − |<em>B</em>∩<em>C</em>| − |<em>A</em>∩<em>C</em>| + |<em>A</em>∩<em>B</em>∩<em>C</em>|</span>推广到 <span class="math inline"><em>n</em></span> 个有限集： <spanclass="math display">$$\begin{aligned}|A_1\cup A_2\cup\dots\cup A_n|=&amp;\sum_{i\le i\len}|A_i|\\&amp;-\sum_{1\le i\le j\le n}|A_i\cap A_j|\\&amp;+\sum_{1\lei\le j\le k\le n}|A_i\cap A_j\capA_k|\\&amp;-\dots\\&amp;+(-1)^{n+1}|A_1\cap A_2\cap\dots\cap A_n|\end{aligned}$$</span></p><h3 id="错位排列">5.1 错位排列</h3><p><span class="math inline"><em>n</em></span>个元素的集合的错位排列数为 <span class="math display">$$D_n=n![1-\frac{1}{1!}+\frac{1}{2!}-\frac{1}{3!}+\dots+(-1)^n\frac{1}{n!}]$$</span></p><h3 id="埃拉托色尼筛伊拉脱森筛">5.2 埃拉托色尼筛(伊拉脱森筛)</h3><p>埃拉托色尼筛是一种找出一定范围内所有素数的算法。</p><p>其原理很简单：假设要求出 100 以内所有素数，则先列出 2-100范围内所有的整数，执行下面的步骤：</p><ol type="1"><li>选出序列中未被标记的第一个数，将其之后所有倍数都进行标记；</li><li>重复步骤 1，直到找不到未被标记的数为止（要跳过步骤 1选取的数）；</li><li>剩余序列中未被标记的元素就是 100 以内所有的素数。</li></ol><p>时间复杂度为 <spanclass="math inline"><em>O</em>(<em>n</em>loglog<em>n</em>)</span>。</p><p>埃拉托色尼筛的本质是一种容斥，基于不超过 100 的合数肯定有一个不超过10 的素因子，因此选出 10 以内所有的素因子有 2、3、5、7，令 <spanclass="math inline"><em>P</em><sub>1</sub></span> 表示能被 2整除的性质，<span class="math inline"><em>P</em><sub>2</sub></span>表示能被 3 整除的性质，<spanclass="math inline"><em>P</em><sub>3</sub></span> 表示能被 5整除的性质，<span class="math inline"><em>P</em><sub>4</sub></span>表示能被 7 整除的性质，则不超过 100 的素数的个数就是 <spanclass="math inline"><em>N</em>(<em>P</em><sub>1</sub><sup>′</sup><em>P</em><sub>2</sub><sup>′</sup><em>P</em><sub>3</sub><sup>′</sup><em>P</em><sub>4</sub><sup>′</sup>)</span>，然后应用容斥原理就可以求了。</p><h2 id="关系">06 关系</h2><h3 id="二元关系">6.1 二元关系</h3><p>由有序对构成的集合称为一个关系，例如（1,b）。一个关系表示一个集合<span class="math inline"><em>A</em></span> 到另一个集合 <spanclass="math inline"><em>B</em></span> 的映射，若存在 <spanclass="math inline">1 ∈ <em>A</em></span>，<spanclass="math inline"><em>b</em> ∈ <em>B</em></span>，且有关系 <spanclass="math inline">(1,<em>b</em>)</span>，则写做 <spanclass="math inline">1<em>R</em><em>b</em></span>，否则在 R标记上画一条斜对角线表示不存在这样的关系 <spanclass="math inline">1<em>R</em≯<em>b</em></span>。</p><p>关系其实就是两个集合的笛卡尔积的子集。那么，含有 <spanclass="math inline"><em>n</em></span> 个元素的集合 A上最多有多少关系？其实就是考虑 <spanclass="math inline"><em>A</em> × <em>A</em></span> 的子集个数，<spanclass="math inline"><em>A</em> × <em>A</em></span> 有 <spanclass="math inline"><em>n</em><sup>2</sup></span> 个元素，<spanclass="math inline"><em>m</em></span> 个元素构成的集合有 <spanclass="math inline">2<sup><em>m</em></sup></span> 个子集，所以 <spanclass="math inline"><em>A</em> × <em>A</em></span> 有 <spanclass="math inline">2<sup>|<em>A</em>|<sup>2</sup></sup></span>个关系。</p><p>二元关系的记号：</p><ol type="1"><li>前缀表示法：<spanclass="math inline"><em>R</em>(<em>x</em>,<em>y</em>)</span></li><li>中缀表示法：<spanclass="math inline"><em>x</em><em>R</em><em>y</em></span></li><li>后缀表示法：<spanclass="math inline">(<em>x</em>,<em>y</em>) ∈ <em>R</em></span></li></ol><p>函数是一种关系，但关系不一定是函数，因为关系可以一对多。</p><h4 id="性质">6.1.1 性质</h4><p>若对于 <span class="math inline"><em>A</em></span> 中每个元素 <spanclass="math inline"><em>a</em></span>，都存在 <spanclass="math inline">(<em>a</em>,<em>a</em>) ∈ <em>R</em></span>，则称<span class="math inline"><em>R</em></span>是自反的。如果这样的有序对一个都不存在，那就称 <spanclass="math inline"><em>R</em></span>是反自反的。（空关系既是自反的又是反自反的）</p><p>若对任意 <spanclass="math inline">(<em>a</em>,<em>b</em>) ∈ <em>R</em></span> 都有<spanclass="math inline">(<em>b</em>,<em>a</em>) ∈ <em>R</em></span>，则称<span class="math inline"><em>R</em></span> 是对称的。如果 <spanclass="math inline">(<em>a</em>,<em>b</em>) ∈ <em>R</em></span> 并且<spanclass="math inline">(<em>b</em>,<em>a</em>) ∈ <em>R</em></span>，则有<span class="math inline"><em>a</em> = <em>b</em></span>，则称 <spanclass="math inline"><em>R</em></span>是反对称的。（恒等关系既是对称的又是反对称的）</p><p>若对 <spanclass="math inline">(<em>a</em>,<em>b</em>) ∈ <em>R</em></span> 和 <spanclass="math inline">(<em>b</em>,<em>c</em>) ∈ <em>R</em></span>，都存在<spanclass="math inline">(<em>a</em>,<em>c</em>) ∈ <em>R</em></span>，则称<span class="math inline"><em>R</em></span> 是传递的。</p><h4 id="关系的合成">6.1.2 关系的合成</h4><p>假设 <span class="math inline"><em>R</em><sub>1</sub></span> 是集合<span class="math inline"><em>A</em></span> 到集合 <spanclass="math inline"><em>B</em></span> 的关系，<spanclass="math inline"><em>R</em><sub>2</sub></span> 是集合 <spanclass="math inline"><em>B</em></span> 到集合 <spanclass="math inline"><em>C</em></span> 的关系，则 <spanclass="math inline"><em>R</em><sub>1</sub></span> 和 <spanclass="math inline"><em>R</em><sub>2</sub></span> 的合成是 <spanclass="math inline"><em>A</em></span> 到 <spanclass="math inline"><em>C</em></span> 的关系，记作 <spanclass="math inline"><em>R</em><sub>1</sub> ∘ <em>R</em><sub>2</sub></span>。</p><p>关系的幂就是多个关系合成：<spanclass="math inline"><em>R</em><sup><em>n</em> + 1</sup> = <em>R</em><sup><em>n</em></sup> ∘ <em>R</em></span>。</p><h4 id="逆关系">6.1.3 逆关系</h4><p>记作 <spanclass="math inline"><em>R</em><sup>−1</sup> = {(<em>y</em>,<em>x</em>)|(<em>x</em>,<em>y</em>) ∈ <em>R</em>}</span>。</p><p>定理：<spanclass="math inline">(<em>R</em><sub>1</sub>∘<em>R</em><sub>2</sub>)<sup>−1</sup> = <em>R</em><sub>2</sub><sup>−1</sup> ∘ <em>R</em><sub>1</sub><sup>−1</sup></span>。</p><h3 id="关系表示">6.2 关系表示</h3><h4 id="矩阵-1">6.2.1 矩阵</h4><ol type="1"><li>自反关系的矩阵对角线元素都是 1。</li><li>对称关系的矩阵是对称矩阵。</li><li>反对称关系的矩阵对角线元素都是 1，并且 <spanclass="math inline"><em>a</em><sub><em>i</em><em>j</em></sub></span> 在<span class="math inline"><em>i</em> ≠ <em>j</em></span> 的情况下，若<spanclass="math inline"><em>a</em><sub><em>i</em><em>j</em></sub> = 1</span>，则<spanclass="math inline"><em>a</em><sub><em>j</em><em>i</em></sub> = 0</span>。</li><li>关系的交、并运算可以转换为矩阵的或、与运算。</li><li>关系的合成可以转换为矩阵的布尔积运算，布尔幂的表示要在幂次上加个方括号。</li></ol><h4 id="图">6.2.2 图</h4><ol type="1"><li>自反关系的图中的每个结点都有一个指向自己的边。</li><li>对称关系的图中每条边都是双向边。</li><li>反对称关系的图中每条边都是单边。</li><li>传递关系的图中，若 <span class="math inline"><em>a</em></span> 与<span class="math inline"><em>b</em></span> 有连线，且 <spanclass="math inline"><em>b</em></span> 与 <spanclass="math inline"><em>c</em></span> 有连线，则 <spanclass="math inline"><em>a</em></span> 与 <spanclass="math inline"><em>c</em></span> 也有连线。</li></ol><h3 id="关闭的闭包">6.3 关闭的闭包</h3><p>所谓闭包，就是指为了让关系满足某个我们所需要的性质（传递、自反、对称）而往其中添加外来关系的集合。</p><p>若某个关系 <span class="math inline"><em>R</em></span>不满足自反关系，则我们用 <spanclass="math inline"><em>r</em>(<em>R</em>)</span> 表示 <spanclass="math inline"><em>R</em></span>的一个自反闭包，该闭包有如下特点：</p><ol type="1"><li><span class="math inline"><em>r</em>(<em>R</em>)</span>是自反的</li><li><spanclass="math inline"><em>R</em> ⊂ <em>r</em>(<em>R</em>)</span></li><li>在所有满足 1、2 的集合中，<spanclass="math inline"><em>r</em>(<em>R</em>)</span> 是最小的</li></ol><p>同理还有传递闭包、对称闭包。</p><p>如果一个关系是对称（自反、对称）关系，则它本身就是自己的对称（自反、对称）闭包。</p><p>如果一个关系是另一个关系的子集，那么它的闭包也是另一个闭包的子集。</p><p>三种闭包的求法：</p><ol type="1"><li>自反闭包：令 <spanclass="math inline"><em>Δ</em> = {(<em>a</em>,<em>a</em>)|<em>a</em> ∈ <em>A</em>}</span>，则<span class="math inline"><em>R</em></span> 的自反闭包就是 <spanclass="math inline"><em>R</em> ∪ <em>Δ</em></span>。</li><li>对称闭包：<spanclass="math inline"><em>R</em> ∪ <em>R</em><sup>−1</sup></span></li><li>传递闭包：<spanclass="math inline"><em>R</em> ∪ <em>R</em><sup>2</sup> ∪ <em>R</em><sup>3</sup> ∪ …</span></li></ol><h3 id="路径">6.4 路径</h3><p>定理：<span class="math inline"><em>a</em></span> 和 <spanclass="math inline"><em>b</em></span> 之间存在一条长度为 <spanclass="math inline"><em>n</em></span> 的路径，当且仅当 <spanclass="math inline">(<em>a</em>,<em>b</em>) ∈ <em>R</em><sup><em>n</em></sup></span>.</p><h3 id="传递闭包">6.5 传递闭包</h3><p>给出连通性闭包的定义： <span class="math display">$$R^{*}=\bigcup_{n=1}^{\infty}R^{n}$$</span> 可以证明，连通性闭包和传递闭包是等价的。</p><p>令 <span class="math inline"><em>M</em><sub><em>R</em></sub></span>是定义在 <span class="math inline"><em>n</em></span> 个元素集合上的关系<span class="math inline"><em>R</em></span> 的 <spanclass="math inline">0 − 1</span> 矩阵，那么传递闭包 <spanclass="math inline"><em>R</em><sup>*</sup></span> 的 <spanclass="math inline">0 − 1</span> 矩阵是： <spanclass="math display"><em>M</em><sub><em>R</em><sup>*</sup></sub> = <em>M</em><sub><em>R</em></sub> ∨ <em>M</em><sub><em>R</em></sub><sup>[2]</sup> ∨ <em>M</em><sub><em>R</em></sub><sup>[3]</sup> ∨ … ∨ <em>M</em><sub><em>R</em></sub><sup>[<em>n</em>]</sup></span>如果使用算法实现，则计算一次矩阵的布尔积需要进行 <spanclass="math inline"><em>n</em><sup>2</sup>(2<em>n</em>−1)</span>次位运算，计算 <span class="math inline"><em>n</em></span>个矩阵的布尔积则共需要 <spanclass="math inline"><em>n</em><sup>2</sup>(2<em>n</em>−1)(<em>n</em>−1)</span>次位运算，故算法的时间复杂度为 <spanclass="math inline"><em>O</em>(<em>n</em><sup>4</sup>)</span>。</p><p>沃舍尔算法对这种朴素的计算方式进行了优化，使得只需要 <spanclass="math inline">2<em>n</em><sup>3</sup></span>次位运算就可以求出这个传递闭包。</p><h3 id="等价关系">6.6 等价关系</h3><p>定义1：当一个关系同时是自反、对称、传递的，就称其为<strong>等价关系</strong>。（tip：由于等价关系是自反的，因此定义在集合<span class="math inline"><em>A</em></span> 上的等价关系必然涵盖了集合<span class="math inline"><em>A</em></span> 中全部元素）</p><p>定义 2：假设 <spanclass="math inline"><em>a</em><em>R</em><em>b</em></span>，且 <spanclass="math inline"><em>R</em></span> 是等价关系，则称 <spanclass="math inline"><em>a</em></span> 与 <spanclass="math inline"><em>b</em></span> 等价，记作 <spanclass="math inline"><em>a</em> ∼ <em>b</em></span></p><p>定义 3：如果 <span class="math inline"><em>R</em></span> 是 <spanclass="math inline"><em>S</em></span> 上的等价关系，且 <spanclass="math inline"><em>a</em> ∈ <em>S</em></span>，则将 <spanclass="math inline"><em>R</em></span> 中所有与 <spanclass="math inline"><em>a</em></span> 有关联的元素 <spanclass="math inline"><em>s</em></span> 构成的集合称为 <spanclass="math inline"><em>a</em></span>的<strong>等价类</strong>，可表示为 <spanclass="math inline">[<em>a</em>]<sub><em>R</em></sub> = {<em>s</em>|(<em>a</em>,<em>s</em>) ∈ <em>S</em>}</span>，而<span class="math inline"><em>s</em></span> 称为 <spanclass="math inline"><em>a</em></span>的<strong>代表元</strong>。（tip：由于自反性，元素 <spanclass="math inline"><em>a</em></span> 的等价类中必定包括了 <spanclass="math inline"><em>a</em></span>）</p><p>定理 1：若 <span class="math inline"><em>R</em></span> 是定义在集合<span class="math inline"><em>A</em></span>上的等价关系，则下面三种表示等价：</p><ol type="1"><li><span class="math inline"><em>a</em><em>R</em><em>b</em></span></li><li><span class="math inline">[<em>a</em>] = [<em>b</em>]</span></li><li><spanclass="math inline">[<em>a</em>] ∩ [<em>b</em>] ≠ ∅</span></li></ol><p>等价类可以<strong>划分集合</strong>：假设 <spanclass="math inline"><em>R</em></span> 是定义在集合 <spanclass="math inline"><em>A</em></span>上的等价关系。等价类和等价类之间必不相交，并且所有等价类的并集就是等价于集合<span class="math inline"><em>A</em></span>。（可以参考模 4同余关系）</p><p>定理2：从上面的讨论我们已经知道给定一个集合和在这个集合上定义的等价关系，我们可以构造该集合的一个划分；那么反过来，给定该集合的一个划分，我们也能找到这样一个等价关系，能划分该集合。</p><h3 id="偏序">6.7 偏序</h3><h4 id="基本概念">6.7.1 基本概念</h4><p>定义 1：如果定义在集合 <span class="math inline"><em>S</em></span>上的关系 <span class="math inline"><em>R</em></span>是自反、反对称、传递的，则称其为<strong>偏序</strong>。集合 <spanclass="math inline"><em>S</em></span> 与定义在其上的偏序 <spanclass="math inline"><em>R</em></span>一起称为<strong>偏序集</strong>，用 <spanclass="math inline">(<em>S</em>,<em>R</em>)</span> 表示。集合 <spanclass="math inline"><em>S</em></span> 中的成员称为偏序集的元素。</p><p>通常使用 <span class="math inline">≼</span> 表示偏序关系，如果 <spanclass="math inline">(<em>a</em>,<em>b</em>) ∈ <em>R</em></span> 则记作<span class="math inline"><em>a</em> ≼ <em>b</em></span>，若 <spanclass="math inline"><em>a</em> ≠ <em>b</em></span>，则记作 <spanclass="math inline"><em>a</em> ≺ <em>b</em></span>，这说明 <spanclass="math inline"><em>a</em></span> 和 <spanclass="math inline"><em>b</em></span> 是可比的。</p><p>偏序集中的元素并不都是可比的，这句话的意思是，并不是所有的序偶组合都存在于偏序集中，只有存在<spanclass="math inline">(<em>a</em>,<em>b</em>) ∈ (<em>S</em>,≼)</span>时才说 <span class="math inline"><em>a</em></span> 和 <spanclass="math inline"><em>b</em></span> 可比，即有 <spanclass="math inline"><em>a</em> ≼ <em>b</em></span>，否则就是不可比的。例如在偏序集<span class="math inline">$(\textbf Z^+, |)$</span> 中（<spanclass="math inline">|</span> 表示整除关系），<spanclass="math inline">5</span> 和 <span class="math inline">7</span>就不是可比的，因为 <span class="math inline">$(5,7)\notin(\textbfZ^+,|)$</span>。</p><p>事实上，之所以说叫“偏”序，就是因为可能存在有些元素是不可比的。如果集合中的每个元素都可比，那么就称这个关系为<strong>全序</strong>，称这个集合为<strong>全序集</strong>（或线序集或链）。</p><p>对于偏序集 <span class="math inline">(<em>S</em>,≼)</span>，如果<span class="math inline">≼</span> 是全序，并且 <spanclass="math inline"><em>S</em></span>的每个非空子集都有一个最小元素，那么就称它为<strong>良序集</strong>。例如，<spanclass="math inline">$\textbf Z^+\times\textbf Z^+$</span> 的元素是形如<span class="math inline">(<em>a</em>,<em>b</em>)</span>的序偶，如果我们按顺序比较两个元素的大小，就有 <spanclass="math inline">(1,5) &lt; (2,3)</span> 或 <spanclass="math inline">(2,3) &lt; (2,6)</span>，由于 <spanclass="math inline">$\textbf Z^+$</span> 是正整数集，所以最小元素就是0，因此 <span class="math inline">$\textbf Z^+\times\textbf Z^+$</span>的最小元素就是 <span class="math inline">(0,0)</span>，这说明 <spanclass="math inline">$\textbf Z^+\times\textbf Z^+$</span> 是良序集。但<span class="math inline">$\textbf Z\times\textbf Z$</span>就不是良序集，因为其中包含负数，那就不存在这样的最小元素。</p><h4 id="哈塞图">6.7.2 哈塞图</h4><p>偏序仍然是一种关系，关系可以用图来表示，这里引出哈塞图，它忽略由于偏序的自反性和传递性而必须出现的边。如果关系是全序的（各个元素之间都存在关系，即都可比），你就会发现其哈塞图是一条链条，这就是称全序集为链的原因。</p><p>构造哈塞图的过程：</p><ol type="1"><li>去掉所有结点上的环（这是由于自反性造成的）</li><li>去掉所有这样的边 <spanclass="math inline">(<em>x</em>,<em>y</em>)</span>：存在元素 <spanclass="math inline"><em>z</em> ∈ <em>S</em></span> 满足 <spanclass="math inline"><em>x</em> ≼ <em>z</em></span> 和 <spanclass="math inline"><em>z</em> ≼ <em>y</em></span>（这是由传递性造成的）</li><li>排列每条边，使大的元素在上，去掉所有箭头（因为所有箭头都指向顶线）</li></ol><p>通过哈塞图可以很直观地看出<strong>极大元</strong>/<strong>极小元</strong>（maximal/minimal）、<strong>最大元</strong>/<strong>最小元</strong>（greatest/least），只需要看哈塞图最顶端和最底端的元素是什么就行了。但是注意，极大元/极小元是指集合中没有其他元素大于/小于该元素，但是可以无法比较，这意味着极大元/极小元允许有多个；而最大元/最小元要求集合中所有元素都严格小于/大于它，不允许存在与它处于同一层但是无法比较的元素，也就是说如果一个哈塞图中，顶端元素不止一个，那这个偏序就不存在极大元；如果底端元素不止一个，那这个偏序就不存在极小元。</p><p>再介绍<strong>上/下界</strong>、<strong>上/下确界</strong>的概念：设<span class="math inline">(<em>S</em>,≼)</span> 为偏序集，且 <spanclass="math inline"><em>A</em></span> 是 <spanclass="math inline"><em>S</em></span> 的一个子集，若 <spanclass="math inline"><em>S</em></span> 中存在一个元素 <spanclass="math inline"><em>a</em></span>，使得 <spanclass="math inline"><em>A</em></span> 中任意一个元素 <spanclass="math inline"><em>b</em></span> 都存在 <spanclass="math inline"><em>b</em> ≼ <em>a</em></span>，则称 <spanclass="math inline"><em>a</em></span> 为 <spanclass="math inline"><em>A</em></span>的上界；反之得到下界（上/下界中的元素不一定要和 <spanclass="math inline"><em>A</em></span>中元素都可比，只需比可比元素大/小就行了）。若 <spanclass="math inline"><em>a</em></span> 是 <spanclass="math inline"><em>A</em></span> 的上界集合中的<u>最小元</u>，则称<span class="math inline"><em>a</em></span> 为上确界；若 <spanclass="math inline"><em>a</em></span> 是 <spanclass="math inline"><em>A</em></span> 的下界集合中的<u>最大元</u>，则称<span class="math inline"><em>a</em></span> 为下确界。</p><h4 id="格">6.7.3 格</h4><p>如果一个偏序集的每一对元素都有上确界和下确界，就称这个偏序集为<strong>格</strong>。</p><p>下面来看一个例子（哈塞图本不应该带箭头，下图在这点上有误，请不要被误导）：</p><pre class="mermaid">graph TD    a --> b    b --> c    b --> d    c --> e    d --> e    e --> f</pre><p>假设我们选取 <spanclass="math inline">{<em>c</em>, <em>d</em>}</span>，那么凡是在它们上面的（<spanclass="math inline"><em>a</em></span> 和 <spanclass="math inline"><em>b</em></span>）都是上界，凡是在它们下面的（<spanclass="math inline"><em>e</em></span> 和 <spanclass="math inline"><em>f</em></span>）都是下界。但上确界只有 <spanclass="math inline"><em>b</em></span>，下确界只有 <spanclass="math inline"><em>e</em></span>。</p><p>假设我们选取 <spanclass="math inline">{<em>c</em>, <em>b</em>}</span>，注意，它们的上界并不只有<span class="math inline"><em>a</em></span>，还有 <spanclass="math inline"><em>b</em></span>，为什么？注意上下界的定义是 <spanclass="math inline">∀<em>b</em> ∈ <em>A</em>(<em>b</em>≼<em>a</em>)</span>，这里是允许等于的，这里显然<span class="math inline"><em>b</em> ≼ <em>b</em></span> 且 <spanclass="math inline"><em>b</em> ≼ <em>c</em></span>，所以 <spanclass="math inline"><em>b</em></span> 自然也属于上界，同理，下界有 <spanclass="math inline">{<em>c</em>, <em>e</em>, <em>f</em>}</span>。因此，<spanclass="math inline">{<em>c</em>, <em>b</em>}</span> 的上确界就是 <spanclass="math inline"><em>b</em></span>，下确界就是 <spanclass="math inline"><em>c</em></span>。</p><p>那为什么刚刚讨论 <spanclass="math inline">{<em>c</em>, <em>d</em>}</span> 的上下界时没有 <spanclass="math inline"><em>c</em></span> 和 <spanclass="math inline"><em>d</em></span>？因为 <spanclass="math inline"><em>c</em></span> 和 <spanclass="math inline"><em>d</em></span> 压根就不可比。</p><blockquote><p>这里要注意一件事：找上/下界、上/下确界时我们是在全集中找，但是找最大/小值、极大/小值时，我们仅在子集中找。例如在上例中找<span class="math inline">{<em>c</em>, <em>b</em>, <em>e</em>}</span>的极大值，我们只能找到 <spanclass="math inline"><em>b</em></span>，而不会有 <spanclass="math inline"><em>a</em></span>。</p></blockquote><p>显然，上图中任意对元素都存在上下确界，因此属于格，下面来看一个不属于格的例子。</p><pre class="mermaid">graph TD    a --> b    a --> c    b --> d    c --> e    d --> f    e --> f    b --> e    c --> d</pre><p>假设选取 <spanclass="math inline">{<em>b</em>, <em>e</em>}</span>，则上确界为 <spanclass="math inline"><em>a</em></span>，这一点毫无疑问。下界为 <spanclass="math inline">{<em>d</em>, <em>e</em>, <em>f</em>}</span>，哪个是下确界？下确界是下界集合中的最大值，根据最大值的定义，由于<span class="math inline"><em>f</em></span> 小于 <spanclass="math inline"><em>d</em></span> 和 <spanclass="math inline"><em>e</em></span>，<spanclass="math inline"><em>d</em></span> 和 <spanclass="math inline"><em>e</em></span>又不可比，所以不存在最大值，因此不存在下确界，故上图不是格。</p><h4 id="拓扑排序">6.7.4 拓扑排序</h4><p>这个简单。</p><h2 id="图论">07 图论</h2><h3 id="基本概念-1">7.1 基本概念</h3><ol type="1"><li>简单图：不存在多重边（不同的边连接同一对顶点）和环（存在指向自身的边）的图</li><li>多重图：存在多重边但不存在环的图</li><li>伪图：存在多重边或环的图</li></ol><p>上面的概念都针对无向图，下面是有向图：</p><ol type="1"><li>简单有向图：不允许出现环和多重边的有向图</li><li>有向多重图：允许出现环和多重边的有向图</li></ol><p>在某些情况下，我们可能需要一张图中既有无向的边，又有有向的边，我们称这样的边为混合图。</p><h3 id="图的术语">7.2 图的术语</h3><h4 id="基本概念-2">7.2.1 基本概念</h4><p>设有图 <spanclass="math inline"><em>G</em> = (<em>V</em>,<em>E</em>)</span>，我们将顶点<span class="math inline"><em>v</em> ∈ <em>V</em></span> 的相邻顶点记作<span class="math inline"><em>N</em>(<em>v</em>)</span>，若有顶点集合<span class="math inline"><em>A</em> ∈ <em>V</em></span>，则将 <spanclass="math inline"><em>A</em></span> 中所有顶点的相邻顶点构成的集合记作<span class="math inline"><em>V</em>(<em>A</em>)</span>，则有 <spanclass="math inline"><em>V</em>(<em>A</em>) = ⋃<sub><em>v</em> ∈ <em>A</em></sub><em>N</em>(<em>v</em>)</span>。</p><p>我们将顶点的度记作 <spanclass="math inline">deg (<em>v</em>)</span>，特殊地，如果 <spanclass="math inline"><em>v</em></span> 是带有环的无向图结点，那么环对<span class="math inline"><em>v</em></span> 的度做出双倍贡献。</p><p>度为 0 的顶点称为孤立的，度为 1 的顶点称为悬挂的。</p><p><strong>握手定理</strong>：无向图中，所有顶点度数之和为边数的两倍。</p><p>定理：无向图有偶数个度为奇数的顶点。</p><p>有向图中，将顶点的入度记为 <spanclass="math inline">deg<sup>−</sup>(<em>v</em>)</span>，将出度记为 <spanclass="math inline">deg<sup>+</sup>(<em>v</em>)</span>。</p><p>在有向图中，所有顶点的入度之和 等于 出度之和 等于 边数。</p><h4 id="特殊简单图">7.2.2 特殊简单图</h4><p>完全图：<span class="math inline"><em>n</em></span>个结点的完全图记作 <spanclass="math inline"><em>K</em><sub><em>n</em></sub></span>，完全图的特点是每个结点之间都有一条直接相连的边。</p><p>圈图：<span class="math inline"><em>n</em></span> 个顶点的圈图记作<spanclass="math inline"><em>C</em><sub><em>n</em></sub></span>，圈图的特点是所有结点围成一个圈。</p><p>轮图：在 <spanclass="math inline"><em>C</em><sub><em>n</em></sub></span>中添加一个顶点，并将这个顶点与其他顶点相连，就得到一个轮图 <spanclass="math inline"><em>W</em><sub><em>n</em></sub></span>，像一个车轮。</p><p><span class="math inline"><em>n</em></span> 立方图：记作 <spanclass="math inline"><em>Q</em><sub><em>n</em></sub></span>，有 <spanclass="math inline">2<sup><em>n</em></sup></span>个顶点，每个顶点用二进制编号，当且仅当两个顶点编号的海明距离为 1时两个顶点相连。</p><h4 id="二分图">7.2.3 二分图</h4><p>有些图可以将顶点分成两部分，就像小时候做的连线题一样。例如表示一个村庄里的婚姻状况，那么有一堆顶点代表男性，一堆顶点代表女性，有婚姻关系的就可以连在一起，由于同性无法建立婚姻关系（至少在中国的法律层面不行）所以同性顶点之间不会有连线，故可以把表示两种性别的顶点分别拉到左右两边。</p><p>若一个图能被二分，则有 <spanclass="math inline"><em>V</em><sub>1</sub>, <em>V</em><sub>2</sub> ∈ <em>V</em></span>，称<spanclass="math inline">(<em>V</em><sub>1</sub>,<em>V</em><sub>2</sub>)</span>为 <span class="math inline"><em>G</em></span>的顶点集的一个二部划分。</p><p>如何判断一个图是否为二分图：给图中每个顶点赋予两种不同的颜色，当且仅当能找到一种着色方法，使得同颜色的顶点之间不存在连线，说明这个图是二分图。</p><p>完全二分图：<span class="math inline"><em>V</em><sub>1</sub></span>和 <span class="math inline"><em>V</em><sub>2</sub></span>中的结点之间都有连线，记作 <spanclass="math inline"><em>K</em><sub><em>m</em>, <em>n</em></sub></span>，其中<span class="math inline"><em>m</em></span> 和 <spanclass="math inline"><em>n</em></span> 分别是 <spanclass="math inline"><em>V</em><sub>1</sub></span> 和 <spanclass="math inline"><em>V</em><sub>2</sub></span> 中的顶点个数。</p><h4 id="从旧图构造新图">7.2.4 从旧图构造新图</h4><p>从一个图中抽出一部分顶点和边构成一张新图，后者被称为前者的<strong>子图</strong>，如果两张图不相等，就称后者为前者的<strong>真子图</strong>。</p><p>导出子图：如果有 <spanclass="math inline"><em>G</em> = (<em>V</em>,<em>E</em>)</span>，<spanclass="math inline"><em>G</em><sup>′</sup> = (<em>W</em>,<em>F</em>)</span>，其中<spanclass="math inline"><em>W</em> ⊂ <em>V</em></span>，且被选出的结点间，该有的边全都有，就称<span class="math inline"><em>G</em><sup>′</sup></span> 是 <spanclass="math inline"><em>G</em></span>的导出子图（但凡删除一条边都不再是导出子图）。</p><p>并图：将两张图结合到一起得到的图称为并图，假设 <spanclass="math inline"><em>G</em><sub>1</sub> = (<em>V</em><sub>1</sub>,<em>E</em><sub>1</sub>)</span>，<spanclass="math inline"><em>G</em><sub>2</sub> = (<em>V</em><sub>2</sub>,<em>E</em><sub>2</sub>)</span>，则它们的并图为<spanclass="math inline"><em>G</em> = <em>G</em><sub>1</sub> ∪ <em>G</em><sub>2</sub> = (<em>V</em><sub>1</sub>∪<em>V</em><sub>2</sub>,<em>E</em><sub>1</sub>∪<em>E</em><sub>2</sub>)</span>.</p><h3 id="图的表示和同构">7.3 图的表示和同构</h3><h4 id="邻接表">7.3.1 邻接表</h4><p>这还用讲？</p><h4 id="邻阶矩阵">7.3.2 邻阶矩阵</h4><p>当用邻接矩阵表示伪图时，元素值代表边的数量。</p><h4 id="关联矩阵">7.3.3 关联矩阵</h4><p>关联矩阵的行号代表顶点编号，列号代表边编号。</p><h4 id="同构">7.3.4 同构</h4><p>两个图同构是指当忽略图的顶点编号后，两个图相等。</p><p>如何证明同构：假设 <spanclass="math inline"><em>G</em><sub>1</sub></span> 中的任意顶点为 <spanclass="math inline"><em>a</em></span>，<spanclass="math inline"><em>G</em><sub>2</sub></span> 中的任意顶点为 <spanclass="math inline"><em>b</em></span>，找一个满射函数 <spanclass="math inline"><em>f</em></span> 使得 <spanclass="math inline"><em>f</em>(<em>a</em>) = <em>b</em></span>，若当<span class="math inline"><em>G</em><sub>1</sub></span> 中 <spanclass="math inline"><em>a</em><sub>1</sub>, <em>a</em><sub>2</sub></span>相邻，<span class="math inline"><em>G</em><sub>2</sub></span> 中 <spanclass="math inline"><em>f</em>(<em>a</em><sub>1</sub>)</span> 和 <spanclass="math inline"><em>f</em>(<em>a</em><sub>2</sub>)</span>也相邻时，则说明 <span class="math inline"><em>G</em><sub>1</sub></span>和 <span class="math inline"><em>G</em><sub>2</sub></span> 同构。</p><p>说明两个图同构是一件很难的事情，但是我们可以通过<strong>同构不变量</strong>来简单说明两个图不同构，例如顶点数量、顶点的度、边的数量等。另外，还可以这样判断不同构：<strong>两张图中度相同的结点与连接它们的边所构成的图一定也是同构的</strong>（经常使用两个度相同的顶点之间的路径来是否长度相同来判断图不同构）。</p><p>一种比较简单的方式来说明两个图同构：找一个满射函数 <spanclass="math inline"><em>f</em></span>，使得两个图的邻接矩阵相等。</p><h3 id="连通性">7.4 连通性</h3><p>通路是边的序列。当一个图是简单图时，我们可以直接用顶点的有序序列来表示一条通路（否则不行，因为这样的话一对顶点无法唯一确定一条边）。当通路的首尾顶点相同且长度大于0 时，就构成了回路。一条不含重复边的回路叫做简单回路。</p><h4 id="无向图的连通性">7.4.1 无向图的连通性</h4><p>当图中任意两个顶点间都存在一条通路时，一个无向图是连通的。</p><p>在连通无向图中，每一对顶点之间都存在一条简单通路。</p><p>一个不连通的图实际就是几个连通的子图构成的图。<strong>连通分支</strong>就是指极大连通子图，所谓极大连通子图，首先是连通子图，齐次它不是其他任何连通子图的真子图。</p><p>有时候，删除某个顶点以及与之相连的边，会导致一个图中产生更多的连通子图，这种点称为<strong>割点</strong>。删除某条边导致一个图中产生更多的连通子图，这种边称为<strong>割边</strong>或<strong>桥</strong>。</p><h4 id="有向图的连通性">7.4.2 有向图的连通性</h4><p>如果有向图中，对于任意一对顶点 <spanclass="math inline"><em>a</em></span> 和 <spanclass="math inline"><em>b</em></span>，都有从 <spanclass="math inline"><em>a</em></span> 到 <spanclass="math inline"><em>b</em></span> 和 <spanclass="math inline"><em>b</em></span> 到 <spanclass="math inline"><em>a</em></span> 的通路，那就称该图是强连通的。</p><p>如果把一个有向图改成无向图后（这个无向图称为该有向图的基本无向图），任意两个顶点都连通，那就说这个有向图是弱连通的。</p><p>强连通分支：是有向图的连通分支且是强连通的。</p><h4 id="通路与同构">7.4.3 通路与同构</h4><p>有多种方式来利用通路和回路判断一个图是否同构。比如，特定长度的回路就是一个有用的<strong>同构不变量</strong>。</p><h4 id="计算通路数">7.4.4 计算通路数</h4><p>假设一个图的邻接矩阵为 <spanclass="math inline"><em>A</em></span>，那么 <spanclass="math inline"><em>A</em><sup><em>n</em></sup></span>（是正常的矩阵乘法不是布尔积！）中 <spanclass="math inline">(<em>i</em>,<em>j</em>)</span>上的元素就代表了该图中顶点 <span class="math inline"><em>i</em></span>和顶点 <span class="math inline"><em>j</em></span> 之间长为 <spanclass="math inline"><em>n</em></span> 的路径数量。</p><h3 id="欧拉通路和哈密顿通路">7.5 欧拉通路和哈密顿通路</h3><h4 id="欧拉通路和欧拉回路">7.5.1 欧拉通路和欧拉回路</h4><p>欧拉通路：指一张图中包含了该图全部边的简单通路。</p><p>欧拉回路：指一张图中包含了该图全部边的简单回路。</p><p>一个有 <span class="math inline">2</span>个及以上的连通多重图有欧拉回路的充要条件：每个顶点的度都是偶数。</p><p>构造欧拉回路的算法（<spanclass="math inline"><em>O</em>(|<em>E</em>|)</span>）：</p><ol type="1"><li>从图中的任意一个顶点出发，沿着边行走，形成一条回路，要求每条边只能走一次，但可以重复经过顶点，直到无法继续前进（即所有与当前顶点相连的边都已被走过）。</li><li>若此时回路包含了图中的所有边 ，则该回路就是欧拉回路。</li><li>如果回路中还没有包含所有边，则在回路中找到一个顶点，该顶点存在还未被走过的边。从这个顶点出发，继续沿着未走过的边行走，形成一个新的回路，并将这个新回路插入到原回路中，替换掉原来的那个顶点。重复这个过程，直到所有的边都被包含在回路中。</li></ol><p>一个连通多重图只有欧拉通路没有欧拉回路的充要条件：恰有 2个度为奇数的顶点。</p><h4 id="哈密顿通路和哈密顿回路">7.5.2 哈密顿通路和哈密顿回路</h4><p>哈密顿通路：指一张图中包含了该图全部顶点的简单通路。</p><p>哈密顿回路：指一张图中包含了该图全部顶点的简单回路。</p><p>目前不存在已知的简单的充要条件来判断一个图中是否含有哈密顿通路和哈密顿回路。但是有这么几个性质：</p><ol type="1"><li>存在度为1的顶点的图不存在哈密顿通路</li><li>若一个顶点的度为2，且该图存在哈密顿通路，那么这个顶点的两条边肯定属于哈密顿通路</li><li>一个哈密顿回路不可能含有其他回路</li></ol><p>狄拉克定理：如果<span class="math inline"><em>G</em></span>是有<spanclass="math inline"><em>n</em></span>个顶点的简单图，其中<spanclass="math inline"><em>n</em> ≥ 3</span>，并且<spanclass="math inline"><em>G</em></span>中每个顶点的度都至少是<spanclass="math inline">⌈<em>n</em>/2⌉</span>，则<spanclass="math inline"><em>G</em></span>有哈密顿回路。</p><p>欧尔定理：如果<span class="math inline"><em>G</em></span>是有<spanclass="math inline"><em>n</em></span>个顶点的简单图，其中<spanclass="math inline"><em>n</em> ≥ 3</span>，并且对于<spanclass="math inline"><em>G</em></span>中每一对不相邻的顶点<spanclass="math inline"><em>u</em></span>和<spanclass="math inline"><em>v</em></span>来说，都有<spanclass="math inline">deg (<em>u</em>) + deg (<em>v</em>) ≥ <em>n</em></span>，则<spanclass="math inline"><em>G</em></span>有哈密顿回路。</p><blockquote><p>狄拉克定理可以看作是欧尔定理的推论。不论是狄拉克定理还是欧尔定理，都只是充分条件，并没有给出必要条件。也就是说这里存在一些图，既不满足狄拉克定理，也不满足欧尔定理，但它含有哈密顿回路。</p></blockquote><h3 id="平面图">7.6 平面图</h3><p>如果有办法可以使边无交叉地画出一幅图，那么就称这幅图为<strong>平面图</strong>，把这个画法称为前者的<strong>平面表示</strong>。</p><h4 id="欧拉公式">7.6.1 欧拉公式</h4><p>一个平面图可以把纸面分隔成好几个二维空间（其中有一个是无限平面），这样的二维空间称为一个<strong>面</strong>。</p><p>欧拉公式：设 <span class="math inline"><em>G</em></span> 是带 <spanclass="math inline"><em>e</em></span> 条边和 <spanclass="math inline"><em>v</em></span> 个顶点的连通平面简单图，设 <spanclass="math inline"><em>r</em></span> 是 <spanclass="math inline"><em>G</em></span> 的平面图表示中的面数，则 <spanclass="math inline"><em>r</em> = <em>e</em> − <em>v</em> + 2</span>.</p><blockquote><p>注：如果不考虑外部面，则 <spanclass="math inline"><em>r</em> = <em>e</em> − <em>v</em> + 1</span>。</p></blockquote><p>推论 1：若 <span class="math inline"><em>G</em></span> 是 <spanclass="math inline"><em>e</em></span> 条边和 <spanclass="math inline"><em>v</em></span> 个顶点的连通平面简单图，其中 <spanclass="math inline"><em>v</em> ≥ 3</span>，则 <spanclass="math inline"><em>e</em> ≤ 2<em>v</em> − 6</span>。</p><p>推论 2：若 <span class="math inline"><em>G</em></span>是连通平面简单图，则 <span class="math inline"><em>G</em></span>中有度数不超过 <span class="math inline">5</span> 的顶点。</p><p>推论 3：若 <span class="math inline"><em>G</em></span> 是 <spanclass="math inline"><em>e</em></span> 条边和 <spanclass="math inline"><em>v</em></span> 个顶点的连通平面简单图，且 <spanclass="math inline"><em>v</em> ≥ 3</span> 且没有长度为 3 的回路，则<span class="math inline"><em>e</em> ≤ 2<em>v</em> − 4</span>。</p><h4 id="库拉图斯基定理">7.6.2 库拉图斯基定理</h4><p>若<spanclass="math inline"><em>G</em></span>是一个平面图，在它的某条边<spanclass="math inline">{<em>a</em>, <em>c</em>}</span>上新建一个中间结点<spanclass="math inline"><em>b</em></span>，使得原本的<spanclass="math inline">{<em>a</em>, <em>c</em>}</span>被新增的两条边<spanclass="math inline">{<em>a</em>, <em>b</em>}</span>和<spanclass="math inline">{<em>b</em>, <em>c</em>}</span>替换掉，这样形成的新图仍然是平面图，这种操作称为<strong>初等细分</strong>。若两幅图可以通过对同一幅图进行初等细分来得到，则称这两张图是<strong>同胚</strong>的。</p><p>库拉图斯基定理：当且仅当图<spanclass="math inline"><em>G</em></span>包含了同胚于<spanclass="math inline"><em>K</em><sub>3, 3</sub></span>或<spanclass="math inline"><em>K</em><sub>5</sub></span>的子图，<spanclass="math inline"><em>G</em></span>是非平面图。</p><p>一般来说，直接应用库拉图斯基定理来判断非平面图是比较困难的，我们可以考虑对图进行逆向初等细分，即一步一步去掉中间结点，看是否能把图还原成<spanclass="math inline"><em>K</em><sub>3, 3</sub></span>或<spanclass="math inline"><em>K</em><sub>5</sub></span>。</p><h3 id="图着色">7.7 图着色</h3><p>对偶图：将一张地图的每块区域看作一个顶点，如果区域与区域之间有公共边界，那就认为它们对应的顶点是相邻的（如果仅仅是相交于一个点则不算），这样画出来的图叫这张地图的<strong>对偶图</strong>。</p><p>图着色是指为图中的每个顶点着色，要求没有一对相邻顶点的颜色是相同的。</p><p><strong>着色数</strong>是指使图<spanclass="math inline"><em>G</em></span>中结点颜色各不相同所需要的最少颜色数，记作<spanclass="math inline"><em>χ</em>(<em>G</em>)</span>。</p><p><strong>四色定理</strong>：平面图的着色数不超过4.（仅适用于平面图）</p><p>几个结论：</p><ol type="1"><li><spanclass="math inline"><em>χ</em>(<em>K</em><sub><em>n</em></sub>) = <em>n</em></span></li><li><spanclass="math inline"><em>χ</em>(<em>K</em><sub><em>n</em>, <em>m</em></sub>) = 2</span></li><li>当<span class="math inline"><em>n</em></span>为偶数时，<spanclass="math inline"><em>χ</em>(<em>C</em><sub><em>n</em></sub>) = 2</span>，当<spanclass="math inline"><em>n</em></span>为奇数时，<spanclass="math inline"><em>χ</em>(<em>C</em><sub><em>n</em></sub>) = 3</span></li></ol>]]></content>
      
      
      <categories>
          
          <category> discrete math </category>
          
      </categories>
      
      
        <tags>
            
            <tag> lecture notes </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>图注意力机制的原理</title>
      <link href="/2024/04/03/%E5%9B%BE%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/"/>
      <url>/2024/04/03/%E5%9B%BE%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/</url>
      
        <content type="html"><![CDATA[<p>该篇笔记总结了图注意力网络（Graph AttantionNetwork）中的图注意力层的数学原理，参考的资料为 GAT 网络的<ahref="https://arxiv.org/abs/1710.10903">原始论文</a>.</p><p>假设输入是一组节点特征，表示为 <spanclass="math inline">$h=\{\vec{h_1},\vec{h_2},\cdots,\vec{h_N}\},\vec{h_i}\in\mathbb{R}^F$</span>，其中<span class="math inline"><em>N</em></span> 表示节点的个数，<spanclass="math inline"><em>F</em></span> 表示特征维数。GAL将会输出一组新的节点特征，并且这组特征的特征维数并不一定与原特征相同，设<span class="math inline"><em>F</em><sup>′</sup></span> 为 GAL输出的特征维数，则新的节点特征可表示为 <spanclass="math inline">$h^{\prime}=\{\vec{h_1^{\prime}},\vec{h_2^{\prime}},\cdots,\vec{h_N^{\prime}}\},\vec{h_i^{\prime}}\in\mathbb{R}^{F^{\prime}}$</span>。</p><span id="more"></span><p>为了把输入特征转换为高维特征，并且使高维特征具有足够高的表达原特征的能力，我们需要执行至少一个可学习的线性变换。考虑到这一点，我们将对每一个节点进行一次线性变换，即将每个节点的特征乘上一个<strong>共享</strong>的权重矩阵<spanclass="math inline"><em>W</em> ∈ ℝ<sup><em>F</em><sup>′</sup> × <em>F</em></sup></span>。然后我们会对每一个节点进行一次自注意力(<em>self-attention</em>) 操作，我们用 <spanclass="math inline"><em>a</em></span>表示这一操作，该操作将会计算得出节点 <spanclass="math inline"><em>i</em></span> 和节点 <spanclass="math inline"><em>j</em></span> 之间的注意力系数 (<em>attentioncoefficients</em>)： <span class="math display">$$e_{ij}=a(W\vec{h_i},W\vec{h_j})$$</span> 注意力系数 <spanclass="math inline"><em>e</em><sub><em>i</em><em>j</em></sub></span>表示了节点 <span class="math inline"><em>j</em></span> 的特征对于节点<span class="math inline"><em>i</em></span>的重要性。在自注意力机制最广泛应用的公式中，模型将会计算每个节点之间的注意力系数，导致结构信息被丢弃。我们通过执行掩码注意力机制(<em>masked attention</em>)来保留图的结构信息——我们只计算相邻节点之间的注意力系数。注意，这里的相邻节点指的是直接相邻(first-order)的节点，而不是连通的节点，并且一个节点本身也是其自己的相邻节点。为了能对注意力系数进行跨节点的比较，我们对它们进行归一化，用以衡量不同节点<span class="math inline"><em>j</em> ∈ 𝒩<sub><em>i</em></sub></span>对节点 <span class="math inline"><em>i</em></span> 的重要程度 (这里的<span class="math inline">𝒩<sub><em>i</em></sub></span> 表示节点 <spanclass="math inline"><em>i</em></span> 的邻居节点的编号)，归一化的过程由softmax 函数实现： <span class="math display">$$\alpha_{ij}=\operatorname{softmax}(e_{ij})=\frac{\operatorname{exp}(e_{ij})}{\sum_{k\in\mathcal{N}_i}\operatorname{exp}(e_{ik})}.$$</span> 在我们的实验中，注意力机制 <spanclass="math inline"><em>a</em></span>是一个单层的前馈神经网络，以一个权重向量 <spanclass="math inline">$\vec{\mathbf{a}}\in\mathbb{R}^{2F^\prime}$</span>​作为参数，使用 LeakyReLU 函数实现非线性变换 (negative slope 设置为0.2)。展开后，系数的计算可表示为： <span class="math display">$$\alpha_{ij}=\frac{\operatorname{exp}(\operatorname{LeakyReLU}(\vec{\mathbf{a}}^{T}[\mathbf{W}\vec{h_i}||\mathbf{W}\vec{h_j}]))}{\sum_{k\in\mathcal{N}_i}\operatorname{exp}(\operatorname{LeakyReLU}(\vec{\mathbf{a}}^{T}[\mathbf{W}\vec{h_i}||\mathbf{W}\vec{h_k}]))}$$</span> 其中，<span class="math inline">·<sup><em>T</em></sup></span>表示转置操作，<span class="math inline">||</span> 是矩阵拼接操作。</p><p>一旦得到注意力系数，GAL就会使用它们来计算出各个特征对应的线性组合，作为节点最终的输出特征，整个过程如下左图所示(右图是多头注意力机制)：</p><p><img src="/img/GAT-01.png" /></p><p>输出特征 <spanclass="math inline"><em>h⃗</em><sub><em>i</em></sub><sup>′</sup></span>的计算公式如下： <spanclass="math display"><em>h⃗</em><sub><em>i</em></sub><sup>′</sup> = <em>σ</em>(∑<sub><em>j</em> ∈ 𝒩<sub><em>i</em></sub></sub><em>α</em><sub><em>i</em><em>j</em></sub><strong>W</strong><em>h⃗</em><sub><em>j</em></sub>)</span>其中，<span class="math inline"><em>σ</em></span>代表一种非线性变换。</p><p>为了使整个自注意力过程更加稳定，我们引入了多头注意力 (<em>multi-headattention</em>)。具体来说，<span class="math inline"><em>K</em></span>个独立的注意力机制将分别执行上式的变换，随后，它们的结果将会被拼接起来，得到下面的输出特征表示：<span class="math display">$$\vec{h}_i^\prime=\overset{K}{\underset{k=1}{\|}}\sigma(\sum_{j\in\mathcal{N_i}}\alpha_{ij}^k\mathbf{W}^k\vec{h}_j)$$</span>特殊的，如果我们在输出层执行多头注意力，拼接过程将不再是显式的，我们会使用平均聚合(<em>averaging</em>)，然后施加一个非线性变换来得到最终输出： <spanclass="math display">$$\vec{h}_i^\prime=\sigma(\frac{1}{K}\sum_{k=1}^{K}\sum_{j\in\mathcal{N_i}}\alpha_{ij}^k\mathbf{W}^k\vec{h}_j)$$</span></p>]]></content>
      
      
      <categories>
          
          <category> deep learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> technique </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>隐马尔可夫模型</title>
      <link href="/2023/12/18/%E9%9A%90%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E6%A8%A1%E5%9E%8B/"/>
      <url>/2023/12/18/%E9%9A%90%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E6%A8%A1%E5%9E%8B/</url>
      
        <content type="html"><![CDATA[<p>隐马尔可夫模型 HMM（Hidden MarkovModel）是一种统计模型，用来描述一个隐含未知量的马尔可夫过程（马尔可夫过程是一类随机过程，它的原始模型是马尔科夫链），它是结构最简单的动态贝叶斯网，是一种著名的有向图模型，主要用于时序数据建模，在语音识别、自然语言处理等领域有广泛应用，是一种<strong>生成式模型</strong>。</p><span id="more"></span><h2 id="马尔可夫模型">01 马尔可夫模型</h2><p>在学习隐马尔可夫模型之前，我们先来了解一下它的前生——马尔可夫模型MM（Markov Model）。</p><p>我们用一个例子进行引入：天气的变化应该具有某种联系。晴天、多云和暴雨这三种天气之间的转换应该存在某种规律，下图中的箭头表示两种天气之间转换的概率：</p><p><img src="/img/隐马尔可夫模型-01.png" /></p><p>于是我们能得到一个<strong>状态转移概率矩阵</strong>：</p><p><img src="/img/隐马尔可夫模型-02.png" /></p><p>根据该矩阵，我们就能在知道今天天气的情况下，预测明天的天气。显然，这种预测是建立在==未来所处的状态仅与当前状态有关==的假设上的，即第二天的天气只取决于前一天的天气。这种假设就是<strong>马尔可夫假设</strong>，符合这种假设描述的随机过程，就被称为<strong>马尔可夫过程</strong>，其具有<strong>马尔可夫性</strong>，即<strong>无后效性</strong>。</p><p>令 <span class="math inline"><em>q</em><sub><em>t</em></sub></span>表示在时刻 <span class="math inline"><em>t</em></span>系统所处的状态，令 <spanclass="math inline"><em>S</em><sub><em>i</em></sub></span>表示某一具体状态，则 <spanclass="math inline"><em>q</em><sub><em>t</em></sub> = <em>S</em><sub><em>i</em></sub></span>表示在某一时刻 <span class="math inline"><em>t</em></span>，系统处于状态<span class="math inline"><em>S</em><sub><em>i</em></sub></span>，令<spanclass="math inline"><em>P</em>(<em>q</em><sub><em>t</em> + 1</sub>=<em>S</em><sub><em>i</em></sub>|<em>q</em><sub><em>t</em></sub>=<em>S</em><sub><em>j</em></sub>)</span>表示在前一时刻 <span class="math inline"><em>t</em></span> 系统处于状态<span class="math inline"><em>S</em><sub><em>j</em></sub></span>的情况下，下一时刻 <span class="math inline"><em>t</em> + 1</span>系统处于状态 <spanclass="math inline"><em>S</em><sub><em>i</em></sub></span>的概率。基于<strong>马尔可夫假设</strong>，则在马尔可夫模型中存在下列关系：<spanclass="math display"><em>P</em>(<em>q</em><sub><em>t</em> + 1</sub>=<em>S</em><sub><em>i</em></sub>|<em>q</em><sub><em>t</em></sub>=<em>S</em><sub><em>j</em></sub>,<em>q</em><sub><em>t</em> − 1</sub>=<em>S</em><sub><em>k</em></sub>,...) = <em>P</em>(<em>q</em><sub><em>t</em> + 1</sub>=<em>S</em><sub><em>i</em></sub>|<em>q</em><sub><em>t</em></sub>=<em>S</em><sub><em>j</em></sub>)</span>除了状态转移概率矩阵（用 <span class="math inline"><em>A</em></span>表示）之外，我们还需要知道所有状态的<strong>初始状态概率向量</strong><span class="math inline"><em>Π</em></span>，设系统中一共有 <spanclass="math inline"><em>N</em></span> 种状态，则 <spanclass="math inline"><em>Π</em></span> 的长度为 <spanclass="math inline"><em>N</em></span>，<spanclass="math inline"><em>Π</em></span>中的每一个元素代表系统的初始状态为某一状态的概率，且有 <spanclass="math inline">$\sum_{i=1}^N\Pi_i=1$</span>。</p><p>假设我们想计算一下今天 <spanclass="math inline"><em>t</em> = 1</span> 的天气状况，则我们可以得到：<span class="math display">$$P(q_1=S_{\text{sun}})=P(q_1=S_{\text{sun}}|q_0=\sum_{i=1}^{3}S_i)=\sum_{i=1}^{3}\Pi_{S_i}\timesA_{S_i\rightarrow S_{\text{sun}}}$$</span> 用文字形式表示就是：</p><p><img src="/img/隐马尔可夫模型-03.png" /></p><h2 id="隐马尔可夫模型">02 隐马尔可夫模型</h2><h3 id="概念">2.1 概念</h3><p>而隐马尔可夫模型就比马尔可夫模型要复杂多了。我们还是用上面这个例子进行引入，但是这次我们漂流到了一个岛上，这里没有天气预报，只有一片海藻，海藻具有干燥、较干、较湿和湿润四种状态。现在我们没有直接的天气信息了，但是天气状况跟海藻的状态还是有一定联系的，虽然看不见天气状况，但其决定了海藻的状态，所以我们还是能从海藻的状态推知天气的状态。</p><p>在这个例子里，海藻是能看到的，那它就是<strong>观测状态</strong>；天气信息是看不到的，那它就是<strong>隐藏状态</strong>。其中，隐藏状态天气时是决定性因素，观测状态是被决定因素，由隐藏状态到观测状态，这就是<strong>隐马尔可夫模型</strong>。</p><p><img src="/img/隐马尔可夫模型-04.png" /></p><p>如上图所示，观测状态（海藻的状态）有 4 个，而隐藏状态（天气）只有 3个，说明观测状态与隐藏状态的数量并不是一一对应的，可以根据需要定义。我们可以画出更加抽象的隐马尔可夫模型的示意图：</p><p><img src="/img/隐马尔可夫模型-05.png" /></p><p>图中，<spanclass="math inline"><em>Z</em><sub><em>i</em></sub></span>表示隐藏状态，<spanclass="math inline"><em>X</em><sub><em>i</em></sub></span>表示观测状态，隐藏状态决定了观测状态，所以箭头由 <spanclass="math inline"><em>Z</em></span> 指向 <spanclass="math inline"><em>X</em></span>。并且，隐藏状态之间还可以相互转换，所以<span class="math inline"><em>Z</em><sub><em>i</em></sub></span> 和<span class="math inline"><em>Z</em><sub><em>j</em></sub></span>之间也有箭头。根据马尔可夫假设，下一时刻的状态只取决于当前时刻的状态，所以，对于观测状态和隐藏状态来讲，都存在如下关系：<span class="math display">$$\begin{aligned}&amp;P=(Z_t|Z_{t-1},X_{t-1},Z_{t-2},X_{t-2},...,Z_1,X_1)=P(Z_t|Z_{t-1})\\&amp;P=(X_t|Z_{t},X_{t},Z_{t-1},X_{t-1},...,Z_1,X_1)=P(X_t|Z_t)\\\end{aligned}$$</span></p><h3 id="组成">2.2 组成</h3><p>马尔可夫模型有两个组成部分——初始状态概率向量 <spanclass="math inline"><em>Π</em></span> 和 状态转移概率矩阵 <spanclass="math inline"><em>A</em></span>。</p><p>而隐马尔可夫模型有则有三个组成部分：</p><ol type="1"><li>初始状态概率向量 <span class="math inline"><em>Π</em></span></li><li>状态转移概率矩阵 <span class="math inline"><em>A</em></span></li><li>观测状态概率矩阵 <span class="math inline"><em>B</em></span></li></ol><p>其中，<span class="math inline"><em>Π</em></span>是针对隐藏状态来说的，因为隐藏状态决定了观测状态；<spanclass="math inline"><em>A</em></span>矩阵是针对隐藏状态来说的，因为隐马尔可夫模型中进行状态转移的是隐藏状态；而<span class="math inline"><em>B</em></span>是由隐藏状态到观测状态转移的概率矩阵，在上例中，矩阵 <spanclass="math inline"><em>B</em></span> 可表示如下：</p><p><img src="/img/隐马尔可夫模型-06.png" /></p><p>也就是说，由 <spanclass="math inline"><em>Z</em><sub><em>i</em></sub> → <em>Z</em><sub><em>j</em></sub></span>的转换看矩阵 <span class="math inline"><em>A</em></span>，由 <spanclass="math inline"><em>Z</em><sub><em>i</em></sub> → <em>X</em><sub><em>i</em></sub></span>的转换看矩阵 <span class="math inline"><em>B</em></span>。</p><p>因此，隐马尔可夫模型 <span class="math inline"><em>λ</em></span>可以用三元符号表示： <spanclass="math display"><em>λ</em>(<em>A</em>,<em>B</em>,<em>Π</em>)</span></p><h3 id="求解目标">2.3 求解目标</h3><p>HMM 的求解目标有三个：</p><ol type="1"><li>给定模型 <spanclass="math inline"><em>λ</em>(<em>A</em>,<em>B</em>,<em>Π</em>)</span>及观测序列 <spanclass="math inline"><em>O</em> = {<em>o</em><sub>1</sub>, <em>o</em><sub>2</sub>, ..., <em>o</em><sub><em>t</em></sub>}</span>，计算该观测序列出现的概率<spanclass="math inline"><em>P</em>(<em>O</em>|<em>λ</em>)</span>；</li><li>给定观测序列 <spanclass="math inline"><em>O</em> = {<em>o</em><sub>1</sub>, <em>o</em><sub>2</sub>, ..., <em>o</em><sub><em>t</em></sub>}</span>，求解参数<span class="math inline">(<em>A</em>,<em>B</em>,<em>Π</em>)</span> 使得<span class="math inline"><em>P</em>(<em>O</em>|<em>λ</em>)</span>最大；</li><li>已知模型 <spanclass="math inline"><em>λ</em>(<em>A</em>,<em>B</em>,<em>Π</em>)</span>和观测序列 <spanclass="math inline"><em>O</em> = {<em>o</em><sub>1</sub>, <em>o</em><sub>2</sub>, ..., <em>o</em><sub><em>t</em></sub>}</span>，求状态序列，使得<spanclass="math inline"><em>P</em>(<em>I</em>|<em>O</em>,<em>λ</em>)</span>最大。</li></ol><h2 id="暴力求解法">03 暴力求解法</h2><p>我们要求的是在给定模型下观测序列出现的概率，那如果我们能把所有的隐藏序列都列出来，也就可以知道联合概率分布<spanclass="math inline"><em>P</em>(<em>O</em>,<em>I</em>|<em>λ</em>)</span>了（其中，<span class="math inline"><em>I</em></span> 为 <spanclass="math inline"><em>O</em></span> 对应的隐藏状态序列），再根据 <spanclass="math inline"><em>P</em>(<em>O</em>|<em>λ</em>) = ∑<sub><em>I</em></sub><em>P</em>(<em>O</em>,<em>I</em>|<em>λ</em>)</span>，我们就能求得观测序列出现的概率。</p><p>根据联合概率分布的公式：<spanclass="math inline"><em>P</em>(<em>X</em>=<em>x</em>,<em>Y</em>=<em>y</em>) = <em>P</em>(<em>X</em>=<em>x</em>)<em>P</em>(<em>Y</em>=<em>y</em>|<em>X</em>=<em>x</em>)</span>，可得<spanclass="math inline"><em>P</em>(<em>O</em>,<em>I</em>|<em>λ</em>)</span>的求解方法： <spanclass="math display"><em>P</em>(<em>O</em>,<em>I</em>|<em>λ</em>) = <em>P</em>(<em>I</em>|<em>λ</em>)<em>P</em>(<em>O</em>|<em>I</em>,<em>λ</em>)</span>其中，<span class="math inline"><em>P</em>(<em>I</em>|<em>λ</em>)</span>是在给定模型下，一个隐藏序列出现的概率，即 <spanclass="math inline"><em>P</em>(<em>I</em>|<em>λ</em>) = <em>P</em>(<em>i</em><sub>1</sub>,<em>i</em><sub>2</sub>,...,<em>i</em><sub><em>n</em></sub>|<em>λ</em>) = <em>P</em>(<em>i</em><sub>1</sub>|<em>λ</em>)<em>P</em>(<em>i</em><sub>2</sub>|<em>λ</em>)...<em>P</em>(<em>i</em><sub><em>n</em></sub>|<em>λ</em>)</span>。那么怎么求<spanclass="math inline"><em>P</em>(<em>i</em><sub><em>n</em></sub>|<em>λ</em>)</span>？别忘了状态转移概率矩阵<span class="math inline"><em>A</em></span> 的存在，<spanclass="math inline"><em>A</em></span>所记录的不就是隐藏状态之间转换的概率吗？所以可得： <spanclass="math display"><em>P</em>(<em>I</em>|<em>λ</em>) = <em>π</em><sub><em>i</em><sub>1</sub></sub><em>a</em><sub><em>i</em><sub>1</sub><em>i</em><sub>2</sub></sub><em>a</em><sub><em>i</em><sub>2</sub><em>i</em><sub>3</sub></sub>...<em>a</em><sub><em>i</em><sub><em>t</em> − 1</sub><em>i</em><sub><em>t</em></sub></sub></span>接下来要求的就是 <spanclass="math inline"><em>P</em>(<em>O</em>|<em>I</em>,<em>λ</em>)</span>，它的含义是：在给定模型下，当隐藏序列为<span class="math inline"><em>I</em></span> 时，观测序列为 <spanclass="math inline"><em>O</em></span> 的概率。求解 <spanclass="math inline"><em>P</em>(<em>O</em>|<em>I</em>,<em>λ</em>)</span>的方法和求解 <spanclass="math inline"><em>P</em>(<em>I</em>|<em>λ</em>)</span>的方法是一样的，还记得观测状态概率矩阵 <spanclass="math inline"><em>B</em></span> 吗？<spanclass="math inline"><em>B</em></span>记录的正是从隐藏序列到观测序列转换的概率，所以 <spanclass="math inline"><em>P</em>(<em>O</em>|<em>I</em>,<em>λ</em>)</span>的计算方法如下： <spanclass="math display"><em>P</em>(<em>O</em>|<em>I</em>,<em>λ</em>) = <em>b</em><sub><em>i</em><sub>1</sub><em>o</em><sub>1</sub></sub><em>b</em><sub><em>i</em><sub>2</sub><em>o</em><sub>2</sub></sub>...<em>b</em><sub><em>i</em><sub><em>t</em></sub><em>o</em><sub><em>t</em></sub></sub></span>于是，我们只需要将上面两个式子乘在一起，就能得到 <spanclass="math inline"><em>P</em>(<em>O</em>,<em>I</em>|<em>λ</em>)</span>了： <spanclass="math display"><em>P</em>(<em>O</em>,<em>I</em>|<em>λ</em>) = <em>π</em><sub><em>i</em><sub>1</sub></sub><em>a</em><sub><em>i</em><sub>1</sub><em>i</em><sub>2</sub></sub><em>b</em><sub><em>i</em><sub>1</sub><em>o</em><sub>1</sub></sub><em>a</em><sub><em>i</em><sub>2</sub><em>i</em><sub>3</sub></sub><em>b</em><sub><em>i</em><sub>2</sub><em>o</em><sub>2</sub></sub>...<em>a</em><sub><em>i</em><sub><em>t</em> − 1</sub><em>i</em><sub><em>t</em></sub></sub><em>b</em><sub><em>i</em><sub><em>t</em></sub><em>o</em><sub><em>t</em></sub></sub></span>则观测序列 <span class="math inline"><em>O</em></span> 出现的概率为：<spanclass="math display"><em>P</em>(<em>O</em>|<em>λ</em>) = ∑<sub><em>I</em></sub><em>P</em>(<em>O</em>,<em>I</em>|<em>λ</em>) = ∑<sub><em>i</em><sub>1</sub>, <em>i</em><sub>2</sub>, ..., <em>i</em><sub><em>T</em></sub></sub><em>π</em><sub><em>i</em><sub>1</sub></sub><em>a</em><sub><em>i</em><sub>1</sub><em>i</em><sub>2</sub></sub><em>b</em><sub><em>i</em><sub>1</sub><em>o</em><sub>1</sub></sub><em>a</em><sub><em>i</em><sub>2</sub><em>i</em><sub>3</sub></sub><em>b</em><sub><em>i</em><sub>2</sub><em>o</em><sub>2</sub></sub>...<em>a</em><sub><em>i</em><sub><em>t</em> − 1</sub><em>i</em><sub><em>T</em></sub></sub><em>b</em><sub><em>i</em><sub><em>t</em></sub><em>o</em><sub><em>T</em></sub></sub></span>解释一下上面的公式：我们要求的是在给定模型下，某一观测序列出现的概率。暴力求解的方法找出所有可能的隐藏序列，将由这些隐藏序列得到该观测序列的概率全部加起来，最终得到该观测序列的概率。假设隐藏状态数有<span class="math inline"><em>N</em></span>个，我们需要遍历每一个隐藏序列，序列的长度为观测状态数 <spanclass="math inline"><em>T</em></span>，所以可能的隐藏序列有 <spanclass="math inline"><em>N</em><sup><em>T</em></sup></span>种，而对于每一个序列，都要遍历其 <spanclass="math inline"><em>T</em></span> 个 <spanclass="math inline"><em>a</em><sub><em>i</em></sub></span> 和 <spanclass="math inline"><em>b</em><sub><em>i</em></sub></span>，加起来就是<spanclass="math inline">2<em>T</em></span>，计算时间复杂度时省去系数，则该算法的<strong>时间复杂度将达到<spanclass="math inline"><em>O</em>(<em>T</em><em>N</em><sup><em>T</em></sup>)</span></strong>。</p><h2 id="前向算法">04 前向算法</h2><h3 id="算法解析">4.1 算法解析</h3><p>暴力求解法告诉我们隐马尔可夫模型的问题看上去是可解，但高昂的时间开销却是不可承受的。对此，有人提出了前向算法，该算法利用<strong>动态规划</strong>的思想来求解该问题，降低了时间复杂度。</p><p>给定 <span class="math inline"><em>t</em></span> 时刻的隐藏状态为<spanclass="math inline"><em>q</em><sub><em>i</em></sub></span>（注意，这里的<span class="math inline"><em>i</em></span>是指一种<u>具体</u>的隐藏状态，例如晴天、雨天等，是固定好的），观测序列为<spanclass="math inline"><em>o</em><sub>1</sub>, <em>o</em><sub>2</sub>, ...<em>o</em><sub><em>n</em></sub></span>的概率叫做<strong>前向概率</strong>，定义为： <spanclass="math display"><em>α</em><sub><em>t</em></sub>(<em>i</em>) = <em>P</em>(<em>o</em><sub>1</sub>,<em>o</em><sub>2</sub>,...,<em>o</em><sub><em>t</em></sub>,<em>S</em><sub><em>t</em></sub>=<em>q</em><sub><em>i</em></sub>|<em>λ</em>)</span>换句话来讲，前向概率就是在给定某一观测序列的情况下，某一时刻的状态刚刚好为<span class="math inline"><em>q</em><sub><em>i</em></sub></span>的概率。</p><p>则，当 <span class="math inline"><em>t</em> = <em>T</em></span>时，<spanclass="math inline"><em>α</em><sub><em>T</em></sub>(<em>i</em>) = <em>P</em>(<em>o</em><sub>1</sub>,<em>o</em><sub>2</sub>,...,<em>o</em><sub><em>T</em></sub>,<em>S</em><sub><em>T</em></sub>=<em>q</em><sub><em>i</em></sub>|<em>λ</em>)</span>表示最后一个时刻，隐藏状态为状态 <spanclass="math inline"><em>q</em><sub><em>i</em></sub></span>并且得到观察序列为 <spanclass="math inline"><em>o</em><sub>1</sub>, <em>o</em><sub>2</sub>, ..., <em>o</em><sub><em>T</em></sub></span>的概率。现在我们回来思考一下我们要解决的最原始的问题是什么，应该是 <spanclass="math inline"><em>P</em>(<em>O</em>|<em>λ</em>) = <em>P</em>(<em>o</em><sub>1</sub>,<em>o</em><sub>2</sub>,...<em>o</em><sub><em>T</em></sub>|<em>λ</em>)</span>，而<spanclass="math inline"><em>α</em><sub><em>T</em></sub>(<em>i</em>)</span>和它相比，末尾多了个 <spanclass="math inline"><em>S</em><sub><em>T</em></sub> = <em>q</em><sub><em>i</em></sub></span>，也就是说<spanclass="math inline"><em>α</em><sub><em>T</em></sub>(<em>i</em>)</span>还要求最终的隐藏状态必须为 <spanclass="math inline"><em>q</em><sub><em>i</em></sub></span>，貌似和原本的问题相比有点画蛇添足，但仔细想想，如果我们能把所有最终可能的隐藏状态都拿过来，求<spanclass="math inline"><em>α</em><sub><em>T</em></sub>(1) + <em>α</em><sub><em>T</em></sub>(2) +  ·  ·  ·  + <em>α</em><sub><em>T</em></sub>(<em>n</em>)</span>，那不就大功告成了？所以现在的问题就变成了如何求解<span class="math inline"><em>T</em></span>时刻的前向概率，这是一个动态规划的问题。</p><p>在第一个时刻：<spanclass="math inline"><em>α</em><sub>1</sub>(<em>i</em>) = <em>P</em>(<em>o</em><sub>1</sub>,<em>S</em><sub>1</sub>=<em>q</em><sub><em>i</em></sub>|<em>λ</em>)</span>表示第一时刻的隐藏状态为 <spanclass="math inline"><em>q</em><sub><em>i</em></sub></span>，观测序列为<span class="math inline"><em>o</em><sub>1</sub></span>的概率，其结果为（这里的 <spanclass="math inline"><em>b</em><sub><em>i</em></sub>(<em>o</em><sub>1</sub>)</span>就表示由隐藏状态 <spanclass="math inline"><em>q</em><sub><em>i</em></sub></span>转换为观测状态 <span class="math inline"><em>o</em><sub>1</sub></span>的概率，是矩阵 <span class="math inline"><em>B</em></span> 的元素）：<spanclass="math display"><em>α</em><sub>1</sub>(<em>i</em>) = <em>π</em><sub><em>i</em></sub><em>b</em><sub><em>i</em></sub>(<em>o</em><sub>1</sub>)</span>在第 <span class="math inline"><em>t</em></span> 时刻，隐藏状态变成了<span class="math inline"><em>q</em><sub><em>j</em></sub></span>（这里的<span class="math inline"><em>q</em><sub><em>j</em></sub></span>不是具体状态，是任意状态都可以），<spanclass="math inline"><em>t</em> + 1</span> 时刻隐藏状态变成了为 <spanclass="math inline"><em>q</em><sub><em>i</em></sub></span>，此时，隐藏状态由<span class="math inline"><em>q</em><sub><em>j</em></sub></span> 变成<span class="math inline"><em>q</em><sub><em>i</em></sub></span>的概率可由矩阵 <span class="math inline"><em>A</em></span> 得到，值为<spanclass="math inline"><em>a</em><sub><em>j</em><em>i</em></sub></span>，而<span class="math inline"><em>q</em><sub><em>j</em></sub></span>可以是任何一种状态，我们都得考虑进去，所以我们得遍历一遍所有隐藏状态，然后相加，即<spanclass="math inline">∑<sub><em>j</em></sub><em>α</em><sub><em>t</em></sub>(<em>j</em>)</span>，所以有：<spanclass="math display"><em>α</em><sub><em>t</em> + 1</sub>(<em>i</em>) = [∑<sub><em>j</em></sub><em>α</em><sub><em>t</em></sub>(<em>j</em>)<em>a</em><sub><em>i</em><em>j</em></sub>]<em>b</em><sub><em>i</em></sub>(<em>o</em><sub><em>t</em> + 1</sub>)</span>如果这个式子看上去还是太麻烦，我们可以拆开来看：<spanclass="math inline"><em>α</em><sub><em>t</em></sub>(<em>j</em>)</span>表示的是前一时刻隐藏状态为 <spanclass="math inline"><em>q</em><sub><em>j</em></sub></span> 的概率，<spanclass="math inline"><em>a</em><sub><em>i</em><em>j</em></sub></span>表示由隐藏状态 <spanclass="math inline"><em>q</em><sub><em>j</em></sub></span> 转换为 <spanclass="math inline"><em>q</em><sub><em>i</em></sub></span>的概率，相乘就是前一时刻的隐藏状态恰好为 <spanclass="math inline"><em>q</em><sub><em>j</em></sub></span>，并且由 <spanclass="math inline"><em>q</em><sub><em>j</em></sub></span> 能转换到<span class="math inline"><em>q</em><sub><em>i</em></sub></span>的概率，由于得考虑全部 <spanclass="math inline"><em>q</em><sub><em>j</em></sub></span>的情况，所以得遍历求和；后面的 <spanclass="math inline"><em>b</em><sub><em>i</em></sub>(<em>o</em><sub><em>t</em> + 1</sub>)</span>则是由隐藏状态 <spanclass="math inline"><em>q</em><sub><em>i</em></sub></span>转换到观测状态 <spanclass="math inline"><em>o</em><sub><em>t</em> + 1</sub></span>的概率，将它与前一部分相乘，就得到了前一时刻的隐藏状态恰好为 <spanclass="math inline"><em>q</em><sub><em>j</em></sub></span>，并且由 <spanclass="math inline"><em>q</em><sub><em>j</em></sub></span> 能转换到<span class="math inline"><em>q</em><sub><em>i</em></sub></span>，又由<span class="math inline"><em>q</em><sub><em>i</em></sub></span> 得到<span class="math inline"><em>o</em><sub><em>t</em> + 1</sub></span>的概率。</p><p>则最终结果就是： <spanclass="math display"><em>P</em>(<em>O</em>|<em>λ</em>) = ∑<sub><em>i</em></sub><em>α</em><sub><em>i</em></sub>(<em>T</em>)</span>计算一下前向算法的时间复杂度：一共要计算 <spanclass="math inline"><em>T</em></span> 次 <spanclass="math inline"><em>α</em></span>，每次计算 <spanclass="math inline"><em>α</em></span> 的时间复杂度为 <spanclass="math inline"><em>N</em><sup>2</sup></span>（原因很简单，自己想），所以前向算法的<strong>时间复杂度为 <spanclass="math inline"><em>O</em>(<em>T</em><em>N</em><sup>2</sup>)</span></strong>。显然，前向算法将暴力算法的时间复杂度从指数级降到了线性级别，极大提升了执行效率。</p><h3 id="公式推导">4.2 公式推导</h3><p>由上述过程，我们可以得到前向算法的递推式：</p><ol type="1"><li>初值：</li></ol><p><spanclass="math display"><em>α</em><sub>1</sub>(<em>i</em>) = <em>π</em><sub><em>i</em></sub><em>b</em><sub><em>i</em></sub>(<em>o</em><sub>1</sub>)，<em>i</em> = 1, 2, ..., <em>N</em></span></p><ol start="2" type="1"><li>递推：</li></ol><p><span class="math display">$$\alpha_{t+1}(i)=[\sum_{j=1}^N\alpha_t(j)a_{ji}]b_i(o_{t+1})，i=1,2,...,N$$</span></p><ol start="3" type="1"><li>终止：</li></ol><p><span class="math display">$$P(O|\lambda)=\sum_{i=1}^N\alpha_T(i)$$</span></p><p>接下来，我们对每一个公式进行推导。</p><p>首先，我们要求解的目标是 <spanclass="math inline"><em>P</em>(<em>O</em>|<em>λ</em>) = ∑<sub><em>I</em></sub><em>P</em>(<em>I</em>,<em>O</em>|<em>λ</em>)</span>，而<spanclass="math inline"><em>α</em><sub><em>T</em></sub>(<em>i</em>) = <em>P</em>(<em>O</em>,<em>S</em><sub><em>T</em></sub>=<em>q</em><sub><em>i</em></sub>|<em>λ</em>)</span>，所以对于终止公式有：<span class="math display">$$\begin{aligned}P(O|\lambda)&amp;=\sum_{I}P(I,O|\lambda)\\&amp;=\sum_{i=1}^NP(o_1,o_2,...,o_T,S_T=q_i|\lambda)\\&amp;=\sum_{i=1}^N\alpha_T(i)\end{aligned}$$</span> 对于递推公式则有： <span class="math display">$$\begin{aligned}\alpha_{t+1}(i)&amp;=P(o_1,o_2,...,o_{t+1},S_{t+1}=q_i|\lambda)\\&amp;=P(S_{t+1}=q_i|\lambda)P(o_1,o_2,...,o_{t+1}|S_{t+1}=q_i,\lambda)\\&amp;=[\sum_{j=1}^NP(S_{t+1}=q_i,S_t=q_j|\lambda)]P(o_1,o_2,...,o_{t+1}|S_{t+1}=q_i,\lambda)\\&amp;=[\sum_{j=1}^NP(S_t=q_j|\lambda)P(S_{t+1}=q_i|S_t=q_j,\lambda)]P(o_1,o_2,...,o_{t+1}|S_{t+1}=q_i,\lambda)\\&amp;=[\sum_{j=1}^NP(o_1,...,o_t,S_t=q_j|\lambda)P(S_{t+1}=q_i|S_t=q_j,\lambda)]P(o_1,o_2,...,o_{t+1}|S_{t+1}=q_i,\lambda)\\&amp;=[\sum_{j=1}^N\alpha_t(j)a_{ji}]b_i(o_{t+1})\end{aligned}$$</span></p><p>对于初值有： <span class="math display">$$\begin{aligned}\alpha_1(i)&amp;=P(o_1,S_1=q_i|\lambda)\\&amp;=P(S_1=q_i|\lambda)P(o_1|S_1=q_i,\lambda)\\&amp;=\pi_1b_i(o_1)\end{aligned}$$</span></p><h2 id="后向算法">05 后向算法</h2><p>后向算法比前向算法稍微复杂一点，这一节着重讲解后向算法初值、递推和终止公式的推导。</p><p>给定隐马尔可夫模型，定义在时刻 <spanclass="math inline"><em>t</em></span> 状态为 <spanclass="math inline"><em>q</em><sub><em>i</em></sub></span> 的条件下，从<span class="math inline"><em>t</em> + 1</span> 到 <spanclass="math inline"><em>T</em></span> 的部分观测序列为 <spanclass="math inline"><em>o</em><sub><em>t</em> + 1</sub>, <em>o</em><sub><em>t</em> + 2</sub>, ..., <em>o</em><sub><em>T</em></sub></span>的概率称为<strong>后向概率</strong>，记作： <spanclass="math display"><em>β</em><sub><em>t</em></sub>(<em>i</em>) = <em>P</em>(<em>o</em><sub><em>t</em> + 1</sub>,<em>o</em><sub><em>t</em> + 2</sub>,...,<em>o</em><sub><em>T</em></sub>|<em>S</em><sub><em>t</em></sub>=<em>q</em><sub><em>i</em></sub>,<em>λ</em>)</span>观察后向概率的公式和定义，我们可以用另一种方法描述后向概率：当前时刻为<span class="math inline"><em>T</em></span>，也就是终止时刻，前 <spanclass="math inline"><em>T</em> − <em>t</em></span> 个时刻的观测序列为<spanclass="math inline"><em>o</em><sub><em>t</em> + 1</sub>, <em>o</em><sub><em>t</em> + 2</sub>, ..., <em>o</em><sub><em>T</em></sub></span>，且<span class="math inline"><em>t</em></span> 时刻隐藏状态恰好为 <spanclass="math inline"><em>q</em><sub><em>i</em></sub></span>的概率。可以发现，后向概率是以终止时刻为起点，倒退回去考虑的，与前向概率正好相反，所以递推的起点是<spanclass="math inline"><em>β</em><sub><em>T</em></sub>(<em>i</em>) = <em>P</em>(∅|<em>S</em><sub><em>T</em></sub>=<em>q</em><sub><em>i</em></sub>,<em>λ</em>)</span>，可以发现，当<span class="math inline"><em>t</em> = <em>T</em></span>时，已不存在后续观测序列，所以我们规定 <spanclass="math inline"><em>β</em><sub><em>T</em></sub>(<em>i</em>) = 1</span>。</p><p>我们要求解的是 <spanclass="math inline"><em>P</em>(<em>O</em>|<em>λ</em>) = <em>P</em>(<em>o</em><sub>1</sub>,<em>o</em><sub>2</sub>,...<em>o</em><sub><em>T</em></sub>|<em>λ</em>)</span>，后向算法递推的终点是序列的起始点，也就是<span class="math inline"><em>t</em> = 1</span>，而 <spanclass="math inline"><em>β</em><sub>1</sub>(<em>i</em>) = <em>P</em>(<em>o</em><sub>2</sub>,<em>o</em><sub>3</sub>,...,<em>o</em><sub><em>T</em></sub>|<em>S</em><sub><em>t</em></sub>=<em>q</em><sub><em>i</em></sub>,<em>λ</em>)</span>，这之间又要怎么转换？这就是后向算法比前向算法复杂的点，它并不像前向算法那样容易推导。首先，我们使用全概率公式和条件概率公式对<span class="math inline"><em>P</em>(<em>O</em>|<em>λ</em>)</span>进行变换： <span class="math display">$$\begin{aligned}P(O|\lambda)&amp;=\sum_{i=1}^{N}P(o_1,o_2,...,o_T,S_1=q_i|\lambda)\\&amp;=\sum_{i=1}^{N}P(o_1,o_2,...,o_T|S_1=q_i,\lambda)P(S_1=q_i|\lambda)\\&amp;=\sum_{i=1}^{N}P(o_1|o_2,...,o_T,S_1=q_i,\lambda)P(o_2,...,o_T|S_1=q_i,\lambda)\pi_i\\&amp;=\sum_{i=1}^Nb_i(o_1)\beta_1(i)\pi_i\end{aligned}$$</span> 经过上面的推导，我们就能发现 <spanclass="math inline"><em>P</em>(<em>O</em>|<em>λ</em>)</span> 和 <spanclass="math inline"><em>β</em><sub>1</sub>(<em>i</em>)</span>的联系。下一个要解决的就是 <spanclass="math inline"><em>β</em><sub><em>t</em></sub>(<em>i</em>)</span>的推导了，首先令 <spanclass="math inline"><em>β</em><sub><em>t</em> + 1</sub>(<em>j</em>) = <em>P</em>(<em>o</em><sub><em>t</em> + 2</sub>,<em>o</em><sub><em>t</em> + 3</sub>,...<em>o</em><sub><em>T</em></sub>|<em>S</em><sub><em>t</em> + 1</sub>=<em>q</em><sub><em>j</em></sub>,<em>λ</em>)</span>，其推导过程如下：<span class="math display">$$\begin{aligned}\beta_t(i)&amp;=P(o_{t+1},o_{t+2},...,o_T|S_t=q_i,\lambda)\\&amp;=\sum_{j=1}^{N}P(o_{t+1},o_{t+2},...,o_T,S_{t+1}=q_j|S_t=q_i,\lambda)\\&amp;=\sum_{j=1}^{N}P(o_{t+1},...,o_T|S_t=q_i,S_{t+1}=q_j,\lambda)P(S_{t+1}=q_j|S_t=q_i,\lambda)\\&amp;=\sum_{j=1}^{N}P(o_{t+1},...,o_T|S_{t+1}=q_j,\lambda)P(S_{t+1}=q_j|S_t=q_i,\lambda)\\&amp;=\sum_{j=1}^{N}P(o_{t+1}|o_{t+2},...,o_T,S_{t+1}=q_j,\lambda)P(o_{t+2},...,o_T|S_{t+1}=q_j,\lambda)P(S_{t+1}=q_j|S_t=q_i,\lambda)\\&amp;=\sum_{j=1}^N\beta_{t+1}(j)b_j(o_{t+1})a_{ij}\end{aligned}$$</span> 至此，我们就得到后向算法中的初值、递推和终止公式：</p><ol type="1"><li>初值：</li></ol><p><spanclass="math display"><em>β</em><sub><em>T</em></sub>(<em>i</em>) = 1</span></p><ol start="2" type="1"><li>递推：</li></ol><p><span class="math display">$$\beta_t(i)=\sum_{j=1}^N\beta_{t+1}(j)b_j(o_{t+1})a_{ij}$$</span></p><ol start="3" type="1"><li>终止：</li></ol><p><span class="math display">$$P(O|\lambda)=\sum_{i=1}^Nb_i(o_1)\beta_1(i)\pi_i$$</span></p><h2 id="baum-welch-算法">06 Baum-Welch 算法</h2><p>讨论完了如何求解 <spanclass="math inline"><em>P</em>(<em>O</em>|<em>λ</em>)</span>，下一步我们就要考虑最难的一个问题——如何求解HMM 的参数，即 <span class="math inline"><em>A</em></span>，<spanclass="math inline"><em>B</em></span>，<spanclass="math inline"><em>Π</em></span>。</p><p>如果是不加任何限制地考虑这个问题，那其实是很简单的。根据<strong>大数定理</strong>：在试验次数足够多的情况下，频数就等于概率。要想得到<span class="math inline"><em>A</em></span> 和 <spanclass="math inline"><em>B</em></span>，只需要对数据进行统计，计算每种状态出现的频数就行了，于是就有：<span class="math display">$$\begin{aligned}&amp;\hat{a}_{ij}=\frac{A_{ij}}{\sum_{j=1}^{N}A_{ij}}&amp;,i=1,2,...,N,j=1,2,...,N\\&amp;\hat{b}_{j}(k)=\frac{B_{jk}}{\sum_{k=1}^{M}B_{jk}}&amp;,j=1,2,...,N,k=1,2,...,M\\\end{aligned}$$</span> 解释一下取值范围：<span class="math inline"><em>A</em></span>是状态转移概率矩阵，这是隐藏状态和隐藏状态之间转移的概率，所以 <spanclass="math inline"><em>i</em></span> 和 <spanclass="math inline"><em>j</em></span> 的最大值都是隐藏状态的数量 <spanclass="math inline"><em>N</em></span>；而 <spanclass="math inline"><em>B</em></span>是生成观测状态概率矩阵，这是隐藏状态到观测状态之间转移的概率，令 <spanclass="math inline"><em>j</em></span>代表隐藏状态，其最大值就是隐藏状态的数量 <spanclass="math inline"><em>N</em></span>，<spanclass="math inline"><em>k</em></span>代表观测状态，其最大值就是观测状态的数量 <spanclass="math inline"><em>M</em></span>，我们之前讲到过，HMM中的隐藏状态和观测状态数量不一定要相同，所以 <spanclass="math inline"><em>N</em></span> 不一定等于 <spanclass="math inline"><em>M</em></span>。</p><p>至于 <span class="math inline"><em>Π</em></span>也很简单，根据往期数据计算就行了。所以如果只是像这样单纯地求解 HMM的参数，只要有数据，那就几乎是没有难度的。但我们来考虑一下求解目标中的第二个：给定观测序列<spanclass="math inline"><em>O</em> = {<em>o</em><sub>1</sub>, <em>o</em><sub>2</sub>, ..., <em>o</em><sub><em>t</em></sub>}</span>，求解参数<span class="math inline">(<em>A</em>,<em>B</em>,<em>Π</em>)</span> 使得<span class="math inline"><em>P</em>(<em>O</em>|<em>λ</em>)</span>最大。这要怎么做？</p><p>之前的讨论是在所有数据均有的情况下进行的，也就是隐藏状态序列 <spanclass="math inline"><em>I</em></span> 和观测状态序列 <spanclass="math inline"><em>O</em></span> 均已知的情况下，但现在只有观测序列<span class="math inline"><em>O</em></span>，要我们求最优的参数 <spanclass="math inline">(<em>A</em>,<em>B</em>,<em>Π</em>)</span>，使 <spanclass="math inline"><em>P</em>(<em>O</em>|<em>λ</em>)</span>最大。也就是说 <span class="math inline"><em>I</em></span>被隐藏了，这相当于是一个含隐变量的参数估计问题，需要 EM 算法来解决。</p><p>EM 算法应用到 HMM 中时通常被称为 Baum-Welch 算法，Baum-Welch 算法是EM 算法的一个特例。</p>]]></content>
      
      
      <categories>
          
          <category> machine learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> technique </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>支持向量机</title>
      <link href="/2023/12/11/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/"/>
      <url>/2023/12/11/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/</url>
      
        <content type="html"><![CDATA[<p><strong>支持向量机（support vector machines,SVM）</strong>是一种二分类模型，它的基本模型是定义在特征空间上的<strong>间隔最大的线性分类器</strong>，间隔最大使它有别于感知机；SVM还包括<strong>核技巧</strong>，这使它成为实质上的非线性分类器。SVM的的学习策略就是间隔最大化，可形式化为一个求解凸二次规划的问题，也等价于正则化的合页损失函数的最小化问题。SVM的的学习算法就是求解凸二次规划的最优化算法。</p><span id="more"></span><h2 id="线性模型">01 线性模型</h2><h3 id="算法思路">1.1 算法思路</h3><p>假设训练样本在空间中的分布如下左图分布，圆圈和星星分别代表两类不同类别的数据，那么我们能找出一条直线，将两者分割开，我们就称这样的训练样本集为一个<strong>线性可分（LinearSeparable）样本集</strong>，这样的模型就被称为<strong>线性模型</strong>；同理，倘若我们找不到这样一条直线，将两者完全分离开，如下右图所示，则称这样的训练样本集为一个<strong>线性不可分（Non-LinearSeparable）样本集</strong>，这样的模型就被称为<strong>非线性模型</strong>。</p><p><img src="/img/支持向量机-01.png" /></p><p><strong>支持向量机</strong>算法的思路大致是这样的：首先讨论如何在线性可分的训练样本集上找一条直线将样本分开，然后想办法将这样的方法推广到线性不可分的训练样本集上。所以，我们首先讨论第一部分：如何找到一条直线将线性可分训练样本集分开。</p><p>对于一个训练样本集，可以证明：<u>只要存在一条直线可以将样本集分开，就肯定存在无数条直线能将该样本集分开</u>（如下图所示）。既然如此，支持向量机提出的第一个问题就是：哪条直线是最好的？</p><p><img src="/img/支持向量机-02.png" /></p><p>通过直觉来判断，我们也可以感受出上图中的红色直线应该是最好的，问题是为什么？要解答这个问题，我们就必须定义一种性能指标（PerformaceMeasure），来评估每一条直线的好坏。</p><p>为了给出这个性能指标，支持向量机做的事情是，将上面的红线向左右两边平行移动，直到这条线碰到一个或几个样本点为止（如下图中两条虚线所示）：</p><p><img src="/img/支持向量机-03.png" /></p><p>然后，支持向量机给出了这个性能指标的定义，就是上图中两条虚线的距离（Gap），用<span class="math inline"><em>d</em></span>表示。而性能最好的那条线，就是能使 <spanclass="math inline"><em>d</em></span>最大的那条线。但是这样还不完善，要知道，能使 <spanclass="math inline"><em>d</em></span>最大的线也不唯一，将上图中的实线左右移动，作一条平行线，只要平行线不越过两条虚线所界定的范围，<spanclass="math inline"><em>d</em></span>就是不变的，所以还得给出另一个限制条件：直线必须位于两根平行线的正中间，也就是使上图中的实线与左右两根平行虚线的距离分别为<span class="math inline">$\frac d2$</span>。</p><p><img src="/img/支持向量机-04.png" /></p><h3 id="数学描述">1.2 数学描述</h3><p>既然性能指标已经确定了，下一个问题就是如何描述这个优化过程了。在描述优化过程之前，我们还是先得给出一些定义，首先，我们将上面的<span class="math inline"><em>d</em></span>称为<strong>间隔（Margin）</strong>，将虚线穿过的向量称为<strong>支持向量（SupportVectors）</strong>。通过上面对支持向量机算法的简单描述，我们可以发现，支持向量机找到的最优直线，只与支持向量有关，与其他向量无关，这就是为什么支持向量机也能用在小样本的数据上。</p><p>先给出线性模型的数学描述：</p><ol type="1"><li><p>定义训练数据及标签为 <spanclass="math inline">(<em>X</em><sub>1</sub>,<em>y</em><sub>1</sub>)、(<em>X</em><sub>2</sub>,<em>y</em><sub>2</sub>)...(<em>X</em><sub><em>n</em></sub>,<em>y</em><sub><em>n</em></sub>)</span>，其中，<spanclass="math inline"><em>X</em></span>是样本的特征，在上面给出的例子里，每个样本的特征是二维的，也就是说 <spanclass="math inline">$X=\left[ \begin{array}{} x_1 \\ x_2\end{array}\right]$</span> ，分别对应 x 轴和 y 轴；而 <spanclass="math inline"><em>y</em></span>是标签，在上面这个二分类问题里，标签只有两种，所以 <spanclass="math inline"><em>y</em> ∈ { − 1, 1}</span>。</p></li><li><p>我们定义一个线性模型为 <spanclass="math inline">(<em>W</em>,<em>b</em>)</span>，其中 <spanclass="math inline"><em>W</em></span> 是一个向量，其维数与特征向量 <spanclass="math inline"><em>X</em></span> 一致，<spanclass="math inline"><em>b</em></span>是一个常数，一个线性模型确定一个<strong>超平面（Hyperplane）</strong>，所谓超平面就是指划分空间的平面，超平面在二维空间里表现为我们上面所说的那条划分样本点的直线，而在更高的维度里就是一个平面，故称之为超平面。超平面由<span class="math inline"><em>W</em></span> 和 <spanclass="math inline"><em>b</em></span> 确定，其方程为 <spanclass="math inline"><em>W</em><sup><em>T</em></sup><em>X</em> + <em>b</em> = 0</span>。机器学习的目标，就是通过所有样本的特征<span class="math inline"><em>X</em></span> 来找到一个 <spanclass="math inline"><em>W</em></span> 和 <spanclass="math inline"><em>b</em></span>，使能够确定一个超平面能划分所有的样本点。</p></li><li><p>一个训练集线性可分是指：对于 <spanclass="math inline">{(<em>X</em><sub><em>i</em></sub>,<em>y</em><sub><em>i</em></sub>)}<sub><em>i</em> = 1 ∼ <em>N</em></sub></span>，<spanclass="math inline">$\exist(W,b)$</span>，使 <spanclass="math inline">∀<em>i</em> = 1 ∼ <em>N</em></span>，有：</p><ol type="1"><li>若 <spanclass="math inline"><em>y</em><sub><em>i</em></sub> =  + 1</span>，则<spanclass="math inline"><em>W</em><sup><em>T</em></sup><em>X</em><sub><em>i</em></sub> + <em>b</em> ≥ 0</span>；</li><li>若 <spanclass="math inline"><em>y</em><sub><em>i</em></sub> =  − 1</span>，则<spanclass="math inline"><em>W</em><sup><em>T</em></sup><em>X</em><sub><em>i</em></sub> + <em>b</em> &lt; 0</span>。</li></ol><p>当然，上述线性可分的定义是不唯一的，将 <spanclass="math inline">≥</span> 和 <span class="math inline">&lt;</span>换个位置也是一样。对于上面的定义，我们可以发现，凡是线性可分问题，一定存在<spanclass="math inline"><em>y</em><sub><em>i</em></sub>[<em>W</em><sup><em>T</em></sup><em>X</em><sub><em>i</em></sub>+<em>b</em>] ≥ 0</span>。</p></li></ol><p>接下来给出支持向量机优化问题的数学描述：</p><ol type="1"><li>目标：最小化 <spanclass="math inline">∥<em>W</em>∥<sup>2</sup></span></li><li>限制条件：<spanclass="math inline"><em>y</em><sub><em>i</em></sub>[<em>W</em><sup><em>T</em></sup><em>X</em><sub><em>i</em></sub>+<em>b</em>] ≥ 1  (<em>i</em>=1∼<em>N</em>)</span></li></ol><p>对于上面这两个公式，相信很多人第一眼是懵的，因为按我们之前的描述，支持向量机算法就是去找一条使<span class="math inline"><em>d</em></span>最大且位于正中间位置的直线，怎么数学公式看起来跟这个过程完全没关系呢？</p><p>要搞清楚这两个公式，我们得先弄清楚两个事实：</p><ol type="1"><li>事实一：<spanclass="math inline"><em>W</em><sup><em>T</em></sup><em>X</em> + <em>b</em> = 0</span>与 <spanclass="math inline"><em>a</em><em>W</em><sup><em>T</em></sup><em>X</em> + <em>a</em><em>b</em> = 0  (<em>a</em>∈<em>R</em><sup>+</sup>)</span>表示的是同一个平面。</li><li>事实二：向量 <span class="math inline"><em>X</em><sub>0</sub></span>到超平面 <spanclass="math inline"><em>W</em><sup><em>T</em></sup><em>X</em> + <em>b</em> = 0</span>的距离是 <span class="math inline">$d=\frac{|W^TX_0+b|}{\lVertW\rVert}$</span>。（不要慌，这其实就是高中学的点到平面的距离公式，以一维平面<spanclass="math inline"><em>w</em><sub>1</sub><em>x</em> + <em>w</em><sub>2</sub><em>y</em> + <em>b</em> = 0</span>，也就是直线为例，点<spanclass="math inline">(<em>x</em><sub>0</sub>,<em>y</em><sub>0</sub>)</span>到这条直线的距离就是 <spanclass="math inline">$d=\frac{|w_1x_0+w_2y_0+b|}{\sqrt{w_1^2+w_2^2}}$</span>，前面的那个公式只不是这个公式在高维情况下的推广）</li></ol><p>基于事实二，我们知道，支持向量机要做的事情，就是在 <spanclass="math inline"><em>X</em><sub>0</sub></span> 是支持向量的情况下，使<span class="math inline"><em>d</em></span> 最大。</p><p>基于事实一，我们知道，我们可以找到一个正实数 <spanclass="math inline"><em>a</em></span> 来缩放 <spanclass="math inline"><em>W</em></span> 和 <spanclass="math inline"><em>b</em></span>，即 <spanclass="math inline">(<em>W</em>,<em>b</em>) → (<em>a</em><em>W</em>,<em>a</em><em>b</em>)</span>，使<span class="math inline"><em>d</em></span> 公式的分子 <spanclass="math inline">|<em>W</em><sup><em>T</em></sup><em>X</em><sub>0</sub>+<em>b</em>| = 1</span>。这样的话，<spanclass="math inline"><em>d</em></span> 的公式就变成了 <spanclass="math inline">$d=\frac{1}{\lVertW\rVert}$</span>。看到这个公式，就能明白为什么支持向量机的优化目标是最小化<span class="math inline">∥<em>W</em>∥<sup>2</sup></span> 了，因为最小化<span class="math inline">∥<em>W</em>∥<sup>2</sup></span> 就是最大化<span class="math inline"><em>d</em></span>。</p><p>现在再来看限制条件，限制条件其实就是规定了，所有样本点到超平面的距离<spanclass="math inline"><em>W</em><sup><em>T</em></sup><em>X</em><sub><em>i</em></sub> + <em>b</em></span>，要么等于<span class="math inline"><em>d</em></span>（支持向量），要么大于 <spanclass="math inline"><em>d</em></span>（非支持向量）。至于为什么要再乘上一个 <spanclass="math inline"><em>y</em><sub><em>i</em></sub></span>，其实看<u>线性可分的定义</u>就知道了，乘上<span class="math inline"><em>y</em><sub><em>i</em></sub></span>是为了与线性可分的定义相统一。</p><blockquote><p>补充：</p><ul><li><p>为什么一定要使 <spanclass="math inline">|<em>W</em><sup><em>T</em></sup><em>X</em><sub>0</sub>+<em>b</em>| = 1</span>，<spanclass="math inline">|<em>W</em><sup><em>T</em></sup><em>X</em><sub>0</sub>+<em>b</em>| = 2</span>可不可以？可以，等于 1 还是等于 2 或是其他值都没有任何关系，这只取决于<span class="math inline"><em>a</em></span> 的大小，而 <spanclass="math inline"><em>a</em></span> 并不改变超平面。</p></li><li><p>对于任何线性可分样本集，一定能找到一个超平面分割所有样本点；反之，如果是线性不可分，那么将找不到任何一个能满足要求的<span class="math inline"><em>W</em></span> 和 <spanclass="math inline"><em>b</em></span>。</p></li><li><p>某些书上会将优化目标写成最小化 <spanclass="math inline">$\frac12\lVertW\rVert^2$</span>，这其实没有任何问题，加上 <spanclass="math inline">$\frac12$</span> 只是为了求导方便。</p></li><li><p>支持向量机要解决的问题其实是一个凸优化问题，而且是一个二次规划问题，二次规划问题的特点是：</p><ul><li>目标函数（Objective Function）是二次项；</li><li>限制条件是一次项。</li></ul><p>对于凸优化问题，要么无解，要么只有一个解。凸优化问题是计算机领域研究最多的问题，因为凸优化问题要么无解，要么只要能找到一个解，那便是它唯一的解。所以只要证明一个问题是凸优化问题，那么我们只要找到一个局部极值，也便找到了它的全局极值，我们便可认定这个问题已经被解决了。</p><p>非凸问题的目标函数图像是一条包含很多局部极值的曲线，会使得机器很容易落入局部最优解的陷阱。支持向量机算法优美的地方就在于，它将求解目标化成了一个凸优化问题。</p></li></ul></blockquote><h2 id="非线性模型">02 非线性模型</h2><h3 id="优化目标">2.1 优化目标</h3><p>之前已经讨论过，非线性模型不是线性可分的，也就是说找不到一个 <spanclass="math inline"><em>W</em></span> 和 <spanclass="math inline"><em>b</em></span>，使之确定一个能完美分割所有样本点的超平面，即限制条件<spanclass="math inline"><em>y</em><sub><em>i</em></sub>[<em>W</em><sup><em>T</em></sup><em>X</em><sub><em>i</em></sub>+<em>b</em>] ≥ 1  (<em>i</em>=1∼<em>N</em>)</span>是不可满足的，原本的优化目标是无解的。SVM处理非线性模型的方法其实不难理解，就是在线性模型的基础上引入了一个<strong>松弛变量（SlackVariable）</strong>，用 <spanclass="math inline"><em>ξ</em> (<em>ξ</em>≥0)</span>表示。新的优化目标如下：</p><ol type="1"><li>目标：最小化 <span class="math inline">$\frac12\lVertW\rVert^2+C\sum_{i=1}^N\xi_i$</span></li><li>限制条件：<ol type="1"><li><spanclass="math inline"><em>y</em><sub><em>i</em></sub>[<em>W</em><sup><em>T</em></sup><em>X</em><sub><em>i</em></sub>+<em>b</em>] ≥ 1 − <em>ξ</em><sub><em>i</em></sub>  (<em>i</em>=1∼<em>N</em>)</span></li><li><spanclass="math inline"><em>ξ</em><sub><em>i</em></sub> ≥ 0</span></li></ol></li></ol><p>可以发现，新的优化目标中，限制条件变成了 <spanclass="math inline"><em>y</em><sub><em>i</em></sub>[<em>W</em><sup><em>T</em></sup><em>X</em><sub><em>i</em></sub>+<em>b</em>] ≥ 1 − <em>ξ</em><sub><em>i</em></sub>  (<em>i</em>=1∼<em>N</em>)</span>，只要这个<span class="math inline"><em>ξ</em><sub><em>i</em></sub></span>取得足够大，那么大于等于号右边就会无限小，那么限制条件就有了满足的可能；但同时，也不能允许<span class="math inline"><em>ξ</em><sub><em>i</em></sub></span>无限大，不然就没有意义了，所以新的最小化目标函数的末尾还要加上 <spanclass="math inline"><em>ξ</em><sub><em>i</em></sub></span>。</p><p>接下来要明确，在上面的式子中，哪些是已知的，哪些是要求解的参数。显然，<spanclass="math inline"><em>X</em></span>和<spanclass="math inline"><em>y</em><sub><em>i</em></sub></span>是已知的，<spanclass="math inline"><em>W</em></span>、<spanclass="math inline"><em>b</em></span>以及<spanclass="math inline"><em>ξ</em></span>是要求的，但是这里还有个<spanclass="math inline"><em>C</em></span>，这个<spanclass="math inline"><em>C</em></span>是什么？<spanclass="math inline"><em>C</em></span>是一个由人事先设定的参数，这种参数一般称为<strong>超参数</strong>（<strong>Hyperparameter</strong>），作用是平衡<spanclass="math inline">$\frac{1}{2}\lVert\ W\rVert^{2}$</span>和<spanclass="math inline">$\sum_{i=1}^{N}\xi_{i}$</span>的权重。至于<spanclass="math inline"><em>C</em></span>具体取多少是没有定论的，一般是凭经验，选定一个区间，然后一个一个尝试。SVM很方便的一点就是，它只有这一个参数需要人来设置，但是在神经网络里，要去一个一个尝试的参数可能有很多。</p><h3 id="高维映射">2.2 高维映射</h3><p>虽然通过引入松弛变量，我们将非线性问题转换为了一个线性可分问题，但是还是存在一个问题，那就是求解目标的本质没有变，最后仍然是找出一条直线，来分割样本点，也就是说，即使一个样本集用肉眼看就能看出其能被一条简单的曲线分割，SVM还是会找一条直线来分割样本点，如下图所示：</p><p><img src="/img/支持向量机-05.png" /></p><p>这显然不是我们想要的。一些算法会很符合直觉地去找非直线来分割样本集，例如决策树是用矩形来分割，但是SVM的思想很精妙，它仍然是找直线，不过它不是在当前空间里去找，而是到高维空间里去找。它定义了一个<strong>高维映射</strong><span class="math inline"><em>ϕ</em>(<em>X</em>)</span>，通过 <spanclass="math inline"><em>ϕ</em></span>，能将 <spanclass="math inline"><em>X</em></span> 这个低维向量转化成一个高维向量<spanclass="math inline"><em>ϕ</em>(<em>X</em>)</span>。也就是说，也许在低维空间中，我们不容易去找一条直线能刚刚好分割所有样本点，那么我们就去高维空间中找，或许在高维空间中，我们就能找到样一条理想的直线了。</p><p>接下来我们用异或问题的例子，来具体解释这个过程为什么有效。异或问题是二维空间下最简单的非线性问题，其在二维空间中存在如下样本点分布：</p><p><img src="/img/支持向量机-06.png" /></p><p>我们先将图中四个样本点表示为 <spanclass="math inline"><em>X</em><sub>1</sub></span>、<spanclass="math inline"><em>X</em><sub>2</sub></span>、<spanclass="math inline"><em>X</em><sub>3</sub></span> 和 <spanclass="math inline"><em>X</em><sub>4</sub></span>，并且 <spanclass="math inline"><em>X</em><sub>1</sub></span> 和 <spanclass="math inline"><em>X</em><sub>2</sub></span> 属于一个类别 <spanclass="math inline"><em>C</em><sub>1</sub></span>，<spanclass="math inline"><em>X</em><sub>3</sub></span> 和 <spanclass="math inline"><em>X</em><sub>4</sub></span> 属于一个类别 <spanclass="math inline"><em>C</em><sub>2</sub></span>，有： <spanclass="math display">$$\begin{aligned}&amp;X_1=\left[ \begin{array}{} 0 \\ 0 \end{array} \right]\quadX_2=\left[ \begin{array}{} 1 \\ 1 \end{array} \right]\quad\in C_1\\&amp;X_3=\left[ \begin{array}{} 1 \\ 0 \end{array} \right]\quadX_4=\left[ \begin{array}{} 0 \\ 1 \end{array} \right]\quad\in C_2\\\end{aligned}$$</span> 定义高维映射函数为： <span class="math display">$$\phi(X):\quad X=\left[ \begin{array}{} a \\ b \end{array}\right]\overset{\phi}{\longrightarrow}\phi(X)=\left[ \begin{array}{} a^2\\ b^2 \\ a \\ b \\ ab \end{array} \right]$$</span> 则经过映射，四个样本点将变为： <span class="math display">$$\begin{aligned}&amp;\phi(X_1)=\left[ \begin{array}{} 0 \\ 0 \\ 0 \\ 0 \end{array}\right]\quad \phi(X_2)=\left[ \begin{array}{} 1 \\ 1 \\ 1 \\ 1\end{array} \right]\quad\in C_1\\&amp;\phi(X_3)=\left[ \begin{array}{} 1 \\ 0 \\ 1 \\ 0 \\ 0 \end{array}\right]\quad \phi(X_4)=\left[ \begin{array}{} 0 \\ 1 \\ 0 \\ 1 \\ 0\end{array} \right]\quad\in C_2\\\end{aligned}$$</span> 现在，<span class="math inline"><em>X</em></span>变成了五维向量，则 <span class="math inline"><em>W</em></span>也要变成五维向量，<span class="math inline"><em>b</em></span>仍然为常量，求解的目标就变成在五维空间中找一个超平面来分割四个样本点了。能做到分割的超平面不唯一，这里举一个例子：<span class="math display">$$W=\left[ \begin{array}{} -1 \\ -1 \\ -1 \\ -1 \\ 6 \end{array}\right]\quad b=1$$</span> 将样本点代入超平面的方程： <span class="math display">$$\begin{aligned}&amp;W^T\phi(X_1)+b=1&gt;0\\&amp;W^T\phi(X_2)+b=3&gt;0\\&amp;W^T\phi(X_3)+b=-1&lt;0\\&amp;W^T\phi(X_4)+b=-1&lt;0\\\end{aligned}$$</span> 如上，该超平面刚刚好把 <spanclass="math inline"><em>X</em><sub>1</sub></span>、<spanclass="math inline"><em>X</em><sub>2</sub></span> 与 <spanclass="math inline"><em>X</em><sub>3</sub></span>、<spanclass="math inline"><em>X</em><sub>4</sub></span>分开了。也就是说，在低维空间中线性不可分的样本集，可能在高维空间中就是线性可分的，这也就是我们要去升维的原因。关于这一点也有很多人研究过，它们的结论是，对于任何线性不可分的样本集，特征空间的维数越高，其被线性分割的概率也越大；若维数趋近无穷大，那么其被线性分割的概率将达到1.</p><h3 id="核函数">2.3 核函数</h3><p>在引入了高维映射之后，优化式 1 就变成了 <spanclass="math inline"><em>y</em><sub><em>i</em></sub>[<em>W</em><sup><em>T</em></sup><em>ϕ</em>(<em>X</em><sub><em>i</em></sub>)+<em>b</em>] ≥ 1 − <em>ξ</em><sub><em>i</em></sub>  (<em>i</em>=1∼<em>N</em>)</span>，虽然看起来只有<span class="math inline"><em>X</em><sub><em>i</em></sub></span>发生了变化，但不要忘记 <span class="math inline"><em>W</em></span>也跟着一起升维了。那么现在面临的问题就是：<u>如何选取 <spanclass="math inline"><em>ϕ</em></span></u> ？SVM 的回答是：无限维。</p><p>将特征空间增长到无限维，线性不可分问题就绝对可以变成线性可分。但是问题在于，当<span class="math inline"><em>ϕ</em>(<em>X</em>)</span>变成无限维，<span class="math inline"><em>W</em></span>也要变成无限维，那这个问题就没有办法做了。这也是 SVM巧妙的另一个地方，它在将特征空间映射到无限维的同时，又采用有限维的手段。</p><p>SVM 的意思是：我们可以不知道无限维映射 <spanclass="math inline"><em>ϕ</em>(<em>X</em>)</span>的显式表达，我们只要知道一个<strong>核函数（Kernel Function）</strong>：<spanclass="math display"><em>K</em>(<em>X</em><sub>1</sub>,<em>X</em><sub>2</sub>) = <em>ϕ</em>(<em>X</em><sub>1</sub>)<sup><em>T</em></sup><em>ϕ</em>(<em>X</em><sub>2</sub>)</span>则优化式 1 仍然可解。<spanclass="math inline"><em>K</em>(<em>X</em><sub>1</sub>,<em>X</em><sub>2</sub>)</span>其实计算的是 <spanclass="math inline"><em>ϕ</em>(<em>X</em><sub>1</sub>)</span> 和 <spanclass="math inline"><em>ϕ</em>(<em>X</em><sub>2</sub>)</span>的内积，虽然 <spanclass="math inline"><em>ϕ</em>(<em>X</em><sub>1</sub>)</span> 和 <spanclass="math inline"><em>ϕ</em>(<em>X</em><sub>2</sub>)</span>是无限维的，但是两者仍然能进行内积计算，得到的结果 <spanclass="math inline"><em>K</em>(<em>X</em><sub>1</sub>,<em>X</em><sub>2</sub>)</span>是一个数。</p><p>核函数的要求是：能将函数的形式最终拆成 <spanclass="math inline"><em>ϕ</em>(<em>X</em><sub>1</sub>)<sup><em>T</em></sup><em>ϕ</em>(<em>X</em><sub>2</sub>)</span>的形式。常用的核函数有如下几个：</p><ol type="1"><li>高斯核：<span class="math inline">$K(X_1,X_2)=e^{-\frac{\lVertX_1-X_2\rVert^2}{2\sigma^2}}=\phi(X_1)^T\phi(X_2)$</span>，<spanclass="math inline"><em>σ</em><sup>2</sup></span> 是方差。</li><li>多项式核：<spanclass="math inline"><em>K</em>(<em>X</em><sub>1</sub>,<em>X</em><sub>2</sub>) = (<em>X</em><sub>1</sub><sup><em>T</em></sup><em>X</em><sub>2</sub>+1)<sup><em>d</em></sup> = <em>ϕ</em>(<em>X</em><sub>1</sub>)<sup><em>T</em></sup><em>ϕ</em>(<em>X</em><sub>2</sub>)</span>，<spanclass="math inline"><em>d</em></span> 是多项式阶数。</li></ol><p>而能将核函数 <spanclass="math inline"><em>K</em>(<em>X</em><sub>1</sub>,<em>X</em><sub>2</sub>)</span>拆成 <spanclass="math inline"><em>ϕ</em>(<em>X</em><sub>1</sub>)<sup><em>T</em></sup><em>ϕ</em>(<em>X</em><sub>2</sub>)</span>的充要条件是：</p><ol type="1"><li><spanclass="math inline"><em>K</em>(<em>X</em><sub>1</sub>,<em>X</em><sub>2</sub>) = <em>K</em>(<em>X</em><sub>2</sub>,<em>X</em><sub>1</sub>)</span>，即交换性；</li><li><spanclass="math inline">∀<em>C</em><sub><em>i</em></sub>, <em>X</em><sub><em>i</em></sub> (<em>i</em>=1∼<em>N</em>)</span>，有<spanclass="math inline">$\sum_{i=1}^{N}\sum_{j=1}^{N}C_iC_jK(X_i,X_j)\ge0$</span>，即半正定性，也就是说，我们选取的核函数，必须要对任意选定的常数<span class="math inline"><em>C</em></span> 和向量 <spanclass="math inline"><em>X</em></span> 都满足该式；</li></ol><h3 id="原问题到对偶问题">2.4 原问题到对偶问题</h3><p>现在我们有了核函数，那么我们要怎样利用核函数，来替代优化式 1 中的<span class="math inline"><em>ϕ</em>(<em>X</em>)</span>呢？在这之前，请先阅读<ahref="#7.3*%20补充：优化理论">优化理论</a>相关的内容。在稍微了解了优化理论中的原问题和对偶问题后，我们要做的，就是<u>把SVM 的优化问题从原问题转换成对偶问题</u>。</p><p>首先，我们<u>把 SVM 的优化问题转换成原问题</u>：</p><p>对于目标函数，<span class="math inline">$\frac12\lVertW\rVert^2+C\sum_{i=1}^N\xi_i$</span> 是一个<strong>凸函数</strong>。</p><p>对于限制条件，<spanclass="math inline"><em>ξ</em><sub><em>i</em></sub> ≥ 0</span>不满足原问题的限制条件形式，得先将大于等于号变成小于等于号，也就是变成<spanclass="math inline"><em>ξ</em><sub><em>i</em></sub> ≤ 0</span>，那么，目标函数就也得变换一下，变成<span class="math inline">$\frac12\lVertW\rVert^2-C\sum_{i=1}^N\xi_i$</span>；同样，另一个限制条件也得变换一下，变成<spanclass="math inline"><em>y</em><sub><em>i</em></sub>[<em>W</em><sup><em>T</em></sup><em>X</em><sub><em>i</em></sub>+<em>b</em>] ≥ 1 + <em>ξ</em><sub><em>i</em></sub>  (<em>i</em>=1∼<em>N</em>)</span>，但是这个不等式也不满足原问题的要求，必须将不等式右边变成0，所以得到 <spanclass="math inline">1 + <em>ξ</em><sub><em>i</em></sub> − <em>y</em><sub><em>i</em></sub>[<em>W</em><sup><em>T</em></sup><em>X</em><sub><em>i</em></sub>+<em>b</em>] ≤ 0  (<em>i</em>=1∼<em>N</em>)</span>。于是得到优化目标的原问题形式，新的优化目标：</p><ol type="1"><li>目标：最小化 <span class="math inline">$\frac12\lVertW\rVert^2-C\sum_{i=1}^N\xi_i$</span></li><li>限制条件：<ol type="1"><li><spanclass="math inline">1 + <em>ξ</em><sub><em>i</em></sub> − <em>y</em><sub><em>i</em></sub>[<em>W</em><sup><em>T</em></sup><em>ϕ</em>(<em>X</em><sub><em>i</em></sub>)+<em>b</em>] ≤ 0  (<em>i</em>=1∼<em>N</em>)</span></li><li><spanclass="math inline"><em>ξ</em><sub><em>i</em></sub> ≤ 0</span></li></ol></li></ol><p><u>将其转换为对偶问题</u>：</p><ol type="1"><li><p>目标：最大化 <spanclass="math inline">$\theta(\alpha,\beta)=\underset{(w,\xi_i,b)}{\inf}\{\frac12\lVertW\rVert^2-C\sum_{i=1}^N\xi_i+\sum_{i=1}^{N}\alpha_i(1+\xi_i-y_i[W^T\phi(X_i)+b])+\sum_{i=1}^N\beta_i\xi_i\}$</span></p></li><li><p>限制条件：</p><ol type="1"><li><spanclass="math inline"><em>α</em><sub><em>i</em></sub> ≥ 0  (<em>i</em>=1∼<em>N</em>)</span></li><li><spanclass="math inline"><em>β</em><sub><em>i</em></sub> ≥ 0  (<em>i</em>=1∼<em>N</em>)</span></li></ol></li></ol><p>解释一下这样变换的原因：</p><ol type="1"><li>原问题中的 <span class="math inline"><em>w</em></span>对应了原问题要求解的变量，有三个，分别是 <spanclass="math inline"><em>W</em></span>、<spanclass="math inline"><em>b</em></span> 和 <spanclass="math inline"><em>ξ</em></span>，所以对偶问题中要遍历所有的 <spanclass="math inline"><em>w</em></span>，到这里就变成了遍历所有的 <spanclass="math inline"><em>W</em></span>、<spanclass="math inline"><em>b</em></span> 和 <spanclass="math inline"><em>ξ</em></span>。</li><li>根据对偶问题的定义，<spanclass="math inline">$L(\omega,\alpha,\beta)=f(\omega)+\sum_{i=1}^{K}\alpha_ig_i(\omega)+\sum_{i=1}^M\beta_ih_i(\omega)$</span>，其中，<spanclass="math inline">$f(w)=\frac12\lVertW\rVert^2-C\sum_{i=1}^N\xi_i$</span>，这一点是没有疑问的，关键是下面，千万不要以为这里的<span class="math inline"><em>α</em></span> 和 <spanclass="math inline"><em>β</em></span> 分别对应了上面的 <spanclass="math inline"><em>α</em><sub><em>i</em></sub></span> 和 <spanclass="math inline"><em>β</em><sub><em>i</em></sub></span>，不是这样的，在对偶问题中，<spanclass="math inline"><em>α</em></span>管的是不等式条件，每个不等式条件要与 <spanclass="math inline"><em>α</em></span> 相乘，<spanclass="math inline"><em>β</em></span> 管的是等式条件，每个等式条件要与<span class="math inline"><em>β</em></span> 相乘。但是在这里，SVM原问题中的限制条件都是不等式，所以应该只有 <spanclass="math inline"><em>α</em></span>，没有 <spanclass="math inline"><em>β</em></span>，只是说为了方便表示，这里仍然沿用<span class="math inline"><em>α</em><sub><em>i</em></sub></span> 和<spanclass="math inline"><em>β</em><sub><em>i</em></sub></span>，并且，由于<span class="math inline"><em>α</em></span> 应该大于0，所以到这里就变成了 <spanclass="math inline"><em>α</em><sub><em>i</em></sub> ≥ 0  (<em>i</em>=1∼<em>N</em>)</span>，并且<spanclass="math inline"><em>β</em><sub><em>i</em></sub> ≥ 0  (<em>i</em>=1∼<em>N</em>)</span>。其他部分就是照抄的关系了。</li></ol><p>接下来我们就来求一下 <spanclass="math inline"><em>L</em>(<em>W</em>,<em>ξ</em><sub><em>i</em></sub>,<em>b</em>,<em>α</em>)</span>的最小值：</p><p>令偏导 <span class="math inline">$\frac{\partial L}{\partialW}=0$</span>，<span class="math inline">$\frac{\partial L}{\partial\xi_i}=0$</span>，<span class="math inline">$\frac{\partial L}{\partialb}=0$</span>： <span class="math display">$$\begin{aligned}&amp;\frac{\partial L}{\partial W}=0\RightarrowW=\sum_{i=1}^N\alpha_iy_i\phi(X_i)&amp;①\\&amp;\frac{\partial L}{\partial \xi_i}=0\RightarrowC=\beta_i+\alpha_i&amp;②\\&amp;\frac{\partial L}{\partial b}=0\Rightarrow\sum_{i=1}^N\alpha_iy_i=0&amp;③\end{aligned}$$</span> 接下来，我们要将上面得到的三个式子代回到 <spanclass="math inline"><em>L</em>(<em>W</em>,<em>ξ</em><sub><em>i</em></sub>,<em>b</em>,<em>α</em>)</span>中去，好消息是，将式 1 和式 3代入之后，式子中的大部分项就能被消掉了，得到 <spanclass="math inline">$\theta(\alpha,\beta)=\frac12\lVertW\rVert^2+\sum_{i=1}^N\alpha_i-\sum_{i=1}^N\alpha_iy_iW^T\phi(X_i)$</span>，先来计算<span class="math inline">$\frac12\lVert W\rVert^2$</span>： <spanclass="math display">$$\begin{aligned}\frac12\lVert W\rVert^2&amp;=\frac12W^TW\\&amp;=\frac12(\sum_{i=1}^N\alpha_iy_i\phi(X_i))^T(\sum_{j=1}^N\alpha_jy_j\phi(X_j))\\&amp;=\frac12\sum_{i=1}^N\sum_{j=1}^N\alpha_i\alpha_jy_iy_j\phi(X_i)^T\phi(X_j)\\&amp;=\frac12\sum_{i=1}^N\sum_{j=1}^N\alpha_i\alpha_jy_iy_jK(X_i,X_j)\end{aligned}$$</span> 一件惊喜的事情：上式的最终化简结果里出现了核函数！接下来化简<spanclass="math inline">$-\sum_{i=1}^N\alpha_iy_iW^T\phi(X_i)$</span>：<span class="math display">$$\begin{aligned}-\sum_{i=1}^N\alpha_iy_iW^T\phi(X_i)&amp;=-\sum_{i=1}^N\alpha_iy_i(\sum_{j=1}^N\alpha_jy_j\phi(X_j))^T\phi(X_i)\\&amp;=-\sum_{i=1}^N\sum_{j=1}^N\alpha_i\alpha_jy_iy_j\phi(X_j)^T\phi(X_i)\\&amp;=-\sum_{i=1}^N\sum_{j=1}^N\alpha_i\alpha_jy_iy_jK(X_i,X_j)\end{aligned}$$</span> 所以，最后会得到： <span class="math display">$$\theta(\alpha,\beta)=\sum_{i=1}^{N}\alpha_i-\frac12\sum_{i=1}^N\sum_{j=1}^N\alpha_i\alpha_jy_iy_jK(X_i,X_j)$$</span> 经过这样一系列的推导，最终问题的形式会变成：</p><ol type="1"><li><p>目标：最大化 <spanclass="math inline">$\theta(\alpha)=\sum_{i=1}^{N}\alpha_i-\frac12\sum_{i=1}^N\sum_{j=1}^N\alpha_i\alpha_jy_iy_jK(X_i,X_j)$</span></p></li><li><p>限制条件：</p><ol type="1"><li><spanclass="math inline">0 ≤ <em>α</em><sub><em>i</em></sub> ≤ <em>C</em>  (<em>i</em>=1∼<em>N</em>)</span></li><li><span class="math inline">$\sum_{i=1}^N\alpha_iy_i=0$</span></li></ol></li></ol><p>解释一下限制条件：根据之前求的偏导我们得到了 <spanclass="math inline"><em>β</em><sub><em>i</em></sub> + <em>α</em><sub><em>i</em></sub> = <em>C</em></span>，由于之前的限制条件规定了<span class="math inline"><em>β</em><sub><em>i</em></sub> ≥ 0</span>以及 <spanclass="math inline"><em>α</em><sub><em>i</em></sub> ≥ 0</span>，所以我们可以直接把这两个条件合并成一个条件，即<spanclass="math inline">0 ≤ <em>α</em><sub><em>i</em></sub> ≤ <em>C</em>  (<em>i</em>=1∼<em>N</em>)</span>，那么为什么要合并呢？因为我们现在的目标函数中只剩下了<span class="math inline"><em>α</em><sub><em>i</em></sub></span> 和<spanclass="math inline"><em>α</em><sub><em>j</em></sub></span>，已经不存在<span class="math inline"><em>β</em></span>了；而第二个限制条件则是直接照抄的令 <spanclass="math inline">$\frac{\partial L}{\partial b}=0$</span>得到的结果。</p><p>在这个对偶问题中，目标函数仍然是一个<strong>凸函数</strong>。并且，其中未知的参数只有<span class="math inline"><em>α</em><sub><em>i</em></sub></span> 和<spanclass="math inline"><em>α</em><sub><em>j</em></sub></span>，核函数是已经确定的了。由于是一个凸优化问题，所以它应该是很容易求解的。有一种凸优化问题求解算法叫做<strong>SMO算法</strong>，在这里不再展开叙述，感兴趣的同学自行了解。总之，我们只需要知道，这个问题是有解的。</p><p>但是到这里还没结束，我们现在已经将 SVM的优化问题从原问题转换成了对偶问题，将 <spanclass="math inline"><em>ϕ</em>(<em>X</em><sub><em>i</em></sub>)</span>用核函数进行了替换，但是还有一个问题：<u>对偶问题是求解 <spanclass="math inline"><em>α</em><sub><em>i</em></sub></span> 和 <spanclass="math inline"><em>α</em><sub><em>j</em></sub></span>，而我们要的是<span class="math inline"><em>W</em></span> 和 <spanclass="math inline"><em>b</em></span>，如何在这两者之间进行转换？</u></p><p>这里又体现了 SVM 的精妙之处，我们并不需要知道 <spanclass="math inline"><em>W</em></span>具体长什么样，根据我们之前求偏导的结果，我们知道 <spanclass="math inline">$W=\sum_{i=1}^N\alpha_iy_i\phi(X_i)$</span>，同时我们也知道，最后分类的方法是，对于测试样本<span class="math inline"><em>X</em></span>，若：</p><ol type="1"><li><spanclass="math inline"><em>W</em><sup><em>T</em></sup><em>ϕ</em>(<em>X</em>) + <em>b</em> ≥ 0</span>，则<span class="math inline"><em>y</em> =  + 1</span></li><li><spanclass="math inline"><em>W</em><sup><em>T</em></sup><em>ϕ</em>(<em>X</em>) + <em>b</em> &lt; 0</span>，则<span class="math inline"><em>y</em> =  − 1</span></li></ol><p>我们将 <spanclass="math inline">$W=\sum_{i=1}^N\alpha_iy_i\phi(X_i)$</span>代入到不等式左边的式子中，就会得到： <span class="math display">$$\begin{aligned}W^T\phi(X)+b&amp;=\sum_{i=1}^N[\alpha_iy_i\phi(X_i)]^T\phi(X)+b\\&amp;=\sum_{i=1}^N\alpha_iy_i\phi(X_i)^T\phi(X)+b\\&amp;=\sum_{i=1}^N\alpha_iy_iK(X_i,X)+b\end{aligned}$$</span> 所以说，我们并不需要知道 <spanclass="math inline"><em>W</em></span>的具体值，我们只需要有核函数，就能对样本进行分类。现在的关键问题是：<u><spanclass="math inline"><em>b</em></span> 是多少</u>？<spanclass="math inline"><em>b</em></span>的求解并不简单，需要用到优化理论中的 <strong>KKT 条件</strong>。</p><p>根据 KKT 条件，当原问题和对偶问题满足强对偶定理时，<spanclass="math inline">∀<em>i</em> = 1 ∼ <em>K</em></span>，要么 <spanclass="math inline"><em>β</em><sub><em>i</em></sub><sup>*</sup> = 0</span>，要么<spanclass="math inline"><em>h</em><sub><em>i</em></sub>(<em>ω</em><sup>*</sup>) = 0</span>；要么<spanclass="math inline"><em>α</em><sub><em>i</em></sub><sup>*</sup> = 0</span>，要么<spanclass="math inline"><em>g</em><sub><em>i</em></sub>(<em>ω</em><sup>*</sup>) = 0</span>，而在这个问题中，<spanclass="math inline"><em>g</em>(<em>W</em>) = 1 + <em>ξ</em><sub><em>i</em></sub> − <em>y</em><sub><em>i</em></sub>[<em>W</em><sup><em>T</em></sup><em>ϕ</em>(<em>X</em><sub><em>i</em></sub>)+<em>b</em>]</span>，所以，要么<spanclass="math inline"><em>α</em><sub><em>i</em></sub> = 0</span>，要么<spanclass="math inline"><em>g</em>(<em>W</em>) = 1 + <em>ξ</em><sub><em>i</em></sub> − <em>y</em><sub><em>i</em></sub>[<em>W</em><sup><em>T</em></sup><em>ϕ</em>(<em>X</em><sub><em>i</em></sub>)+<em>b</em>] = 0</span>.现在，我们取一个 <spanclass="math inline"><em>α</em><sub><em>i</em></sub></span>，使之 <spanclass="math inline">0 &lt; <em>α</em><sub><em>i</em></sub> &lt; <em>C</em></span>（这是肯定能满足的，原因见限制条件），则根据KKT 条件，肯定有 <spanclass="math inline">1 + <em>ξ</em><sub><em>i</em></sub> − <em>y</em><sub><em>i</em></sub>[<em>W</em><sup><em>T</em></sup><em>ϕ</em>(<em>X</em><sub><em>i</em></sub>)+<em>b</em>] = 0</span>。又因为<spanclass="math inline"><em>β</em><sub><em>i</em></sub> + <em>α</em><sub><em>i</em></sub> = <em>C</em></span>，根据KKT 条件，所以 <spanclass="math inline"><em>β</em><sub><em>i</em></sub> ≠ 0</span>，<spanclass="math inline"><em>h</em>(<em>W</em>) = <em>ξ</em><sub><em>i</em></sub> = 0</span>.将 <span class="math inline"><em>ξ</em><sub><em>i</em></sub> = 0</span>代入前式，就有： <span class="math display">$$\begin{aligned}&amp;1-y_i[W^T\phi(X_i)+b]=0\\&amp;\Downarrow\text{to rearrange the terms}\\&amp;b=\frac{1-y_iW^T\phi(X_i)}{y_i}\\&amp;\Downarrow\text{to substitute}W^T\phi(X)=\sum_{i=1}^N\alpha_iy_iK(X_i,X)\text{ into it}\\&amp;b=\frac{1-y_i\sum_{i=1}^N\alpha_iy_iK(X_i,X)}{y_i}\end{aligned}$$</span> 于是，就连 <span class="math inline"><em>b</em></span>我们也知道了。在现实中，我们一般会遍历所有 <spanclass="math inline"><em>α</em><sub><em>i</em></sub> ∉ {0, <em>C</em>}</span>（在上面的讨论中我们只取了一个<span class="math inline"><em>α</em></span>），然后计算 <spanclass="math inline"><em>b</em></span>，最后取 <spanclass="math inline"><em>b</em></span>的平均值，这样能使结果更加精确。</p><h3 id="算法流程总结">2.5 算法流程总结</h3><h4 id="训练流程">训练流程</h4><ol type="1"><li>输入：<spanclass="math inline">{(<em>X</em><sub><em>i</em></sub>,<em>y</em><sub><em>i</em></sub>)}  <em>i</em> = 1 ∼ <em>N</em></span></li><li>求解优化问题（SMO 算法）：<ol type="1"><li>最大化 <spanclass="math inline">$\theta(\alpha)=\sum_{i=1}^{N}\alpha_i-\frac12\sum_{i=1}^N\sum_{j=1}^N\alpha_i\alpha_jy_iy_jK(X_i,X_j)$</span></li><li>限制条件：<ol type="1"><li><spanclass="math inline">0 ≤ <em>α</em><sub><em>i</em></sub> ≤ <em>C</em>  (<em>i</em>=1∼<em>N</em>)</span></li><li><span class="math inline">$\sum_{i=1}^N\alpha_iy_i=0$</span></li></ol></li></ol></li><li>通过上一步计算出来的 <spanclass="math inline"><em>α</em><sub><em>i</em></sub></span> 来计算 <spanclass="math inline"><em>b</em></span>：<spanclass="math inline">$b=\frac{1-y_i\sum_{i=1}^N\alpha_iy_iK(X_i,X)}{y_i}$</span></li></ol><h4 id="测试流程">测试流程</h4><ol type="1"><li>输入测试样本 <span class="math inline"><em>X</em></span></li><li>分类：<ol type="1"><li>若 <spanclass="math inline">$\sum_{i=1}^N\alpha_iy_iK(X_i,X)+b\ge0$</span>，则<span class="math inline"><em>y</em> =  + 1</span></li><li>若 <spanclass="math inline">$\sum_{i=1}^N\alpha_iy_iK(X_i,X)+b\lt0$</span>，则<span class="math inline"><em>y</em> =  − 1</span></li></ol></li></ol><blockquote><p>可以发现，最终训练流程和测试流程中完全不需要用到无限维的 <spanclass="math inline"><em>ϕ</em>(<em>X</em>)</span>，只需要使用核函数就行了。这也就是SVM 用有限维手段来处理无限维问题的方法。</p></blockquote><h2 id="补充优化理论">03* 补充：优化理论</h2><p>在优化领域中，在优化理论中，<strong>原问题（PrimeProblem）</strong>和<strong>对偶问题（DualProblem）</strong>是一对相关的数学问题。</p><p>原问题的定义如下：</p><ol type="1"><li>目标：最小化 <spanclass="math inline"><em>f</em>(<em>ω</em>)</span></li><li>限制条件：<ol type="1"><li><spanclass="math inline"><em>g</em><sub><em>i</em></sub>(<em>ω</em>) ≤ 0  (<em>i</em>=1∼<em>K</em>)</span></li><li><spanclass="math inline"><em>h</em><sub><em>i</em></sub>(<em>ω</em>) = 0  (<em>i</em>=1∼<em>M</em>)</span></li></ol></li></ol><p>原问题是非常普适化的，虽然上面展示的是最小化问题，但只需要在 <spanclass="math inline"><em>f</em>(<em>ω</em>)</span>前加一个负号，立马就变成了最大化问题；同样地，在 <spanclass="math inline"><em>g</em><sub><em>i</em></sub>(<em>ω</em>) ≤ 0</span>中加一个负号，也就变成了 <spanclass="math inline"> − <em>g</em><sub><em>i</em></sub>(<em>ω</em>) ≥ 0</span>；而在式2 的左边减去一个常数 <spanclass="math inline"><em>C</em></span>，就立马变成了 <spanclass="math inline"><em>h</em><sub><em>i</em></sub>(<em>ω</em>) − <em>C</em> = 0</span>，这样就可以把等式右边的0 变成任意常数 <span class="math inline"><em>C</em></span>。</p><p>对偶问题是从原问题派生出来的一个新问题，对偶问题首先定义了一个函数：<span class="math display">$$\begin{aligned}L(\omega,\alpha,\beta)&amp;=f(\omega)+\sum_{i=1}^{K}\alpha_ig_i(\omega)+\sum_{i=1}^M\beta_ih_i(\omega)\quad&amp;①\\&amp;=f(\omega)+\alpha^Tg(\omega)+\beta^Th(\omega)\quad&amp;②\end{aligned}$$</span> 上式中，<span class="math inline"><em>α</em></span> 和 <spanclass="math inline"><em>β</em></span> 是两个和 <spanclass="math inline"><em>ω</em></span>维数一样的向量，并且分别乘上了不等式的限制条件和等式的限制条件。式 ①是该式的代数形式，式 ②是该式的矩阵形式。有了这个函数，我们就可以给出对偶问题的定义了：</p><ol type="1"><li>目标：最大化 <spanclass="math inline">$\theta(\alpha,\beta)=\underset{\omega}{\inf}\{L(\omega,\alpha,\beta)\}$</span></li><li>限制条件：<spanclass="math inline"><em>α</em><sub><em>i</em></sub> ≥ 0  (<em>i</em>=1∼<em>K</em>)</span></li></ol><p>解释一下这里的目标函数，<span class="math inline">inf </span>就是求最小值的意思，下面的 <span class="math inline"><em>ω</em></span>是指，遍历所有每个 <span class="math inline"><em>ω</em></span> 对应的<spanclass="math inline"><em>L</em>(<em>ω</em>,<em>α</em>,<em>β</em>)</span>，所以 <spanclass="math inline">$\underset{\omega}{\inf}\{L(\omega,\alpha,\beta)\}$</span>就是指，求所有 <span class="math inline"><em>ω</em></span> 对应的 <spanclass="math inline"><em>L</em>(<em>ω</em>,<em>α</em>,<em>β</em>)</span>中，<spanclass="math inline"><em>L</em>(<em>ω</em>,<em>α</em>,<em>β</em>)</span>最小的取值。而通过 <spanclass="math inline"><em>θ</em>(<em>α</em>,<em>β</em>)</span>可以看出，<span class="math inline"><em>α</em></span> 和 <spanclass="math inline"><em>β</em></span> 是固定的，也就是说，我们每确定一组<span class="math inline"><em>α</em></span> 和 <spanclass="math inline"><em>β</em></span>，就去求一次 <spanclass="math inline"><em>L</em>(<em>ω</em>,<em>α</em>,<em>β</em>)</span>的最小值，所以 <span class="math inline"><em>θ</em></span> 是只和 <spanclass="math inline"><em>α</em></span> 和 <spanclass="math inline"><em>β</em></span> 有关的函数。而我们的目标又是最大化<spanclass="math inline"><em>θ</em>(<em>α</em>,<em>β</em>)</span>，所以，实质上我们就是要使<spanclass="math inline"><em>L</em>(<em>ω</em>,<em>α</em>,<em>β</em>)</span>的最小值最大化。而对偶问题的限制条件很简单，只要求每个 <spanclass="math inline"><em>α</em><sub><em>i</em></sub></span> 大于 0即可。</p><p>接下来我们就来介绍一下原问题和对偶问题的关系，有一条定理是这样的：</p><blockquote><p>如果 <span class="math inline"><em>ω</em><sup>*</sup></span>是原问题的解，而 <span class="math inline"><em>α</em><sup>*</sup></span>和 <span class="math inline"><em>β</em><sup>*</sup></span>是对偶问题的解，则有 <spanclass="math inline"><em>f</em>(<em>ω</em><sup>*</sup>) ≥ <em>θ</em>(<em>α</em><sup>*</sup>,<em>β</em><sup>*</sup>)</span>。</p></blockquote><p>这条定理的证明如下：</p><p>由于 <span class="math inline"><em>α</em><sup>*</sup></span> 和 <spanclass="math inline"><em>β</em><sup>*</sup></span>是对偶问题的解，则下式肯定成立： <span class="math display">$$\theta(\alpha^*,\beta^*)=\underset{\omega}{\inf}\{L(\omega,\alpha^*,\beta^*\}\leL(\omega^*,\alpha^*,\beta^*)$$</span> 这里的 <span class="math inline"><em>ω</em><sup>*</sup></span>是指一个具体的 <span class="math inline"><em>ω</em></span> 的值。根据<spanclass="math inline"><em>L</em>(<em>ω</em>,<em>α</em>,<em>β</em>)</span>的定义，展开不等式右边的式子有： <span class="math display">$$L(\omega^*,\alpha^*,\beta^*)=f(\omega^*)+\sum_{i=1}^{K}\alpha_i^*g_i(\omega^*)+\sum_{i=1}^M\beta_i^*h_i(\omega^*)$$</span> 既然 <span class="math inline"><em>ω</em><sup>*</sup></span>是原问题的解，那么 <spanclass="math inline"><em>ω</em><sup>*</sup></span>必然满足原问题的两个限制条件，也就是说 <spanclass="math inline"><em>g</em><sub><em>i</em></sub>(<em>ω</em><sup>*</sup>) ≤ 0</span>，<spanclass="math inline"><em>h</em><sub><em>i</em></sub>(<em>ω</em><sup>*</sup>) = 0</span>；另外，既然<span class="math inline"><em>α</em><sup>*</sup></span>是对偶问题的解，那么 <spanclass="math inline"><em>α</em><sup>*</sup></span> 也必然满足 <spanclass="math inline"><em>α</em><sup>*</sup> ≥ 0</span>。进一步，既然<spanclass="math inline"><em>h</em><sub><em>i</em></sub>(<em>ω</em><sup>*</sup>) = 0</span>，那么上式中<spanclass="math inline">$\sum_{i=1}^M\beta_i^*h_i(\omega^*)=0$</span>；既然<spanclass="math inline"><em>g</em><sub><em>i</em></sub>(<em>ω</em><sup>*</sup>) ≤ 0</span>，<spanclass="math inline"><em>α</em><sup>*</sup> ≥ 0</span>，那么上式中 <spanclass="math inline">$\sum_{i=1}^{K}\alpha_i^*g_i(\omega^*)\le0$</span>，所以存在：<span class="math display">$$\theta(\alpha^*,\beta^*)=\underset{\omega}{\inf}\{L(\omega,\alpha^*,\beta^*\}\leL(\omega^*,\alpha^*,\beta^*)\le f(\omega^*)$$</span> 证毕。</p><p>遂定义： <spanclass="math display"><em>G</em> = <em>f</em>(<em>ω</em><sup>*</sup>) − <em>θ</em>(<em>α</em><sup>*</sup>,<em>β</em><sup>*</sup>) ≥ 0</span><span class="math inline"><em>G</em></span>叫做原问题与对偶问题的<strong>间距（DualityGap）</strong>。对应某些特定的优化问题，可以证明 <spanclass="math inline"><em>G</em> = 0</span>.这里不再证明，直接给出问题的结论——<strong>强对偶定理</strong>：</p><blockquote><p>若 <span class="math inline"><em>f</em>(<em>ω</em>)</span>为凸函数，且 <spanclass="math inline"><em>g</em>(<em>ω</em>) = <em>A</em><em>ω</em> + <em>b</em></span>（线性函数），<spanclass="math inline"><em>h</em>(<em>ω</em>) = <em>C</em><em>W</em> + <em>d</em></span>（一组线性函数），则此优化问题的原问题和对偶问题的间距是0，即 <spanclass="math inline"><em>f</em>(<em>ω</em><sup>*</sup>) = <em>θ</em>(<em>α</em><sup>*</sup>,<em>β</em><sup>*</sup>)</span>。</p></blockquote><p>问题是，强对偶定理的前提成立意味着什么？假设现在原问题和对偶问题满足强对偶定理，即<spanclass="math inline"><em>f</em>(<em>ω</em><sup>*</sup>) = <em>θ</em>(<em>α</em><sup>*</sup>,<em>β</em><sup>*</sup>)</span>成立，那么就有 <spanclass="math inline">$f(\omega^*)=\theta(\alpha^*,\beta^*)=\underset{\omega}{\inf}\{L(\omega,\alpha^*,\beta^*\}$</span>，也就是说，<u>原问题的解<spanclass="math inline"><em>ω</em><sup>*</sup></span>，刚刚就是对偶问题在<span class="math inline"><em>α</em><sup>*</sup></span> 和 <spanclass="math inline"><em>β</em><sup>*</sup></span>确定时，取到最小值的那个点</u>。</p><p>更加精妙的是，当 <span class="math inline"><em>G</em> = 0</span>成立时，<spanclass="math inline">$\sum_{i=1}^{K}\alpha_i^*g_i(\omega^*)+\sum_{i=1}^M\beta_i^*h_i(\omega^*)=0$</span>，其中，<spanclass="math inline">$\sum_{i=1}^M\beta_i^*h_i(\omega^*)$</span> 等于 0不用再说了，但 <spanclass="math inline">$\sum_{i=1}^{K}\alpha_i^*g_i(\omega^*)=0$</span>意味着，<u><spanclass="math inline">∀<em>i</em> = 1 ∼ <em>K</em></span>，要么 <spanclass="math inline"><em>α</em><sub><em>i</em></sub><sup>*</sup> = 0</span>，要么<spanclass="math inline"><em>g</em><sub><em>i</em></sub>(<em>ω</em><sup>*</sup>) = 0</span></u>。这个条件叫做<strong>KKT 条件</strong>。</p>]]></content>
      
      
      <categories>
          
          <category> machine learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> technique </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>A-star搜索</title>
      <link href="/2023/12/06/A-star%E6%90%9C%E7%B4%A2/"/>
      <url>/2023/12/06/A-star%E6%90%9C%E7%B4%A2/</url>
      
        <content type="html"><![CDATA[<h2 id="启发式搜索">01 启发式搜索</h2><p>能有助于简化搜索过程的信息称为启发信息，利用启发信息的搜索过程称为启发式搜索。</p><p>求解问题中能利用的大多是非完备的启发信息，所谓非完备就是指，信息也许对搜索有正面影响的，但是我们无法得知它是否总能提供正面影响，不知道它是否会造成负面影响。就例如极值点导数为0，这是一条完备的信息，因为它可被证明总是成立。造成这种结果的原因如下：</p><ol type="1"><li>求解问题系统不可能知道与实际问题有关的全部信息，因而无法知道该问题的全部状态空间，也不可能用一套算法来求解所以问题；</li><li>有些问题在理论上虽然存在着求解算法，但是在工程实践中，这些算法不是效率太低，就是根本无法实现(就比如宽度优先搜索，它总能找到最优解，但是无法实现)。</li></ol><p>启发式搜索在搜索过程中根据启发信息评估各个节点的重要性，优先搜索重要的节点。<strong>估价函数</strong>的任务就是估计待搜索节点“有希望”的程度。估价函数<span class="math inline"><em>f</em><sub><em>n</em></sub></span>表示从初始节点经过 <span class="math inline"><em>n</em></span>节点到达目的节点的路径的最小代价估计值，其一般形式为： <spanclass="math display"><em>f</em>(<em>n</em>) = <em>g</em>(<em>n</em>) + <em>h</em>(<em>n</em>)</span><span id="more"></span></p><p>其中，<span class="math inline"><em>g</em>(<em>n</em>)</span>是从初始节点到结点 <span class="math inline"><em>n</em></span>的<strong>实际代价</strong>，<spanclass="math inline"><em>h</em>(<em>n</em>)</span> 是从节点 <spanclass="math inline"><em>n</em></span>到目的节点的最佳路径的<strong>估计代价</strong>。一般地，在 <u><spanclass="math inline"><em>f</em>(<em>n</em>)</span> 中，<spanclass="math inline"><em>g</em>(<em>n</em>)</span>的比重越大，越倾向于宽度优先搜索方式，而 <spanclass="math inline"><em>h</em>(<em>n</em>)</span>的比重越大，表示启发性能更强</u>。如果 <spanclass="math inline"><em>h</em>(<em>n</em>)</span> 的比重降为0，则搜索过程将变为盲目搜索，因为不再考虑启发信息。</p><p>估价函数的设计方法有很多种，并且不同的估价函数对问题有不同的影响。以八数码问题为例，最简单的估价函数可以取一格局与目的格局相比，其位置不同的数码数目；这种估价函数是最简单实现的，但是效果未必好，一种比较好的估价函数的设计是取各数码移到目的位置所需移动的距离的总和，这是最理想的，但是不可能实现；还可以将每一对逆转数码<ahref="#fn1" class="footnote-ref" id="fnref1"role="doc-noteref"><sup>1</sup></a>乘以一个倍数3；但是这种做法的局限性太大，所以还可以在此基础上再加上位置不符的数码的个数。</p><h2 id="a-搜索算法">02 A 搜索算法</h2><p>启发式图搜索法的基本特点：寻找并设计一个与问题有关的 <spanclass="math inline"><em>h</em>(<em>n</em>)</span> 以构造 <spanclass="math inline"><em>f</em>(<em>n</em>) = <em>g</em>(<em>n</em>) + <em>h</em>(<em>n</em>)</span>，然后以<span class="math inline"><em>f</em>(<em>n</em>)</span>的大小来排列待扩展状态的次序，每次选择 <spanclass="math inline"><em>f</em>(<em>n</em>)</span>值<strong>最小者</strong>进行扩展。这也就是 A 搜索算法的执行流程。</p><p>利用 A 搜索算法求解八数码问题，估价函数定义为： <spanclass="math display"><em>f</em>(<em>n</em>) = <em>d</em>(<em>n</em>) + <em>w</em>(<em>n</em>)</span>其中，<span class="math inline"><em>d</em>(<em>n</em>)</span>代表状态的深度，每步为单位代价；<spanclass="math inline"><em>w</em>(<em>n</em>)</span>以与目标格局不符的数码数量作为启发信息的度量。例如：</p><p><img src="/img/A-star搜索-01.png" /></p><p>初始格局处于第 0 层，因此 <spanclass="math inline"><em>d</em>(<em>S</em>) = 0</span>，其中，<spanclass="math inline">*</span> 代表空格，计算 <spanclass="math inline"><em>w</em>(<em>n</em>)</span> 的时候，既可以算上<spanclass="math inline">*</span>，也可以不算，反正不影响节点扩展，如果不算入，那么<span class="math inline"><em>w</em>(<em>S</em>) = 4</span>(算入的话结果为 5)。由初始格局可得到 3种状态，即分别把空格往上、左、右移动，上图中只展示了 3 种情况。其中，A格局的 <spanclass="math inline"><em>d</em>(<em>A</em>) = 1</span>，<spanclass="math inline"><em>w</em>(<em>A</em>) = 5</span>，所以 <spanclass="math inline"><em>f</em>(<em>A</em>) = 6</span>；B 格局的 <spanclass="math inline"><em>d</em>(<em>B</em>) = 1</span>，<spanclass="math inline"><em>w</em>(<em>B</em>) = 3</span>，所以 <spanclass="math inline"><em>f</em>(<em>B</em>) = 4</span>；C 格局的 <spanclass="math inline"><em>d</em>(<em>C</em>) = 1</span>，<spanclass="math inline"><em>w</em>(<em>C</em>) = 5</span>，所以 <spanclass="math inline"><em>f</em>(<em>C</em>) = 6</span>。根据 A搜索算法的原则，估价函数值最小的是 B格局，因此应该扩展该节点，接下来的过程也是一样。最终经过 5次搜索，也就是扩展 5层节点，最终能到达目标格局，该过程的完整搜索树在课本 P143页，这里就不再展开了。</p><p>那么 A搜索算法能否保证找到最优解？其实仔细想想就知道，我们构造的估价函数中，<spanclass="math inline"><em>h</em>(<em>n</em>)</span>是对待扩展节点到目标节点的代价估计，不一定准确，所以 A搜索算法不一定能保证找到最优解。</p><blockquote><p>如果有代价一样的结点怎么办：可以随机。</p></blockquote><h2 id="a-搜索算法-1">03 A* 搜索算法</h2><p>A* 搜索算法是对 A 搜索算法的改进。我们说 A搜索算法无法保证找到最优解，而 A*搜索算法则保证了一定能搜索到解，并且一定能搜索到最优解。A* 算法给出了 A算法能得到最优解的条件，我们令 <spanclass="math inline"><em>h</em><sup>*</sup>(<em>n</em>)</span> 为状态<span class="math inline"><em>n</em></span>到目标状态的实际最小代价，<spanclass="math inline"><em>h</em>(<em>n</em>)</span>是我们定义的估计代价，则当 <spanclass="math inline">∀<em>n</em></span>，<spanclass="math inline"><em>h</em>(<em>n</em>) ≤ <em>h</em><sup>*</sup>(<em>n</em>)</span>时，我们就称该搜索算法为 A* 搜索算法。</p><p>这就好比是有人托我们帮他买衣服，这种品牌的衣服的最低价格是 1000元(相当于 <spanclass="math inline"><em>h</em><sup>*</sup></span>)，如果他希望以不高于1000 (相当于 <spanclass="math inline"><em>h</em></span>)的价格买到这件衣服(此时 <spanclass="math inline"><em>h</em> ≤ <em>h</em><sup>*</sup></span>)，那我们就要搜索很多家店(前提是这家店要存在)，这种情况下，虽然搜索的店较多，但是我们必然能找到一家店能满足要求；但是倘若他希望以1500 元以内(相当于 <spanclass="math inline"><em>h</em></span>)的价格买到这件衣服(此时 <spanclass="math inline"><em>h</em> &gt; <em>h</em><sup>*</sup></span>)，那搜索的范围就大大减少了，因为最低价是1000，那么价格高于 1000的店找起来肯定没那么费力气，但缺点就是找到的店铺未必是最便宜的。</p><p>在上面我们讨论的八数码问题中，我们选取一格局与目标格局不符的数码数量作为启发信息的度量<spanclass="math inline"><em>w</em>(<em>n</em>)</span>，而八数码问题中的<span class="math inline"><em>h</em><sup>*</sup>(<em>n</em>)</span>应该是各数码移到目的位置所需移动的距离的总和，显然 <spanclass="math inline"><em>w</em>(<em>n</em>) ≤ <em>h</em><sup>*</sup>(<em>n</em>)</span>，满足了<spanclass="math inline"><em>h</em>(<em>n</em>) ≤ <em>h</em><sup>*</sup>(<em>n</em>)</span>的条件，所以也算一种 A* 搜索算法。</p><section id="footnotes" class="footnotes footnotes-end-of-document"role="doc-endnotes"><hr /><ol><li id="fn1"><p>逆转数码的概念涉及到逆序数。例如 1 4 23，在这个序列中，它并未按照从小到大的顺序排序，2 和 3 比 4小，但是却排在 4 后面，所以该序列的逆序数就是 2.逆序数可以用来判断一个八数码问题是否有解，至于原因就不在此赘述了。<ahref="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li></ol></section>]]></content>
      
      
      <categories>
          
          <category> AI </category>
          
      </categories>
      
      
        <tags>
            
            <tag> technique </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>鲁滨逊归结原理</title>
      <link href="/2023/12/06/%E9%B2%81%E6%BB%A8%E9%80%8A%E5%BD%92%E7%BB%93%E5%8E%9F%E7%90%86/"/>
      <url>/2023/12/06/%E9%B2%81%E6%BB%A8%E9%80%8A%E5%BD%92%E7%BB%93%E5%8E%9F%E7%90%86/</url>
      
        <content type="html"><![CDATA[<h2 id="归结推理">1 归结推理</h2><p>反证法：<spanclass="math inline"><em>P</em> ⇒ <em>Q</em></span>，当且仅当 <spanclass="math inline"><em>P</em> ∧ ¬<em>Q</em> ⇔ <em>F</em></span>，即<span class="math inline"><em>Q</em></span> 为 <spanclass="math inline"><em>P</em></span> 的逻辑结论，当且仅当 <spanclass="math inline"><em>P</em> ∧ ¬<em>Q</em></span> 是不可满足的。</p><p>定理：<span class="math inline"><em>Q</em></span> 为 <spanclass="math inline"><em>P</em><sub>1</sub></span>，<spanclass="math inline"><em>P</em><sub>2</sub></span>，……，<spanclass="math inline"><em>P</em><sub><em>n</em></sub></span>的逻辑结论，当且仅当 <spanclass="math inline">(<em>P</em><sub>1</sub>∧<em>P</em><sub>2</sub>∧…∧<em>P</em><sub><em>n</em></sub>) ∧ ¬<em>Q</em></span>是不可满足的。</p><p>归结推理就是基于上面两条定理，将原命题转换成反命题，然后证明其反命题是不可满足的，即可得证原命题是真命题。归结推理的整体思路是：</p><ol type="1"><li>欲证明 <span class="math inline"><em>P</em> ⇒ <em>Q</em></span></li><li>化为反命题 <spanclass="math inline"><em>P</em> ∧ ¬<em>Q</em></span></li><li>化成子句集</li><li>证明子句集不可满足(鲁滨逊归结原理)</li></ol><span id="more"></span><h2 id="子句集">2 子句集</h2><p>什么是子句？如何将谓词公式化为子句集？</p><p>我们称一个不能再分割的命题为<strong>原子谓词公式</strong>，将原子谓词公式及其否定形式称为<strong>文字</strong>，而<strong>子句</strong>就是任何文字的<u>析取式</u>，任何文字本身也是子句。<strong>空子句</strong>是一个不包含任何文字的子句，它永远为假，不可满足，通常表示为<spanclass="math inline"><em>N</em><em>I</em><em>L</em></span>，虽然听上去没什么用，但它却是归结推理中最重要的子句，之后你会知道为什么。以上就是子句的概念，而子句集就是由子句构成的集合。</p><p>以下面这道题为例讲解如何将一个谓词公式化为子句集： <spanclass="math display">(∀<em>x</em>)((∀<em>y</em>)<em>P</em>(<em>x</em>,<em>y</em>)→¬(∀<em>y</em>)(<em>Q</em>(<em>x</em>,<em>y</em>)→<em>R</em>(<em>x</em>,<em>y</em>)))</span>第一步：消去谓词公式中的 <span class="math inline">→</span> 和 <spanclass="math inline">↔︎</span>，得到： <spanclass="math display">(∀<em>x</em>)(¬(∀<em>y</em>)<em>P</em>(<em>x</em>,<em>y</em>)∨¬(∀<em>y</em>)(¬<em>Q</em>(<em>x</em>,<em>y</em>)∨<em>R</em>(<em>x</em>,<em>y</em>)))</span>第二步：将否定符号 <span class="math inline">¬</span>移到紧靠谓词的位置上： <spanclass="math display">(∀<em>x</em>)((∃<em>y</em>)¬<em>P</em>(<em>x</em>,<em>y</em>)∨(∃<em>y</em>)(<em>Q</em>(<em>x</em>,<em>y</em>)∧¬<em>R</em>(<em>x</em>,<em>y</em>)))</span>第三步：变量标准化，将重复的变量名换掉： <spanclass="math display">(∀<em>x</em>)((∃<em>y</em>)¬<em>P</em>(<em>x</em>,<em>y</em>)∨(∃<em>z</em>)(<em>Q</em>(<em>x</em>,<em>z</em>)∧¬<em>R</em>(<em>x</em>,<em>z</em>)))</span>第四步：消去存在量词，要用到 Skolem 函数，令 <spanclass="math inline"><em>y</em> = <em>f</em>(<em>x</em>)</span>，<spanclass="math inline"><em>z</em> = <em>g</em>(<em>x</em>)</span>： <spanclass="math display">(∀<em>x</em>)(¬<em>P</em>(<em>x</em>,<em>f</em>(<em>x</em>))∨(<em>Q</em>(<em>x</em>,<em>g</em>(<em>x</em>))∧¬<em>R</em>(<em>x</em>,<em>g</em>(<em>x</em>))))</span>第五步：化为前束形，即将所有的全称谓词提到公式最前面，使母式中不存在任何量词，上式已满足前束形。</p><p>第六步：化为 Skolem 标准形，即将母式化为合取式： <spanclass="math display">(∀<em>x</em>)((¬<em>P</em>(<em>x</em>,<em>f</em>(<em>x</em>))∨<em>Q</em>(<em>x</em>,<em>g</em>(<em>x</em>))∧(¬<em>P</em>(<em>x</em>,<em>f</em>(<em>x</em>))∨¬<em>R</em>(<em>x</em>,<em>g</em>(<em>x</em>))))</span>第七步：略去全称量词： <spanclass="math display">(¬<em>P</em>(<em>x</em>,<em>f</em>(<em>x</em>)) ∨ <em>Q</em>(<em>x</em>,<em>g</em>(<em>x</em>)) ∧ (¬<em>P</em>(<em>x</em>,<em>f</em>(<em>x</em>))∨¬<em>R</em>(<em>x</em>,<em>g</em>(<em>x</em>)))</span>第八步：把和取词看作分隔符，把整体化为集合： <spanclass="math display">{¬<em>P</em>(<em>x</em>,<em>f</em>(<em>x</em>)) ∨ <em>Q</em>(<em>x</em>,<em>g</em>(<em>x</em>)), ¬<em>P</em>(<em>x</em>,<em>f</em>(<em>x</em>)) ∨ ¬<em>R</em>(<em>x</em>,<em>g</em>(<em>x</em>))}</span></p><p>第九步：子句变量标准化，即将不同的子句中的变量名字区分开，用不同的符号表示：<spanclass="math display">{¬<em>P</em>(<em>x</em>,<em>f</em>(<em>x</em>)) ∨ <em>Q</em>(<em>x</em>,<em>g</em>(<em>x</em>)), ¬<em>P</em>(<em>y</em>,<em>f</em>(<em>y</em>)) ∨ ¬<em>R</em>(<em>y</em>,<em>g</em>(<em>y</em>))}</span>以上，就得到了谓词公式的子句集。</p><h2 id="鲁滨逊归结原理">3 鲁滨逊归结原理</h2><p>子句集中的各子句是合取关系，所以只要有一个不可满足，则整个子句集不可满足。所以，我们需要去找一个空子句，假如子句集中存在空子句，那就肯定不可满足。但是子句集中直接出现空子句的情况是很少的，那么如何找到空子句？这就是鲁滨逊归结原理要解决的问题，根据鲁滨逊归结原理对子句集进行归结，如果最终归结出一个空子句，则说明该子句集不可满足，进一步说明原命题不可满足。</p><p>鲁滨逊归结原理（也称消解原理）的基本思路是：检查子句集 <spanclass="math inline">S</span> 中是否包含空子句，若包含，则 <spanclass="math inline">S</span> 不可满足；若不包含，在 <spanclass="math inline">S</span>中选择合适的子句进行归结，一旦归结出空子句，就说明 <spanclass="math inline">S</span> 是不可满足的。</p><p>归结的定义：设 <spanclass="math inline"><em>C</em><sub>1</sub></span> 和 <spanclass="math inline"><em>C</em><sub>2</sub></span>是子句集中的任意两个子句，如果 <spanclass="math inline"><em>C</em><sub>1</sub></span> 中的文字 <spanclass="math inline"><em>L</em><sub>1</sub></span> 与 <spanclass="math inline"><em>C</em><sub>2</sub></span> 中的文字 <spanclass="math inline"><em>L</em><sub>2</sub></span> 互补，那么从 <spanclass="math inline"><em>C</em><sub>1</sub></span> 和 <spanclass="math inline"><em>C</em><sub>2</sub></span> 中分别消去 <spanclass="math inline"><em>L</em><sub>1</sub></span> 和 <spanclass="math inline"><em>L</em><sub>2</sub></span>，并将两个子句中余下的部分<strong>析取</strong>，构成一个新子句<span class="math inline"><em>C</em><sub>12</sub></span>。</p><hr /><p>例题：设 <spanclass="math inline"><em>C</em><sub>1</sub> = ¬<em>P</em> ∨ <em>Q</em></span>，<spanclass="math inline"><em>C</em><sub>2</sub> = ¬<em>Q</em> ∨ <em>R</em></span>，<spanclass="math inline"><em>C</em><sub>3</sub> = <em>P</em></span>，请对<spanclass="math inline">{<em>C</em><sub>1</sub>, <em>C</em><sub>2</sub>, <em>C</em><sub>3</sub>}</span>进行归结。</p><p><span class="math inline"><em>C</em><sub>1</sub></span> 和 <spanclass="math inline"><em>C</em><sub>2</sub></span> 中存在互补子句 <spanclass="math inline"><em>Q</em></span> 和 <spanclass="math inline">¬<em>Q</em></span>，所以消去这两个子句集，并将余下子句析取，得到<spanclass="math inline"><em>C</em><sub>12</sub> = ¬<em>P</em> ∨ <em>R</em></span>；<spanclass="math inline"><em>C</em><sub>12</sub></span> 和 <spanclass="math inline"><em>C</em><sub>3</sub></span> 中存在互补子句 <spanclass="math inline">¬<em>P</em></span> 和 <spanclass="math inline"><em>P</em></span>，所以消去这两个子句集，并将余下子句析取，得到<spanclass="math inline"><em>C</em><sub>123</sub> = <em>R</em></span>。所以<span class="math inline"><em>C</em><sub>123</sub></span>就是该子句集归结的结果。</p><hr /><p>定理：归结式 <span class="math inline"><em>C</em><sub>12</sub></span>是其亲本子句 <span class="math inline"><em>C</em><sub>1</sub></span> 和<span class="math inline"><em>C</em><sub>2</sub></span>的逻辑结论，即，如果 <spanclass="math inline"><em>C</em><sub>1</sub></span> 和 <spanclass="math inline"><em>C</em><sub>2</sub></span> 为真，则 <spanclass="math inline"><em>C</em><sub>12</sub></span> 也为真。</p><p>上述定理有一条推论：设 <spanclass="math inline"><em>C</em><sub>1</sub></span> 和 <spanclass="math inline"><em>C</em><sub>2</sub></span> 是子句集 <spanclass="math inline">$\text S$</span> 中的两个子句集，<spanclass="math inline"><em>C</em><sub>12</sub></span> 是它们的归结式，若用<span class="math inline"><em>C</em><sub>12</sub></span> 代替 <spanclass="math inline"><em>C</em><sub>1</sub></span> 和 <spanclass="math inline"><em>C</em><sub>2</sub></span> 后得到新子句集 <spanclass="math inline">S<sub>1</sub></span>，则由 <spanclass="math inline">S<sub>1</sub></span> 不可满足性可推出 <spanclass="math inline">S</span> 的不可满足性。但是注意，这条推论不能证明若<span class="math inline">S</span> 是不可满足的，则 <spanclass="math inline">S<sub>1</sub></span>也不可满足，所以还有另一条推论：设 <spanclass="math inline"><em>C</em><sub>1</sub></span> 和 <spanclass="math inline"><em>C</em><sub>2</sub></span> 是子句集 <spanclass="math inline">$\text S$</span> 中的两个子句集，<spanclass="math inline"><em>C</em><sub>12</sub></span> 是它们的归结式，若<span class="math inline"><em>C</em><sub>12</sub></span> 加入原子句集<span class="math inline">S</span>，得到新子句集 <spanclass="math inline">S<sub>2</sub></span>，则 <spanclass="math inline">$\text S$</span> 和 <span class="math inline">$\textS_2$</span> 在不可满足性上是<u>等价</u>的，即若 <spanclass="math inline">S</span> 是不可满足的，则 <spanclass="math inline">S<sub>2</sub></span>也不可满足，反之亦然。不过我们的目的只是为了证明原子句集不可满足，也就是归结出一个空子句，所以上述两个推论均可用。</p><h2 id="归结反演">4 归结反演</h2><p>应用鲁滨逊归结原理证明定理的过程称为<strong>归结反演</strong>。它总共分为以下四个步骤：</p><ol type="1"><li>将已知前提表示为谓词公式 <spanclass="math inline"><em>F</em></span>；</li><li>将待证明的结论表示为谓词公式 <spanclass="math inline"><em>Q</em></span>，并否定得到 <spanclass="math inline">¬<em>Q</em></span>；</li><li>把谓词公式集 <spanclass="math inline">{<em>F</em>, ¬<em>Q</em>}</span> 化为子句集 <spanclass="math inline">$\text S$</span>；</li><li>应用归结原理对子句集 <span class="math inline">$\text S$</span>中的子句进行归结，并把每次归结得到的归结式都并入到 <spanclass="math inline">$\text S$</span>中。如此反复进行，若出现了空子句，则停止归结，此时就证明了 <spanclass="math inline"><em>Q</em></span> 为真。</li></ol><hr /><p>例题：某公司招聘工作人员，A，B，C三人面试，经面试后公司表示如下想法：</p><ul><li>三人中至少录取一人；</li><li>如果录取 A 而不录取 B，则一定录取 C；</li><li>如果录取 B，则一定录取 C。</li></ul><p>求证：公司一定录取 C。</p><p>解：第一步，将已知前提表示为谓词公式，先定义谓词：设 <spanclass="math inline"><em>P</em>(<em>x</em>)</span> 表示录取 <spanclass="math inline"><em>x</em></span>。于是可得如下前提：</p><ul><li><spanclass="math inline"><em>P</em>(<em>A</em>) ∨ <em>P</em>(<em>B</em>) ∨ <em>P</em>(<em>C</em>)</span></li><li><spanclass="math inline"><em>P</em>(<em>A</em>) ∧ ¬<em>P</em>(<em>B</em>) → <em>P</em>(<em>C</em>)</span></li><li><spanclass="math inline"><em>P</em>(<em>B</em>) → <em>P</em>(<em>C</em>)</span></li></ul><p>第二步，将待证明的结论表示为谓词公式，并将其否定：<spanclass="math inline">¬<em>P</em>(<em>C</em>)</span>。</p><p>第三步，将上述谓词公式化为子句集：</p><ol type="1"><li><spanclass="math inline"><em>P</em>(<em>A</em>) ∨ <em>P</em>(<em>B</em>) ∨ <em>P</em>(<em>C</em>)</span></li><li><spanclass="math inline">¬<em>P</em>(<em>A</em>) ∨ <em>P</em>(<em>B</em>) ∨ <em>P</em>(<em>C</em>)</span></li><li><spanclass="math inline">¬<em>P</em>(<em>B</em>) ∨ <em>P</em>(<em>C</em>)</span></li><li><span class="math inline">¬<em>P</em>(<em>C</em>)</span></li></ol><p>第四步，应用归结原理进行归结：</p><ol start="5" type="1"><li><spanclass="math inline"><em>P</em>(<em>B</em>) ∨ <em>P</em>(<em>C</em>)  <em>归</em><em>结</em>(1)<em>和</em>(2)</span></li><li><spanclass="math inline"><em>P</em>(<em>C</em>)          <em>归</em><em>结</em>(3)<em>和</em>(5)</span></li><li><spanclass="math inline"><em>N</em><em>I</em><em>L</em>           <em>归</em><em>结</em>(4)<em>和</em>(6)</span></li></ol><p>由于归结出了空子句，所以成功证明了 <spanclass="math inline">¬<em>P</em>(<em>C</em>)</span> 为假，因此原命题<span class="math inline"><em>P</em>(<em>C</em>)</span>为真，公司一定录取 C。</p><hr /><p>例题：已知：</p><ul><li>任何人的兄弟不是女性；</li><li>任何人的姐妹必是女性；</li><li>Mary 是 Bill 的姐妹。</li></ul><p>求证：Mary 不是 Tom 的兄弟。</p><p>解：第一步，将已知前提表示为谓词公式，先定义谓词：设 <spanclass="math inline"><em>b</em><em>r</em><em>o</em><em>t</em><em>h</em><em>e</em><em>r</em>(<em>x</em>,<em>y</em>)</span>表示录取 <span class="math inline"><em>x</em></span> 是 <spanclass="math inline"><em>y</em></span> 的兄弟，设 <spanclass="math inline"><em>s</em><em>i</em><em>s</em><em>t</em><em>e</em><em>r</em>(<em>x</em>,<em>y</em>)</span>表示录取 <span class="math inline"><em>x</em></span> 是 <spanclass="math inline"><em>y</em></span> 的姐妹，设 <spanclass="math inline"><em>w</em><em>o</em><em>m</em><em>a</em><em>n</em>(<em>x</em>)</span>表示录取 <span class="math inline"><em>x</em></span>是女性。于是可得如下前提：</p><ul><li><spanclass="math inline">(∀<em>x</em>)(∀<em>y</em>)(<em>b</em><em>r</em><em>o</em><em>t</em><em>h</em><em>e</em><em>r</em>(<em>x</em>,<em>y</em>)→¬<em>w</em><em>o</em><em>m</em><em>a</em><em>n</em>(<em>x</em>))</span></li><li><spanclass="math inline">(∀<em>x</em>)(∀<em>y</em>)(<em>s</em><em>i</em><em>s</em><em>t</em><em>e</em><em>r</em>(<em>x</em>,<em>y</em>)→<em>w</em><em>o</em><em>m</em><em>a</em><em>n</em>(<em>x</em>))</span></li><li><spanclass="math inline"><em>s</em><em>i</em><em>s</em><em>t</em><em>e</em><em>r</em>(<em>M</em><em>a</em><em>r</em><em>y</em>,<em>B</em><em>i</em><em>l</em><em>l</em>)</span></li></ul><p>第二步，将待证明的结论表示为谓词公式，并将其否定：<spanclass="math inline"><em>b</em><em>r</em><em>o</em><em>t</em><em>h</em><em>e</em><em>r</em>(<em>M</em><em>a</em><em>r</em><em>y</em>,<em>T</em><em>o</em><em>m</em>)</span>。</p><p>第三步，将上述谓词公式化为子句集：</p><ol type="1"><li><spanclass="math inline"><em>C</em><sub>1</sub> = ¬<em>b</em><em>r</em><em>o</em><em>t</em><em>h</em><em>e</em><em>r</em>(<em>x</em>,<em>y</em>) ∨ ¬<em>w</em><em>o</em><em>m</em><em>a</em><em>n</em>(<em>x</em>)</span></li><li><spanclass="math inline"><em>C</em><sub>2</sub> = ¬<em>s</em><em>i</em><em>s</em><em>t</em><em>e</em><em>r</em>(<em>x</em>,<em>y</em>) ∨ <em>w</em><em>o</em><em>m</em><em>a</em><em>n</em>(<em>x</em>)</span></li><li><spanclass="math inline"><em>C</em><sub>3</sub> = <em>s</em><em>i</em><em>s</em><em>t</em><em>e</em><em>r</em>(<em>M</em><em>a</em><em>r</em><em>y</em>,<em>B</em><em>i</em><em>l</em><em>l</em>)</span></li><li><spanclass="math inline"><em>C</em><sub>4</sub> = <em>b</em><em>r</em><em>o</em><em>t</em><em>h</em><em>e</em><em>r</em>(<em>M</em><em>a</em><em>r</em><em>y</em>,<em>T</em><em>o</em><em>m</em>)</span></li></ol><p>第四步，应用归结原理进行归结：</p><ol type="1"><li><spanclass="math inline"><em>C</em><sub>23</sub> = <em>w</em><em>o</em><em>m</em><em>a</em><em>n</em>(<em>M</em><em>a</em><em>r</em><em>y</em>)</span></li><li><spanclass="math inline"><em>C</em><sub>123</sub> = ¬<em>b</em><em>o</em><em>r</em><em>t</em><em>h</em><em>e</em><em>r</em>(<em>M</em><em>a</em><em>r</em><em>y</em>,<em>y</em>)</span></li><li><spanclass="math inline"><em>C</em><sub>1234</sub> = <em>N</em><em>I</em><em>L</em></span></li></ol><p>由于归结出了空子句，所以成功证明了 <spanclass="math inline"><em>b</em><em>r</em><em>o</em><em>t</em><em>h</em><em>e</em><em>r</em>(<em>M</em><em>a</em><em>r</em><em>y</em>,<em>T</em><em>o</em><em>m</em>)</span>为假，因此原命题 <spanclass="math inline">¬<em>b</em><em>r</em><em>o</em><em>t</em><em>h</em><em>e</em><em>r</em>(<em>M</em><em>a</em><em>r</em><em>y</em>,<em>T</em><em>o</em><em>m</em>)</span>为真，Mary 不是 Tom 的兄弟。</p>]]></content>
      
      
      <categories>
          
          <category> AI </category>
          
      </categories>
      
      
        <tags>
            
            <tag> lecture notes </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>初识Transformer</title>
      <link href="/2023/10/24/%E5%88%9D%E8%AF%86Transformer/"/>
      <url>/2023/10/24/%E5%88%9D%E8%AF%86Transformer/</url>
      
        <content type="html"><![CDATA[<h2 id="seq2seq">01 Seq2seq</h2><p>Transformer 是一个 sequence-to-sequence model，我们一般简写作seq2seq。seq2seq 的意思是指，输入是一个 sequence，输出也是一个sequence，并且两个 sequence 的长度不一定，这里的长度不一定的意思是指：1.输入的长度不一定；2. 输出的长度由机器自己决定；3.输入和输出的长度并不存在必然关系。</p><p>比较典型的 seq2seq 的例子有语音识别、文字翻译、语音翻译等，还有例如text-to-speech (文字转语音) 的 model 也是一种 seq2seq，甚至是聊天机器人chatbot，也是 seq2seq。</p><p>其实，seq2seq 在 NLP 方面的应用是很广泛的，很多你认为可能跟 seq2seq无关的任务也可以转换成 seq2seq。NLP 的本质其实是 question answering(QA)，而只要能想象成 QA，就基本上都能用 seq2seq 解决。</p><p>Seq2seq 还可以用于解决 multi-label classification，multi-label是将一个东西分到多个类别里的任务，并且一个东西可能属于不止一个类别，而seq2seq的输出长度是机器自己决定的，也就是机器觉得有几个输出就几个输出，也就是机器觉得这个东西属于哪几个类别那就是哪几个类别，所以seq2seq 也可以硬解 multi-label classification。</p><p>除此以外，就连图像识别问题也可以用seq2seq，但是这里就不展开了。至少至此，我们已经知道了 seq2seq是一个强大的model，那么它究竟是怎么做到的，接下来我们就要开始研究了。</p><span id="more"></span><h2 id="mechanism">02 Mechanism</h2><p>Seq2seq 主要由两大部分组成 —— encoder 和 decoder。</p><p><img src="/img/Transformer-01.png" /></p><p>Encoder 负责接收和处理数据，然后将处理过的数据交给 decoder，decoder则根据数据决定输出结果。接下来，我们就来具体来看看 encoder 和 decoder的结构。</p><h3 id="encoder">2.1 Encoder</h3><p>简单来说，encoder要做的事情就是将接收的一排向量，转换成另一排向量，这一个过程可以用很多种方法来完成，例如RNN 和 CNN，而 transformer 采用的是[[Self-attention|self-attention]]，大名鼎鼎的注意力机制也就是从transformer 中诞生的。</p><p>Encoder 内部其实是一个一个 block (块)，每个 block都接收一排向量，输出一排向量，而且每个 block的工作也大致都是相同的，其实都是对 input 做 self-attention，然后将output1 丢进一个全连接网络，得到 output2，这个就是一个 block的输出。所以 encoder 的结构大致如下：</p><p><img src="/img/Transformer-02.png" /></p><p>实际上，transformer 中的 self-attention 是比我们之前讲的self-attention 更复杂的。在 transformer 中，self-attention 的 output还要再加上最开始的 input，才算得到最终的 output，这种架构被称为<strong>residual connection(残差连接)</strong>，这个技术旨在解决深度神经网络训练过程中的梯度消失和梯度爆炸等问题。然后，还要对得到的residule 做 layer normalization，方法是：求 residule 整个序列的均值<span class="math inline"><em>m</em></span> 和标准差 <spanclass="math inline"><em>σ</em></span>，然后做标准化：</p><p><span class="math display">$$x_i^\prime=\frac{x_i-m}{\sigma}$$</span></p><p>这样得到的序列才是全连接网络的输入，但是还没完，全连接网络的输出仍要再进行一次residule connection，即将输入和输出相加，然后同样要对 residule 进行layer normalization，这样才能得到 block 的输出。</p><p>现在我们来看一下 <em>Attention is all you need</em> 这篇论文中所画的encoder 的结构：</p><p><img src="/img/Transformer-03.png" /></p><p>首先，input 进行 embedding 之后，作为输入进行 multi-headattention，考虑到有些时候序列可能是有序的，所以图中还画出了 positionalencoding 的环节，这项技术在 self-attention 的笔记中有提到。Attention之后，得到的输出要先于最初的输入进行相加 (add)，然后进行 layernormalization，这就是上图中淡黄色框的含义。再网上要进行 feedforward，其实就是把上一步得到的结果喂给全连接网络，然后对得到的结果再进行一次Add &amp; Norm。这就是上图的含义。</p><h3 id="decoder">2.2 Decoder</h3><p>Decoder 的架构其实分为两种 —— autoregressive (AT) 和non-autoregressive (NAT)，接下来要将的 autoprogressive是比较常见的架构，我们将以语音辨识为例进行讲解。</p><h4 id="autoprogressive">2.2.1 Autoprogressive</h4><h5 id="decoder-的工作流程">decoder 的工作流程</h5><p>假设我们在处理语音辨识的问题，现在，语音已经通过 encoder转换成了一个序列，decoder要接收这个序列，然后输出对应的文字。我们暂且不提 decoder 是如何接收encoder 的输出的，我们假设 decoder 能接收到 encoder的输出，然后来解释一下 decoder 的工作流程。</p><p>首先，我们得给 decoder 一个特殊的 token，我们称之为 BOS (begin ofsentence)，接下来简称 BEGIN。当 decoder 接收到这个 token时，它就开始输出，它的输出应该是一个 one-hotvector，也就是一个独热编码的向量，其大小等同于词汇的大小，以中文为例，可能就是所有中文字的数量。当然你可能会说这有点太大了，那实际上，我们可能只取常见的几千个中文字，生僻字我们不去管。</p><p>这个 vector 中的数字就是每个对应位置上的中文字的可能性，它们的总和是1，由对 decoder 的输出做 softmax 之后得到，最终 decoder生成汉字就是取其中可能性最大的那个。</p><p>总而言之，我们现在得到了第一个汉字，假设这个汉字是“机”，那么下一步，我们要把这个decoder 生成的汉字加入到 decoder 的输入中，也就是说，现在的 decoder的输入不只有 BEGIN了，还有了“机”，于是重复上面的步骤，得到第二个输出“器”，周而复始……最终得到完整的输出“机器学习”。在这个过程中，decoder当然也有读入 encoder的输出，但是这一部分我们先不讲。总结上面的过程，我们可以说，其实 decoder就是将自己前一刻的输出当作输入，进一步得到下一时刻的输出。这里就诞生了一个问题：要是decoder 自己预测的内容出错了怎么办？会不会造成 errorpropagation，也就是一步错步步错？当然是有可能的，但是这个问题我们之后再谈，我们先暂且当作没这回事。</p><h5 id="decoder-和-encoder-的结构对比">decoder 和 encoder的结构对比</h5><p>现在我们来看看论文中画的 decoder 的结构，decoder看上去很复杂，但如果我们将其与 encoder的结构进行对比，似乎能发现一些相似之处。</p><p><img src="/img/Transformer-04.png" /></p><p>你可能会发现，如果我们把 decoder 中间那一块盖起来，encoder的结构好像就跟 decoder 差不了多少了，无非是 decoder的输出最后还要经过一个线性层，再经过 softmax激活，来输出可能性罢了。唯一的不同可能就是 decoder 中，一开始的attention 是 masked multi-head attention，那这个 masked 是什么意思？</p><h5 id="masked-self-attention">masked self-attention</h5><p>在我们原来所讲的 self-attention 中，输出的 <spanclass="math inline"><em>b</em><sub>1</sub></span> 是考虑了 <spanclass="math inline"><em>a</em><sub>1</sub> ∼ <em>a</em><sub><em>n</em></sub></span>所有的资讯之后得到的 <spanclass="math inline"><em>a</em><sub>1</sub></span> 的资讯，<spanclass="math inline"><em>b</em><sub>2</sub> ∼ <em>b</em><sub><em>n</em></sub></span>皆是如此；而在 masked self-attention中，输出不再能考虑后面的资讯，意思是，<spanclass="math inline"><em>b</em><sub>1</sub></span> 只是考虑了 <spanclass="math inline"><em>a</em><sub>1</sub></span> 后，<spanclass="math inline"><em>a</em><sub>1</sub></span> 的资讯；<spanclass="math inline"><em>b</em><sub>2</sub></span> 是考虑了 <spanclass="math inline"><em>a</em><sub>1</sub>, <em>a</em><sub>2</sub></span>之后，<span class="math inline"><em>a</em><sub>2</sub></span>的资讯；<span class="math inline"><em>b</em><sub>3</sub></span> 是考虑了<spanclass="math inline"><em>a</em><sub>1</sub>, <em>a</em><sub>2</sub>, <em>a</em><sub>3</sub></span>之后，<span class="math inline"><em>a</em><sub>3</sub></span>的资讯……只有 <spanclass="math inline"><em>b</em><sub><em>n</em></sub></span>是考虑了整个输入后，输出的资讯。这就是 masked self-attention。</p><p>至于为什么要用 masked，其实原因很简单。我们之前也解释了，decoder会拿自己的输出作为输入，输出是从左到右依次产生的，输入肯定也只能从左到右依次输入，所以就出现了这样只能读左边，而不能读右边的masked self-attention 机制。</p><h5 id="何时终止输出">何时终止输出</h5><p>之前说过，transformer 的输出长度是机器自己决定的，也就是由 decoder决定的，但到目前为止，我们都还没有讨论过，decoder到底是如何决定输出长度的，它是怎么知道什么时候该停止输出的。就像前面举的例子，在decoder 输出完“机器学习”之后，万一它又把BEGIN+“机器学习”作为输入，输出了个“惯”字怎么办？</p><p>对这一点的处理是很巧妙的，要解决这个问题，我们还得再准备一个特殊的token，叫作 END。就像 BEGIN 是开始的标志，END 就是终止的标志，并且，END存储在 encoder 的输出 one-hot vector 里，也就是说，one-hot vector的长度是 vocabulary 的长度再加上 END。如果根据 encoder的计算，发现输出的 one-hot vector 中，END的概率是最高的，那就意味着输出该结束了。通过这种方式，encoder就能自行决定何时终止输出。</p><h4 id="non-autoregressive">2.2.2 Non-autoregressive</h4><p>接下来我们简短地介绍一下另一种 decoder 的架构 ——non-autoregressive，简称 NAT。</p><h5 id="at-vs-nat">AT vs NAT</h5><p>AT 的运作方式是，先将开始标志 BEGIN作为输入传入，得到一个输出，随后将自己的输出作为输入再传进来，再得到下一个输出，然后再将下一个输出继续传进来，得到下下个输出，循环往复，直到输出END；而 NAT 则不一样，它一开始就接收多个BEGIN，于是得到多个输出，也就是一个完整的句子，然后就结束了。</p><h5 id="何时终止输出-1">何时终止输出</h5><p>那么问题就来了，既然我们一开始都不知道输出的长度，那我们要怎么知道该给decoder 多少个 BEGIN？</p><p>解决这个问题的方法有很多，一种方法就是准备一个 classifier，将 encoder的输出先丢给 classifier，由 classifier决定输出的长度，于是就丢对应长度的 BEGIN。</p><p>另一种可能的处理方法是，不管输出的长度，直接丢给 decoder 尽可能大的BEGIN 数目，然后看看 decoder 的输出中，哪里出现了 END 标志，我们只取 END前面的内容，END 后面的输出就不管了。</p><h5 id="nat-的优势">NAT 的优势</h5><p>NAT 的优势在于，它的运算是平行的，而不像 AT那样，要预测下一个输出，就必须等前一个输出完成，所以 NAT 的速度应该是比AT要快的；另外，它的输出长度是可控的，这就允许我们人为地去控制输出。</p><p>NAT 是一个热门的研究话题，它的性能比 AT 要好，也比 AT的可控性高，但一个严重的问题是，NAT 的准确率是远不及 AT 的。NAT 要想赶上AT 的准确率，往往需要很多的秘诀才能做到，而 AT可能只要随便跑跑就能达到比较高的准确率。所以，如何让 NAT 赶上 AT的准确率，是目前一个大热门。不过这里就不细讲了，毕竟这是一个大坑，有兴趣的话可以自己去了解。</p><h3 id="encoder-decoder">2.3 Encoder-Decoder</h3><p>接下来，我们就要来看看之前说先暂且遮起来的那一块了。</p><p><img src="/img/Transformer-05.png" /></p><h5 id="cross-attention">cross attention</h5><p>图中被框起来的那一部分叫作 cross attention，它是连接 encoder 和decoder 的桥梁。你可以看到，从 encoder 引出了两个箭头连接到了 multi-headattention，除此以外还有一个箭头是从 decoder的上一层引出的，那这个模组到底是怎么运作的？我们继续以语音辨识的例子进行阐述。</p><p>首先，encoder 读进一个向量 <spanclass="math inline"><em>a</em><sup>1</sup>, <em>a</em><sup>2</sup>, <em>a</em><sup>3</sup></span>，并且输出一个等长的向量<spanclass="math inline"><em>b</em><sup>1</sup>, <em>b</em><sup>2</sup>, <em>b</em><sup>3</sup></span>，用同样的方法，得到对应的<spanclass="math inline"><em>k</em><sup>1</sup>, <em>k</em><sup>2</sup>, <em>k</em><sup>3</sup></span>和 <spanclass="math inline"><em>v</em><sup>1</sup>, <em>v</em><sup>2</sup>, <em>v</em><sup>3</sup></span>；同时，decoder读进一个 BEGIN，进行 masked self-attention，由于是attention，所以输出也肯定是一个和 BEGIN等长的向量，接下来，将这个输出乘上一个矩阵，进行 transform，得到 <spanclass="math inline"><em>q</em></span>；然后用 <spanclass="math inline"><em>q</em></span> 与 <spanclass="math inline"><em>k</em><sup>1</sup>, <em>k</em><sup>2</sup>, <em>k</em><sup>3</sup></span>去分别计算，得到 <spanclass="math inline"><em>α</em><sup>1</sup>, <em>α</em><sup>2</sup>, <em>α</em><sup>3</sup></span>，然后再分别乘上<spanclass="math inline"><em>v</em><sup>1</sup>, <em>v</em><sup>2</sup>, <em>v</em><sup>3</sup></span>，将加权的结果加起来，得到<span class="math inline"><em>v</em></span>，这个 <spanclass="math inline"><em>v</em></span>就是接下来的全连接网络的输入。在这个过程中，<spanclass="math inline"><em>α</em>, <em>k</em>, <em>v</em></span> 都来自encoder，而 <span class="math inline"><em>q</em></span> 来自decoder，所以这个过程就叫做 <strong>cross attention</strong>。</p><p><img src="/img/Transformer-06.png" /></p><p>如果说现在 decoder已经产生了一个输出“机”，那么接下来的操作也是一样的，将“机”作为输入传进去，进行masked self-attention，然后得到 <spanclass="math inline"><em>q</em><sup>′</sup></span>，去做相同的运算得到<spanclass="math inline"><em>v</em><sup>′</sup></span>，再丢进全连接网络得到下一个输出，如此往复……</p><h3 id="conclusion">2.4 Conclusion</h3><p>Transformer的工作流程已经讲完了，为了深入理解这些过程，而不是仅仅停留于表面的数学运算，我们还得从实例中剖析这个过程。Ecoder所做的工作其实是对 input 进行提炼，方法就是 self-attention；而 decoder则是将 encoder 提炼后的数据，以及一个开始标志 BEGIN作为输入，开始生成结果，并且每得到一个 output，就将这个结果加入到自己的input 中，做 masked self-attention，以此不断得到新的 output，直到decoder 输出 END 为止。</p><p>下面我们以机器翻译为例，假设我们需要机器翻译“我喜欢你”这句话，那么transfomer 做的第一件事是，将这句话输入到 encoder 当中，通过self-attention 获取语义编码，这里以 <spanclass="math inline"><em>c</em></span> 标识，这里的 <spanclass="math inline"><em>c</em></span> 是一个向量，其中包括了 <spanclass="math inline"><em>c</em><sub>1</sub>, <em>c</em><sub>2</sub>, ...</span>等等，每一个汉字就对应一个 <spanclass="math inline"><em>c</em></span>。</p><p><spanclass="math display"><em>c</em> = Encoder ("我喜欢你")</span></p><p>然后，decoder 将该语义编码与一个 token，也就是上面说的 BEGIN作为输入，得到第一个单词的输出：</p><p><spanclass="math display">I = Decoder (<em>c</em><sub>1</sub>,<em>B</em><em>E</em><em>G</em><em>I</em><em>N</em>)</span></p><p>然后将这个单词作为输入，再进行一次输出：</p><p><spanclass="math display">love  = Decoder (<em>c</em><sub>2</sub>,<em>B</em><em>E</em><em>G</em><em>I</em><em>N</em>,<em>I</em>)</span></p><p>重复上面的过程，直到输出 END：</p><p><span class="math display">$$\begin{aligned}&amp;\text{you} = \operatorname{Decoder}(c_3,BEGIN,I,love)\\\\&amp;\text{END}=\operatorname{Decoder}(c_4,BEGIN,I,love,you)\end{aligned}$$</span></p><p>于是最后得到的翻译结果：<span class="math inline">I loveyou.</span></p><h2 id="training">03 Training</h2><p>现在我们已经把 transformer 的内部运作方式给讲完了，下一步，就是讲讲transformer 的训练。</p><p>我们知道，机器学习的目标就是不断降低 loss。而 seq2seq这件事，似乎就是在做分类，最后的输出是从所有汉字中选择一个最可能的，所以衡量seq2seq 的 loss，可以使用和 classifier 类似的方法。在 transformer中，我们继续以语音辨识为例，decoder输出的每一个汉字都与正确答案之间有一个 crossentropy，模型优化的目标就是，使所有输出与正确答案之间的 cross entropy的总和最小。</p><h3 id="copy-mechanism">3.1 Copy mechanism</h3><p>有些时候，输出的序列会和输入的序列有重合的部分，例如下面这个 chatbot的例子：</p><p><img src="/img/Transformer-07.png" /></p><p>再比如说文献摘要，让机器来给一篇文献写摘要，摘要中肯定会有与正文内容重复的部分。我们当然是希望机器能够从输入的内容中，直接把这些重合的部分提取出来，而不是自己合成，那这就需要借助copy mechanism，让机器能够直接从输入中把部分内容 copy过来。这样的模型当然是存在的，最早的具有复制能力的模型是 copynetwork，后来还有论文 <ahref="https://arxiv.org/abs/1603.06393">Incorporating Copy Mechanism inSequence-to-Sequence Learning</a>也做过这个，如果感兴趣可以自己去了解。</p><h3 id="guided-attention">3.2 Guided attention</h3><p>机器观察输入，对输入做 attention的顺序是不固定的，这是机器自己学习的结果，自己学习就会导致问题。例如在TTS (Text to Speech) 的任务中，有些时候，机器居然会漏字。如果是一般的chatbot 或者 summarization 问题，那么漏一两个字可能也没什么关系，但在TTS 中，漏字是非常严重的问题。这个时候，我们可以用 guided attention来教给机器一个固定的顺序来处理输入。</p><p>例如说，在 TTS 中，读入一段文字之后，机器应该从左到右依次做attention，这样才是正确解决问题的方法。但如果机器颠三倒四，先看后面，再看前面，最后看中间，那显然有些事情就做错了，需要我们人工纠正。</p><p><img src="/img/Transformer-08.png" /></p><p>所以 guided attention 就是要求机器的 attention遵循一个固定的法则，这个法则肯定是我们事先就知道了这个问题的处理方式才得出的，guidedattention 也一般只适用于那些有固定解法的问题。</p><p>一些关于 guided attention 的关键词汇：<u>monotonicattention</u>、<u>location-awareattention</u>。如果感兴趣可以自行搜索。</p><h3 id="beam-search">3.3 Beam search</h3><p>继续以语音辨识为例，假设现在的 vocabulary 中只有 A 和 B两个字，那么我们可以构建出一棵树，每次 decoder 都只面对 A 和 B两种选择。那么根据原则，在每一个节点处，decoder会选择分数最高的那个，一直到叶子节点。这种搜索方法叫作 greedydecoding，因为它每次都挑分数最高的。</p><p>但是有时候，选择当前分数最高的，未必能选出一条总分数最高的path，如下图所示：</p><p><img src="/img/Transformer-09.png" /></p><p>虽然一开始 B 的分数比 A低，但是我们可以看到，之后路径上的节点的分数明显比 A之后路径上节点分数要高，所以总体而言，一开始选分数较低的那个，反而能得到一条更好的path。但是我们要怎么做才能选出这条 path 呢？一种方法是dfs，即全部走一遍，但显然这种方法不显示，毕竟中文里的汉字有几千个，不可能用dfs 来搜索。</p><p>那么还有一种方法就是 beamsearch，它可以找出一条相对好的，但也不是很精准的path。那么这个算法到底有没有用呢？有趣的是，它有时候有效，有时候就没什么作用。那什么时候有用呢？根据研究，当一个问题有一个比较明确的解时，beamsearch就会比较有用；但当一个问题需要发挥机器自己的想象力来完成的时候，beamsearch 就比较没用。如果感兴趣，可以自行了解。</p>]]></content>
      
      
      <categories>
          
          <category> deep learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> technique </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
