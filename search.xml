<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>A WSL Error Encountered When Installing Docker</title>
      <link href="/2025/04/07/A-WSL-Error-Encountered-When-installing-Docker/"/>
      <url>/2025/04/07/A-WSL-Error-Encountered-When-installing-Docker/</url>
      
        <content type="html"><![CDATA[<p>This document explains the problems the author had installing Dockerand how to solve them. You can find and download the installationpackage for Docker <a href="https://www.docker.com/">here</a>.</p><p>After installing Docker, an <code>Unexpected WSL error</code> wasencountered, and the process was terminated. After conducting someonline research, I discovered that this error message was indicatingthat I needed to enable the <strong>Hyper-V</strong>, <strong>WindowsSubsystem for Linux</strong> and <strong>Virtual MachinePlatform</strong> functions on my system. I opened the configurationpanel and found that the latter two functions had been properly enabled,but the option for the first one was missing. It took me a some time tofind the solution.</p><span id="more"></span><p>To address the issue, it was necessary to create and run a file named<code>Hyper-V.bat</code> as administrator, which contains the followingcontent:</p><figure class="highlight bat"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">pushd</span> &quot;%~dp0&quot;</span><br><span class="line"><span class="built_in">dir</span> /b <span class="variable">%SystemRoot%</span>\servicing\Packages\*Hyper-V*.mum &gt;hyper-v.txt</span><br><span class="line"><span class="keyword">for</span> /f <span class="variable">%%i</span> <span class="keyword">in</span> (&#x27;<span class="built_in">findstr</span> /i . hyper-v.txt <span class="number">2</span>^&gt;<span class="built_in">nul</span>&#x27;) <span class="keyword">do</span> dism /online /norestart /add-package:&quot;<span class="variable">%SystemRoot%</span>\servicing\Packages\<span class="variable">%%i</span>&quot;</span><br><span class="line"><span class="built_in">del</span> hyper-v.txt</span><br><span class="line">Dism /online /enable-feature /featurename:Microsoft-Hyper-V-All /LimitAccess /ALL</span><br></pre></td></tr></table></figure><p>The program will then require you to restart your machine, at whichpoint you will see that Hyper-V is properly enabled. But Docker stillcannot run normally.</p><p>After that, you need to install WSL2. To do this, run the followingcommand:</p><figure class="highlight cmd"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">wsl --install# may require VPN</span><br><span class="line">wsl --<span class="built_in">set</span>-default-version <span class="number">2</span># <span class="built_in">set</span> the default wsl version to wsl2</span><br><span class="line">wsl --update# may <span class="keyword">not</span> be necessary</span><br></pre></td></tr></table></figure><p>Finally, Docker can run without any problems after all this work.</p><hr /><p>Off topic:</p><p>When using the VSCode extension <code>Remote Containers</code> to setup containers, you might encounter the message “the container does notmeet all the requirements of the VS Code Server”. This happens becauseVSCode has increased the minimum requirements for remote server buildtoolchain since version 1.86. To solve the problem, just downgrade yourVSCode to a version below 1.86. You can download version 1.85.2 <ahref="https://update.code.visualstudio.com/1.85.2/win32-x64-archive/stable">here</a>.Besides, it is necessary to downgrade your extensions as well.</p>]]></content>
      
      
      <categories>
          
          <category> solutions </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Docker </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Introduction to Program Analysis</title>
      <link href="/2025/04/07/Introduction-to-Program-Analysis/"/>
      <url>/2025/04/07/Introduction-to-Program-Analysis/</url>
      
        <content type="html"><![CDATA[<h2 id="what-is-program-analysis">01 What Is Program Analysis</h2><p>Program analysis is to discover useful facts about programs. Youprobably have known some manual or automated testing tools like:</p><ul><li><p>Manual testing or semi-automated testing: JUnit, Selenium,etc.</p></li><li><p>Manual “analysis” of programs: Code inspection, debugging,etc.</p></li></ul><p>The focus of this course is <strong>automated</strong> programanalysis.</p><span id="more"></span><p>Program analysis can be broadly classified into three kinds:</p><ul><li>Static (compile-time)<ul><li>Infer facts by inspecting source or binary code</li><li>Typically:<ul><li>Consider all inputs</li><li>Overapproximate possible behavior</li></ul></li><li>E.g. compilers, lint-like tools</li></ul></li><li>Dynamic (execution-time)<ul><li>Infer facts by monitoring program executions</li><li>Typically:<ul><li>Consider current input</li><li>Underapproximate possible behavior</li></ul></li><li>E.g. automated testing tools, profilers</li></ul></li><li>Hybrid (combining dynamic and static)</li></ul><h2 id="terminology">02 Terminology</h2><p>The following is a snippet of JavaScript code.</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">var</span> r = <span class="title class_">Math</span>.<span class="title function_">random</span>();<span class="comment">//value in [0,1)</span></span><br><span class="line"><span class="keyword">var</span> out = <span class="string">&quot;yes&quot;</span>;</span><br><span class="line"><span class="keyword">if</span>(r &lt; <span class="number">0.5</span>)</span><br><span class="line">out = <span class="string">&quot;no&quot;</span>;</span><br><span class="line"><span class="keyword">if</span>(r == <span class="number">1</span>)</span><br><span class="line">out = <span class="string">&quot;maybe&quot;</span>;</span><br><span class="line"><span class="variable language_">console</span>.<span class="title function_">log</span>(out);</span><br></pre></td></tr></table></figure><p>Q: What are the possible outputs?</p><h3 id="overapproximation-v.s.-underapproximation">2.1 Overapproximationv.s. Underapproximation</h3><p>Judging from the static code, it seems that there are three possibleoutputs: “yes”, “no” or “maybe”. (Overapproximation)</p><p>If we consider the case of only one execution like<code>r=0.7</code>, its output is “yes”. (Underapproximation)</p><p>However, both responses are erroneous. The first option yields theimplausible output “maybe”, while the second excludes the feasibleoutput “no”. These erroneous responses serve as quintessentialillustrations of over- and under-approximation, respectively.</p><ul><li>Overapproximation: Consider all paths</li><li>Underapproximation: Execute the program once</li></ul><h3 id="soundness-completeness">2.2 Soundness &amp; Completeness</h3><p>It is easy for us humans to give the right answer —— “yes” or “no”.We think these answers are <strong>sound</strong> and<strong>complete</strong>.</p><p>“Soundness” means it contains all the possible outputs we want (mightgive <strong>false positives</strong>).</p><p>“Completeness” means it excludes all the impossible outputs we do notwant (might give <strong>false negtives</strong>).</p><p>When we put these two ideas together, we get a definition thatincludes exactly all possible outputs.</p><h3 id="false-positives-false-negatives">2.3 False Positives &amp; FalseNegatives</h3><p>The definitions of false positives and false negatives:</p><ul><li>False positives: impossible outputs that are indicated possible</li><li>False negatives: possible outputs that are indicated impossible</li></ul><p>Let <span class="math inline"><em>P</em></span> be <em>Program</em>,<span class="math inline"><em>i</em></span> be <em>Input</em>, <spanclass="math inline"><em>P</em>(<em>i</em>)</span> be <em>Behavior</em>.The following graph shows the relations between the above ideas.</p><p><img src="/img/Introduction-to-Program-Analysis-01.png" /></p><h3 id="precision-recall">2.4 Precision &amp; Recall</h3><p>Differentiate precision and recall:</p><ul><li><p>Precision: how many retrieved items are relevant</p></li><li><p>Recall: how many relevant items are retrieved</p></li></ul><p>Take the overapproximated answer aforementioned for instance, theprecision and the recall are:</p><p><span class="math display">$$\mathrm{precision}=\frac{2}{3}=0.67$$</span></p><p><span class="math display">$$\mathrm{recall}=\frac{1}{2}=0.5$$</span></p><h3 id="program-invariants">2.5 Program Invariants</h3><p>Program Invariants are logical assertions whose certain conditions orproperties remain true throughout the execution of a program. Theseinvariants are key to program correctness. They help verify that theprogram behaves as expected and play an important role in softwaredevelopment.</p><p>See the below code snippet:</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">int</span> <span class="title function_">p</span><span class="params">(<span class="type">int</span> x)</span> &#123; <span class="keyword">return</span> x * x; &#125;</span><br><span class="line"><span class="type">void</span> <span class="title function_">main</span><span class="params">()</span> &#123;</span><br><span class="line">    <span class="type">int</span> z;</span><br><span class="line">    <span class="keyword">if</span> (getc() == <span class="string">&#x27;a&#x27;</span>)</span><br><span class="line">    z = p(<span class="number">6</span>) + <span class="number">6</span>;</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">    z = p(<span class="number">-7</span>) - <span class="number">7</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Q: An invariant at the end of the program is <code>(z == c)</code>for some constant <code>c</code>. What is <code>c</code>?</p><p>Clearly, the <code>z</code> will yield <code>42</code> regardless ofany inputs. Therefore, <code>(z == 42)</code> is definitely aninvariant, while <code>(z == 30)</code> is definitely not aninvariant.</p><p>Using the invariant to avert disaster:</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">int</span> <span class="title function_">p</span><span class="params">(<span class="type">int</span> x)</span> &#123; <span class="keyword">return</span> x * x; &#125;</span><br><span class="line"><span class="type">void</span> <span class="title function_">main</span><span class="params">()</span> &#123;</span><br><span class="line">    <span class="type">int</span> z;</span><br><span class="line">    <span class="keyword">if</span> (getc() == <span class="string">&#x27;a&#x27;</span>)</span><br><span class="line">    z = p(<span class="number">6</span>) + <span class="number">6</span>;</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">    z = p(<span class="number">-7</span>) - <span class="number">7</span>;</span><br><span class="line">    <span class="keyword">if</span> (z != <span class="number">42</span>) &#123;</span><br><span class="line">        disaster();<span class="comment">// disaster averted</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="others">03 Others</h2><h3 id="undecidability-of-program-properties">3.1 Undecidability ofProgram Properties</h3><ul><li>Q: Can program analysis be sound and complete? A: Not if we want itto terminate!</li><li>Questions like “is a program point reachable on some input?” are<strong>undecidable</strong>.</li><li>Designing a program analysis is an art —— tradeoffs dictated byconsumer.</li></ul><h3 id="why-take-this-course">3.2 Why Take This Course?</h3><ul><li><p>Learn methods to improve software quality, reliability, security,performance, etc.</p></li><li><p>Become a better software developer/tester</p></li><li><p>Build specialized tools for software analysis, testing andverification</p></li><li><p>Finding Jobs &amp; Do research</p></li></ul><h3 id="who-needs-program-analysis">3.3 Who Needs Program Analysis?</h3><p>Three primary consumers of program analysis:</p><ul><li>Compilers</li><li><strong>Software Quality Tools (Primary focus of thiscourse)</strong></li><li>Integrated Development Environments (IDEs)</li></ul><h4 id="compilers">3.3.1 Compilers</h4><p>Program analysis serves as the bridge between high-level languagesand architectures.</p><p>For example, we use program analysis to generate efficient code.</p><p>Before:</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">int</span> <span class="title function_">p</span><span class="params">(<span class="type">int</span> x)</span> &#123; <span class="keyword">return</span> x * x; &#125;</span><br><span class="line"><span class="type">void</span> <span class="title function_">main</span><span class="params">(<span class="type">int</span> arg)</span> &#123;</span><br><span class="line">    <span class="type">int</span> z;</span><br><span class="line">    <span class="keyword">if</span> (arg != <span class="number">0</span>)</span><br><span class="line">    z = p(<span class="number">6</span>) + <span class="number">6</span>;</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">    z = p(<span class="number">-7</span>) - <span class="number">7</span>;</span><br><span class="line">    print (z);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>After:</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">int</span> <span class="title function_">p</span><span class="params">(<span class="type">int</span> x)</span> &#123; <span class="keyword">return</span> x * x; &#125;</span><br><span class="line"><span class="type">void</span> <span class="title function_">main</span><span class="params">()</span> &#123;</span><br><span class="line">print (<span class="number">42</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="software-quality-tools">3.3.2 Software Quality Tools</h4><p>Software quality tools are tools for testing, debugging, andverification.</p><p>Software quality tools use program analysis for:</p><ul><li>Finding programming errors</li><li>Proving program invariants</li><li>Generating test cases</li><li>Localizing causes of errors</li><li>…</li></ul><p>Some software quality tools:</p><ul><li>Static Program Analysis<ul><li>Suspicious error patterns: <em>Lint</em>, <em>SpotBugs</em>,<em>Coverity</em></li><li>Memory leak detection: <em>Facebook Infer</em></li><li>Checking API usage rules: <em>Microsoft SLAM</em></li><li>Verifying invariants: <em>ESC/Java</em></li></ul></li><li>Dynamic Program Analysis<ul><li>Array bound checking: <em>Purify</em></li><li>Datarace detection: <em>Eraser</em></li><li>Memory leak detection: <em>Valgrind</em></li><li>Finding likely invariants: <em>Daikon</em></li></ul></li></ul><h4 id="integrated-development-environments">3.3.3 IntegratedDevelopment Environments</h4><p>Examples: <em>Eclipse</em> and <em>VS Code</em></p><p>Use program analysis to help programmers:</p><ul><li>Understand programs</li><li>Refactor programs<ul><li>Restructuring a program without changing its behavior</li></ul></li></ul><p>Useful in dealing with large, complex programs</p><h2 id="quiz">04 Quiz</h2><ul><li>Dynamic vs. Static Analysis:</li></ul><table><colgroup><col style="width: 12%" /><col style="width: 44%" /><col style="width: 43%" /></colgroup><thead><tr class="header"><th style="text-align: center;"></th><th style="text-align: center;">Dynamic</th><th style="text-align: center;">Static</th></tr></thead><tbody><tr class="odd"><td style="text-align: center;">Cost</td><td style="text-align: center;"><u>Proportional to program’s executiontime</u></td><td style="text-align: center;"><u>Proportional to program’ssize</u></td></tr><tr class="even"><td style="text-align: center;">Effectiveness</td><td style="text-align: center;"><u>Unsound (may miss errors)</u></td><td style="text-align: center;"><u>Incomplete (may report spuriouserrors)</u></td></tr></tbody></table><ul><li>Unsoundness yields <u>false negatives</u>; incompleteness yields<u>false positives</u>.</li></ul>]]></content>
      
      
      <categories>
          
          <category> Software Analysis, Testing and Verification </category>
          
      </categories>
      
      
        <tags>
            
            <tag> lecture notes </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>隐马尔可夫模型</title>
      <link href="/2023/12/18/%E9%9A%90%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E6%A8%A1%E5%9E%8B/"/>
      <url>/2023/12/18/%E9%9A%90%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E6%A8%A1%E5%9E%8B/</url>
      
        <content type="html"><![CDATA[<p>隐马尔可夫模型 HMM（Hidden MarkovModel）是一种统计模型，用来描述一个隐含未知量的马尔可夫过程（马尔可夫过程是一类随机过程，它的原始模型是马尔科夫链），它是结构最简单的动态贝叶斯网，是一种著名的有向图模型，主要用于时序数据建模，在语音识别、自然语言处理等领域有广泛应用，是一种<strong>生成式模型</strong>。</p><span id="more"></span><h2 id="马尔可夫模型">01 马尔可夫模型</h2><p>在学习隐马尔可夫模型之前，我们先来了解一下它的前生——马尔可夫模型MM（Markov Model）。</p><p>我们用一个例子进行引入：天气的变化应该具有某种联系。晴天、多云和暴雨这三种天气之间的转换应该存在某种规律，下图中的箭头表示两种天气之间转换的概率：</p><p><img src="/img/隐马尔可夫模型-01.png" /></p><p>于是我们能得到一个<strong>状态转移概率矩阵</strong>：</p><p><img src="/img/隐马尔可夫模型-02.png" /></p><p>根据该矩阵，我们就能在知道今天天气的情况下，预测明天的天气。显然，这种预测是建立在==未来所处的状态仅与当前状态有关==的假设上的，即第二天的天气只取决于前一天的天气。这种假设就是<strong>马尔可夫假设</strong>，符合这种假设描述的随机过程，就被称为<strong>马尔可夫过程</strong>，其具有<strong>马尔可夫性</strong>，即<strong>无后效性</strong>。</p><p>令 <span class="math inline"><em>q</em><sub><em>t</em></sub></span>表示在时刻 <span class="math inline"><em>t</em></span>系统所处的状态，令 <spanclass="math inline"><em>S</em><sub><em>i</em></sub></span>表示某一具体状态，则 <spanclass="math inline"><em>q</em><sub><em>t</em></sub> = <em>S</em><sub><em>i</em></sub></span>表示在某一时刻 <span class="math inline"><em>t</em></span>，系统处于状态<span class="math inline"><em>S</em><sub><em>i</em></sub></span>，令<spanclass="math inline"><em>P</em>(<em>q</em><sub><em>t</em> + 1</sub>=<em>S</em><sub><em>i</em></sub>|<em>q</em><sub><em>t</em></sub>=<em>S</em><sub><em>j</em></sub>)</span>表示在前一时刻 <span class="math inline"><em>t</em></span> 系统处于状态<span class="math inline"><em>S</em><sub><em>j</em></sub></span>的情况下，下一时刻 <span class="math inline"><em>t</em> + 1</span>系统处于状态 <spanclass="math inline"><em>S</em><sub><em>i</em></sub></span>的概率。基于<strong>马尔可夫假设</strong>，则在马尔可夫模型中存在下列关系：<spanclass="math display"><em>P</em>(<em>q</em><sub><em>t</em> + 1</sub>=<em>S</em><sub><em>i</em></sub>|<em>q</em><sub><em>t</em></sub>=<em>S</em><sub><em>j</em></sub>,<em>q</em><sub><em>t</em> − 1</sub>=<em>S</em><sub><em>k</em></sub>,...) = <em>P</em>(<em>q</em><sub><em>t</em> + 1</sub>=<em>S</em><sub><em>i</em></sub>|<em>q</em><sub><em>t</em></sub>=<em>S</em><sub><em>j</em></sub>)</span>除了状态转移概率矩阵（用 <span class="math inline"><em>A</em></span>表示）之外，我们还需要知道所有状态的<strong>初始状态概率向量</strong><span class="math inline"><em>Π</em></span>，设系统中一共有 <spanclass="math inline"><em>N</em></span> 种状态，则 <spanclass="math inline"><em>Π</em></span> 的长度为 <spanclass="math inline"><em>N</em></span>，<spanclass="math inline"><em>Π</em></span>中的每一个元素代表系统的初始状态为某一状态的概率，且有 <spanclass="math inline">$\sum_{i=1}^N\Pi_i=1$</span>。</p><p>假设我们想计算一下今天 <spanclass="math inline"><em>t</em> = 1</span> 的天气状况，则我们可以得到：<span class="math display">$$P(q_1=S_{\text{sun}})=P(q_1=S_{\text{sun}}|q_0=\sum_{i=1}^{3}S_i)=\sum_{i=1}^{3}\Pi_{S_i}\timesA_{S_i\rightarrow S_{\text{sun}}}$$</span> 用文字形式表示就是：</p><p><img src="/img/隐马尔可夫模型-03.png" /></p><h2 id="隐马尔可夫模型">02 隐马尔可夫模型</h2><h3 id="概念">2.1 概念</h3><p>而隐马尔可夫模型就比马尔可夫模型要复杂多了。我们还是用上面这个例子进行引入，但是这次我们漂流到了一个岛上，这里没有天气预报，只有一片海藻，海藻具有干燥、较干、较湿和湿润四种状态。现在我们没有直接的天气信息了，但是天气状况跟海藻的状态还是有一定联系的，虽然看不见天气状况，但其决定了海藻的状态，所以我们还是能从海藻的状态推知天气的状态。</p><p>在这个例子里，海藻是能看到的，那它就是<strong>观测状态</strong>；天气信息是看不到的，那它就是<strong>隐藏状态</strong>。其中，隐藏状态天气时是决定性因素，观测状态是被决定因素，由隐藏状态到观测状态，这就是<strong>隐马尔可夫模型</strong>。</p><p><img src="/img/隐马尔可夫模型-04.png" /></p><p>如上图所示，观测状态（海藻的状态）有 4 个，而隐藏状态（天气）只有 3个，说明观测状态与隐藏状态的数量并不是一一对应的，可以根据需要定义。我们可以画出更加抽象的隐马尔可夫模型的示意图：</p><p><img src="/img/隐马尔可夫模型-05.png" /></p><p>图中，<spanclass="math inline"><em>Z</em><sub><em>i</em></sub></span>表示隐藏状态，<spanclass="math inline"><em>X</em><sub><em>i</em></sub></span>表示观测状态，隐藏状态决定了观测状态，所以箭头由 <spanclass="math inline"><em>Z</em></span> 指向 <spanclass="math inline"><em>X</em></span>。并且，隐藏状态之间还可以相互转换，所以<span class="math inline"><em>Z</em><sub><em>i</em></sub></span> 和<span class="math inline"><em>Z</em><sub><em>j</em></sub></span>之间也有箭头。根据马尔可夫假设，下一时刻的状态只取决于当前时刻的状态，所以，对于观测状态和隐藏状态来讲，都存在如下关系：<span class="math display">$$\begin{aligned}&amp;P=(Z_t|Z_{t-1},X_{t-1},Z_{t-2},X_{t-2},...,Z_1,X_1)=P(Z_t|Z_{t-1})\\&amp;P=(X_t|Z_{t},X_{t},Z_{t-1},X_{t-1},...,Z_1,X_1)=P(X_t|Z_t)\\\end{aligned}$$</span></p><h3 id="组成">2.2 组成</h3><p>马尔可夫模型有两个组成部分——初始状态概率向量 <spanclass="math inline"><em>Π</em></span> 和 状态转移概率矩阵 <spanclass="math inline"><em>A</em></span>。</p><p>而隐马尔可夫模型有则有三个组成部分：</p><ol type="1"><li>初始状态概率向量 <span class="math inline"><em>Π</em></span></li><li>状态转移概率矩阵 <span class="math inline"><em>A</em></span></li><li>观测状态概率矩阵 <span class="math inline"><em>B</em></span></li></ol><p>其中，<span class="math inline"><em>Π</em></span>是针对隐藏状态来说的，因为隐藏状态决定了观测状态；<spanclass="math inline"><em>A</em></span>矩阵是针对隐藏状态来说的，因为隐马尔可夫模型中进行状态转移的是隐藏状态；而<span class="math inline"><em>B</em></span>是由隐藏状态到观测状态转移的概率矩阵，在上例中，矩阵 <spanclass="math inline"><em>B</em></span> 可表示如下：</p><p><img src="/img/隐马尔可夫模型-06.png" /></p><p>也就是说，由 <spanclass="math inline"><em>Z</em><sub><em>i</em></sub> → <em>Z</em><sub><em>j</em></sub></span>的转换看矩阵 <span class="math inline"><em>A</em></span>，由 <spanclass="math inline"><em>Z</em><sub><em>i</em></sub> → <em>X</em><sub><em>i</em></sub></span>的转换看矩阵 <span class="math inline"><em>B</em></span>。</p><p>因此，隐马尔可夫模型 <span class="math inline"><em>λ</em></span>可以用三元符号表示： <spanclass="math display"><em>λ</em>(<em>A</em>,<em>B</em>,<em>Π</em>)</span></p><h3 id="求解目标">2.3 求解目标</h3><p>HMM 的求解目标有三个：</p><ol type="1"><li>给定模型 <spanclass="math inline"><em>λ</em>(<em>A</em>,<em>B</em>,<em>Π</em>)</span>及观测序列 <spanclass="math inline"><em>O</em> = {<em>o</em><sub>1</sub>, <em>o</em><sub>2</sub>, ..., <em>o</em><sub><em>t</em></sub>}</span>，计算该观测序列出现的概率<spanclass="math inline"><em>P</em>(<em>O</em>|<em>λ</em>)</span>；</li><li>给定观测序列 <spanclass="math inline"><em>O</em> = {<em>o</em><sub>1</sub>, <em>o</em><sub>2</sub>, ..., <em>o</em><sub><em>t</em></sub>}</span>，求解参数<span class="math inline">(<em>A</em>,<em>B</em>,<em>Π</em>)</span> 使得<span class="math inline"><em>P</em>(<em>O</em>|<em>λ</em>)</span>最大；</li><li>已知模型 <spanclass="math inline"><em>λ</em>(<em>A</em>,<em>B</em>,<em>Π</em>)</span>和观测序列 <spanclass="math inline"><em>O</em> = {<em>o</em><sub>1</sub>, <em>o</em><sub>2</sub>, ..., <em>o</em><sub><em>t</em></sub>}</span>，求状态序列，使得<spanclass="math inline"><em>P</em>(<em>I</em>|<em>O</em>,<em>λ</em>)</span>最大。</li></ol><h2 id="暴力求解法">03 暴力求解法</h2><p>我们要求的是在给定模型下观测序列出现的概率，那如果我们能把所有的隐藏序列都列出来，也就可以知道联合概率分布<spanclass="math inline"><em>P</em>(<em>O</em>,<em>I</em>|<em>λ</em>)</span>了（其中，<span class="math inline"><em>I</em></span> 为 <spanclass="math inline"><em>O</em></span> 对应的隐藏状态序列），再根据 <spanclass="math inline"><em>P</em>(<em>O</em>|<em>λ</em>) = ∑<sub><em>I</em></sub><em>P</em>(<em>O</em>,<em>I</em>|<em>λ</em>)</span>，我们就能求得观测序列出现的概率。</p><p>根据联合概率分布的公式：<spanclass="math inline"><em>P</em>(<em>X</em>=<em>x</em>,<em>Y</em>=<em>y</em>) = <em>P</em>(<em>X</em>=<em>x</em>)<em>P</em>(<em>Y</em>=<em>y</em>|<em>X</em>=<em>x</em>)</span>，可得<spanclass="math inline"><em>P</em>(<em>O</em>,<em>I</em>|<em>λ</em>)</span>的求解方法： <spanclass="math display"><em>P</em>(<em>O</em>,<em>I</em>|<em>λ</em>) = <em>P</em>(<em>I</em>|<em>λ</em>)<em>P</em>(<em>O</em>|<em>I</em>,<em>λ</em>)</span>其中，<span class="math inline"><em>P</em>(<em>I</em>|<em>λ</em>)</span>是在给定模型下，一个隐藏序列出现的概率，即 <spanclass="math inline"><em>P</em>(<em>I</em>|<em>λ</em>) = <em>P</em>(<em>i</em><sub>1</sub>,<em>i</em><sub>2</sub>,...,<em>i</em><sub><em>n</em></sub>|<em>λ</em>) = <em>P</em>(<em>i</em><sub>1</sub>|<em>λ</em>)<em>P</em>(<em>i</em><sub>2</sub>|<em>λ</em>)...<em>P</em>(<em>i</em><sub><em>n</em></sub>|<em>λ</em>)</span>。那么怎么求<spanclass="math inline"><em>P</em>(<em>i</em><sub><em>n</em></sub>|<em>λ</em>)</span>？别忘了状态转移概率矩阵<span class="math inline"><em>A</em></span> 的存在，<spanclass="math inline"><em>A</em></span>所记录的不就是隐藏状态之间转换的概率吗？所以可得： <spanclass="math display"><em>P</em>(<em>I</em>|<em>λ</em>) = <em>π</em><sub><em>i</em><sub>1</sub></sub><em>a</em><sub><em>i</em><sub>1</sub><em>i</em><sub>2</sub></sub><em>a</em><sub><em>i</em><sub>2</sub><em>i</em><sub>3</sub></sub>...<em>a</em><sub><em>i</em><sub><em>t</em> − 1</sub><em>i</em><sub><em>t</em></sub></sub></span>接下来要求的就是 <spanclass="math inline"><em>P</em>(<em>O</em>|<em>I</em>,<em>λ</em>)</span>，它的含义是：在给定模型下，当隐藏序列为<span class="math inline"><em>I</em></span> 时，观测序列为 <spanclass="math inline"><em>O</em></span> 的概率。求解 <spanclass="math inline"><em>P</em>(<em>O</em>|<em>I</em>,<em>λ</em>)</span>的方法和求解 <spanclass="math inline"><em>P</em>(<em>I</em>|<em>λ</em>)</span>的方法是一样的，还记得观测状态概率矩阵 <spanclass="math inline"><em>B</em></span> 吗？<spanclass="math inline"><em>B</em></span>记录的正是从隐藏序列到观测序列转换的概率，所以 <spanclass="math inline"><em>P</em>(<em>O</em>|<em>I</em>,<em>λ</em>)</span>的计算方法如下： <spanclass="math display"><em>P</em>(<em>O</em>|<em>I</em>,<em>λ</em>) = <em>b</em><sub><em>i</em><sub>1</sub><em>o</em><sub>1</sub></sub><em>b</em><sub><em>i</em><sub>2</sub><em>o</em><sub>2</sub></sub>...<em>b</em><sub><em>i</em><sub><em>t</em></sub><em>o</em><sub><em>t</em></sub></sub></span>于是，我们只需要将上面两个式子乘在一起，就能得到 <spanclass="math inline"><em>P</em>(<em>O</em>,<em>I</em>|<em>λ</em>)</span>了： <spanclass="math display"><em>P</em>(<em>O</em>,<em>I</em>|<em>λ</em>) = <em>π</em><sub><em>i</em><sub>1</sub></sub><em>a</em><sub><em>i</em><sub>1</sub><em>i</em><sub>2</sub></sub><em>b</em><sub><em>i</em><sub>1</sub><em>o</em><sub>1</sub></sub><em>a</em><sub><em>i</em><sub>2</sub><em>i</em><sub>3</sub></sub><em>b</em><sub><em>i</em><sub>2</sub><em>o</em><sub>2</sub></sub>...<em>a</em><sub><em>i</em><sub><em>t</em> − 1</sub><em>i</em><sub><em>t</em></sub></sub><em>b</em><sub><em>i</em><sub><em>t</em></sub><em>o</em><sub><em>t</em></sub></sub></span>则观测序列 <span class="math inline"><em>O</em></span> 出现的概率为：<spanclass="math display"><em>P</em>(<em>O</em>|<em>λ</em>) = ∑<sub><em>I</em></sub><em>P</em>(<em>O</em>,<em>I</em>|<em>λ</em>) = ∑<sub><em>i</em><sub>1</sub>, <em>i</em><sub>2</sub>, ..., <em>i</em><sub><em>T</em></sub></sub><em>π</em><sub><em>i</em><sub>1</sub></sub><em>a</em><sub><em>i</em><sub>1</sub><em>i</em><sub>2</sub></sub><em>b</em><sub><em>i</em><sub>1</sub><em>o</em><sub>1</sub></sub><em>a</em><sub><em>i</em><sub>2</sub><em>i</em><sub>3</sub></sub><em>b</em><sub><em>i</em><sub>2</sub><em>o</em><sub>2</sub></sub>...<em>a</em><sub><em>i</em><sub><em>t</em> − 1</sub><em>i</em><sub><em>T</em></sub></sub><em>b</em><sub><em>i</em><sub><em>t</em></sub><em>o</em><sub><em>T</em></sub></sub></span>解释一下上面的公式：我们要求的是在给定模型下，某一观测序列出现的概率。暴力求解的方法找出所有可能的隐藏序列，将由这些隐藏序列得到该观测序列的概率全部加起来，最终得到该观测序列的概率。假设隐藏状态数有<span class="math inline"><em>N</em></span>个，我们需要遍历每一个隐藏序列，序列的长度为观测状态数 <spanclass="math inline"><em>T</em></span>，所以可能的隐藏序列有 <spanclass="math inline"><em>N</em><sup><em>T</em></sup></span>种，而对于每一个序列，都要遍历其 <spanclass="math inline"><em>T</em></span> 个 <spanclass="math inline"><em>a</em><sub><em>i</em></sub></span> 和 <spanclass="math inline"><em>b</em><sub><em>i</em></sub></span>，加起来就是<spanclass="math inline">2<em>T</em></span>，计算时间复杂度时省去系数，则该算法的<strong>时间复杂度将达到<spanclass="math inline"><em>O</em>(<em>T</em><em>N</em><sup><em>T</em></sup>)</span></strong>。</p><h2 id="前向算法">04 前向算法</h2><h3 id="算法解析">4.1 算法解析</h3><p>暴力求解法告诉我们隐马尔可夫模型的问题看上去是可解，但高昂的时间开销却是不可承受的。对此，有人提出了前向算法，该算法利用<strong>动态规划</strong>的思想来求解该问题，降低了时间复杂度。</p><p>给定 <span class="math inline"><em>t</em></span> 时刻的隐藏状态为<spanclass="math inline"><em>q</em><sub><em>i</em></sub></span>（注意，这里的<span class="math inline"><em>i</em></span>是指一种<u>具体</u>的隐藏状态，例如晴天、雨天等，是固定好的），观测序列为<spanclass="math inline"><em>o</em><sub>1</sub>, <em>o</em><sub>2</sub>, ...<em>o</em><sub><em>n</em></sub></span>的概率叫做<strong>前向概率</strong>，定义为： <spanclass="math display"><em>α</em><sub><em>t</em></sub>(<em>i</em>) = <em>P</em>(<em>o</em><sub>1</sub>,<em>o</em><sub>2</sub>,...,<em>o</em><sub><em>t</em></sub>,<em>S</em><sub><em>t</em></sub>=<em>q</em><sub><em>i</em></sub>|<em>λ</em>)</span>换句话来讲，前向概率就是在给定某一观测序列的情况下，某一时刻的状态刚刚好为<span class="math inline"><em>q</em><sub><em>i</em></sub></span>的概率。</p><p>则，当 <span class="math inline"><em>t</em> = <em>T</em></span>时，<spanclass="math inline"><em>α</em><sub><em>T</em></sub>(<em>i</em>) = <em>P</em>(<em>o</em><sub>1</sub>,<em>o</em><sub>2</sub>,...,<em>o</em><sub><em>T</em></sub>,<em>S</em><sub><em>T</em></sub>=<em>q</em><sub><em>i</em></sub>|<em>λ</em>)</span>表示最后一个时刻，隐藏状态为状态 <spanclass="math inline"><em>q</em><sub><em>i</em></sub></span>并且得到观察序列为 <spanclass="math inline"><em>o</em><sub>1</sub>, <em>o</em><sub>2</sub>, ..., <em>o</em><sub><em>T</em></sub></span>的概率。现在我们回来思考一下我们要解决的最原始的问题是什么，应该是 <spanclass="math inline"><em>P</em>(<em>O</em>|<em>λ</em>) = <em>P</em>(<em>o</em><sub>1</sub>,<em>o</em><sub>2</sub>,...<em>o</em><sub><em>T</em></sub>|<em>λ</em>)</span>，而<spanclass="math inline"><em>α</em><sub><em>T</em></sub>(<em>i</em>)</span>和它相比，末尾多了个 <spanclass="math inline"><em>S</em><sub><em>T</em></sub> = <em>q</em><sub><em>i</em></sub></span>，也就是说<spanclass="math inline"><em>α</em><sub><em>T</em></sub>(<em>i</em>)</span>还要求最终的隐藏状态必须为 <spanclass="math inline"><em>q</em><sub><em>i</em></sub></span>，貌似和原本的问题相比有点画蛇添足，但仔细想想，如果我们能把所有最终可能的隐藏状态都拿过来，求<spanclass="math inline"><em>α</em><sub><em>T</em></sub>(1) + <em>α</em><sub><em>T</em></sub>(2) +  ·  ·  ·  + <em>α</em><sub><em>T</em></sub>(<em>n</em>)</span>，那不就大功告成了？所以现在的问题就变成了如何求解<span class="math inline"><em>T</em></span>时刻的前向概率，这是一个动态规划的问题。</p><p>在第一个时刻：<spanclass="math inline"><em>α</em><sub>1</sub>(<em>i</em>) = <em>P</em>(<em>o</em><sub>1</sub>,<em>S</em><sub>1</sub>=<em>q</em><sub><em>i</em></sub>|<em>λ</em>)</span>表示第一时刻的隐藏状态为 <spanclass="math inline"><em>q</em><sub><em>i</em></sub></span>，观测序列为<span class="math inline"><em>o</em><sub>1</sub></span>的概率，其结果为（这里的 <spanclass="math inline"><em>b</em><sub><em>i</em></sub>(<em>o</em><sub>1</sub>)</span>就表示由隐藏状态 <spanclass="math inline"><em>q</em><sub><em>i</em></sub></span>转换为观测状态 <span class="math inline"><em>o</em><sub>1</sub></span>的概率，是矩阵 <span class="math inline"><em>B</em></span> 的元素）：<spanclass="math display"><em>α</em><sub>1</sub>(<em>i</em>) = <em>π</em><sub><em>i</em></sub><em>b</em><sub><em>i</em></sub>(<em>o</em><sub>1</sub>)</span>在第 <span class="math inline"><em>t</em></span> 时刻，隐藏状态变成了<span class="math inline"><em>q</em><sub><em>j</em></sub></span>（这里的<span class="math inline"><em>q</em><sub><em>j</em></sub></span>不是具体状态，是任意状态都可以），<spanclass="math inline"><em>t</em> + 1</span> 时刻隐藏状态变成了为 <spanclass="math inline"><em>q</em><sub><em>i</em></sub></span>，此时，隐藏状态由<span class="math inline"><em>q</em><sub><em>j</em></sub></span> 变成<span class="math inline"><em>q</em><sub><em>i</em></sub></span>的概率可由矩阵 <span class="math inline"><em>A</em></span> 得到，值为<spanclass="math inline"><em>a</em><sub><em>j</em><em>i</em></sub></span>，而<span class="math inline"><em>q</em><sub><em>j</em></sub></span>可以是任何一种状态，我们都得考虑进去，所以我们得遍历一遍所有隐藏状态，然后相加，即<spanclass="math inline">∑<sub><em>j</em></sub><em>α</em><sub><em>t</em></sub>(<em>j</em>)</span>，所以有：<spanclass="math display"><em>α</em><sub><em>t</em> + 1</sub>(<em>i</em>) = [∑<sub><em>j</em></sub><em>α</em><sub><em>t</em></sub>(<em>j</em>)<em>a</em><sub><em>i</em><em>j</em></sub>]<em>b</em><sub><em>i</em></sub>(<em>o</em><sub><em>t</em> + 1</sub>)</span>如果这个式子看上去还是太麻烦，我们可以拆开来看：<spanclass="math inline"><em>α</em><sub><em>t</em></sub>(<em>j</em>)</span>表示的是前一时刻隐藏状态为 <spanclass="math inline"><em>q</em><sub><em>j</em></sub></span> 的概率，<spanclass="math inline"><em>a</em><sub><em>i</em><em>j</em></sub></span>表示由隐藏状态 <spanclass="math inline"><em>q</em><sub><em>j</em></sub></span> 转换为 <spanclass="math inline"><em>q</em><sub><em>i</em></sub></span>的概率，相乘就是前一时刻的隐藏状态恰好为 <spanclass="math inline"><em>q</em><sub><em>j</em></sub></span>，并且由 <spanclass="math inline"><em>q</em><sub><em>j</em></sub></span> 能转换到<span class="math inline"><em>q</em><sub><em>i</em></sub></span>的概率，由于得考虑全部 <spanclass="math inline"><em>q</em><sub><em>j</em></sub></span>的情况，所以得遍历求和；后面的 <spanclass="math inline"><em>b</em><sub><em>i</em></sub>(<em>o</em><sub><em>t</em> + 1</sub>)</span>则是由隐藏状态 <spanclass="math inline"><em>q</em><sub><em>i</em></sub></span>转换到观测状态 <spanclass="math inline"><em>o</em><sub><em>t</em> + 1</sub></span>的概率，将它与前一部分相乘，就得到了前一时刻的隐藏状态恰好为 <spanclass="math inline"><em>q</em><sub><em>j</em></sub></span>，并且由 <spanclass="math inline"><em>q</em><sub><em>j</em></sub></span> 能转换到<span class="math inline"><em>q</em><sub><em>i</em></sub></span>，又由<span class="math inline"><em>q</em><sub><em>i</em></sub></span> 得到<span class="math inline"><em>o</em><sub><em>t</em> + 1</sub></span>的概率。</p><p>则最终结果就是： <spanclass="math display"><em>P</em>(<em>O</em>|<em>λ</em>) = ∑<sub><em>i</em></sub><em>α</em><sub><em>i</em></sub>(<em>T</em>)</span>计算一下前向算法的时间复杂度：一共要计算 <spanclass="math inline"><em>T</em></span> 次 <spanclass="math inline"><em>α</em></span>，每次计算 <spanclass="math inline"><em>α</em></span> 的时间复杂度为 <spanclass="math inline"><em>N</em><sup>2</sup></span>（原因很简单，自己想），所以前向算法的<strong>时间复杂度为 <spanclass="math inline"><em>O</em>(<em>T</em><em>N</em><sup>2</sup>)</span></strong>。显然，前向算法将暴力算法的时间复杂度从指数级降到了线性级别，极大提升了执行效率。</p><h3 id="公式推导">4.2 公式推导</h3><p>由上述过程，我们可以得到前向算法的递推式：</p><ol type="1"><li>初值：</li></ol><p><spanclass="math display"><em>α</em><sub>1</sub>(<em>i</em>) = <em>π</em><sub><em>i</em></sub><em>b</em><sub><em>i</em></sub>(<em>o</em><sub>1</sub>)，<em>i</em> = 1, 2, ..., <em>N</em></span></p><ol start="2" type="1"><li>递推：</li></ol><p><span class="math display">$$\alpha_{t+1}(i)=[\sum_{j=1}^N\alpha_t(j)a_{ji}]b_i(o_{t+1})，i=1,2,...,N$$</span></p><ol start="3" type="1"><li>终止：</li></ol><p><span class="math display">$$P(O|\lambda)=\sum_{i=1}^N\alpha_T(i)$$</span></p><p>接下来，我们对每一个公式进行推导。</p><p>首先，我们要求解的目标是 <spanclass="math inline"><em>P</em>(<em>O</em>|<em>λ</em>) = ∑<sub><em>I</em></sub><em>P</em>(<em>I</em>,<em>O</em>|<em>λ</em>)</span>，而<spanclass="math inline"><em>α</em><sub><em>T</em></sub>(<em>i</em>) = <em>P</em>(<em>O</em>,<em>S</em><sub><em>T</em></sub>=<em>q</em><sub><em>i</em></sub>|<em>λ</em>)</span>，所以对于终止公式有：<span class="math display">$$\begin{aligned}P(O|\lambda)&amp;=\sum_{I}P(I,O|\lambda)\\&amp;=\sum_{i=1}^NP(o_1,o_2,...,o_T,S_T=q_i|\lambda)\\&amp;=\sum_{i=1}^N\alpha_T(i)\end{aligned}$$</span> 对于递推公式则有： <span class="math display">$$\begin{aligned}\alpha_{t+1}(i)&amp;=P(o_1,o_2,...,o_{t+1},S_{t+1}=q_i|\lambda)\\&amp;=P(S_{t+1}=q_i|\lambda)P(o_1,o_2,...,o_{t+1}|S_{t+1}=q_i,\lambda)\\&amp;=[\sum_{j=1}^NP(S_{t+1}=q_i,S_t=q_j|\lambda)]P(o_1,o_2,...,o_{t+1}|S_{t+1}=q_i,\lambda)\\&amp;=[\sum_{j=1}^NP(S_t=q_j|\lambda)P(S_{t+1}=q_i|S_t=q_j,\lambda)]P(o_1,o_2,...,o_{t+1}|S_{t+1}=q_i,\lambda)\\&amp;=[\sum_{j=1}^NP(o_1,...,o_t,S_t=q_j|\lambda)P(S_{t+1}=q_i|S_t=q_j,\lambda)]P(o_1,o_2,...,o_{t+1}|S_{t+1}=q_i,\lambda)\\&amp;=[\sum_{j=1}^N\alpha_t(j)a_{ji}]b_i(o_{t+1})\end{aligned}$$</span></p><p>对于初值有： <span class="math display">$$\begin{aligned}\alpha_1(i)&amp;=P(o_1,S_1=q_i|\lambda)\\&amp;=P(S_1=q_i|\lambda)P(o_1|S_1=q_i,\lambda)\\&amp;=\pi_1b_i(o_1)\end{aligned}$$</span></p><h2 id="后向算法">05 后向算法</h2><p>后向算法比前向算法稍微复杂一点，这一节着重讲解后向算法初值、递推和终止公式的推导。</p><p>给定隐马尔可夫模型，定义在时刻 <spanclass="math inline"><em>t</em></span> 状态为 <spanclass="math inline"><em>q</em><sub><em>i</em></sub></span> 的条件下，从<span class="math inline"><em>t</em> + 1</span> 到 <spanclass="math inline"><em>T</em></span> 的部分观测序列为 <spanclass="math inline"><em>o</em><sub><em>t</em> + 1</sub>, <em>o</em><sub><em>t</em> + 2</sub>, ..., <em>o</em><sub><em>T</em></sub></span>的概率称为<strong>后向概率</strong>，记作： <spanclass="math display"><em>β</em><sub><em>t</em></sub>(<em>i</em>) = <em>P</em>(<em>o</em><sub><em>t</em> + 1</sub>,<em>o</em><sub><em>t</em> + 2</sub>,...,<em>o</em><sub><em>T</em></sub>|<em>S</em><sub><em>t</em></sub>=<em>q</em><sub><em>i</em></sub>,<em>λ</em>)</span>观察后向概率的公式和定义，我们可以用另一种方法描述后向概率：当前时刻为<span class="math inline"><em>T</em></span>，也就是终止时刻，前 <spanclass="math inline"><em>T</em> − <em>t</em></span> 个时刻的观测序列为<spanclass="math inline"><em>o</em><sub><em>t</em> + 1</sub>, <em>o</em><sub><em>t</em> + 2</sub>, ..., <em>o</em><sub><em>T</em></sub></span>，且<span class="math inline"><em>t</em></span> 时刻隐藏状态恰好为 <spanclass="math inline"><em>q</em><sub><em>i</em></sub></span>的概率。可以发现，后向概率是以终止时刻为起点，倒退回去考虑的，与前向概率正好相反，所以递推的起点是<spanclass="math inline"><em>β</em><sub><em>T</em></sub>(<em>i</em>) = <em>P</em>(∅|<em>S</em><sub><em>T</em></sub>=<em>q</em><sub><em>i</em></sub>,<em>λ</em>)</span>，可以发现，当<span class="math inline"><em>t</em> = <em>T</em></span>时，已不存在后续观测序列，所以我们规定 <spanclass="math inline"><em>β</em><sub><em>T</em></sub>(<em>i</em>) = 1</span>。</p><p>我们要求解的是 <spanclass="math inline"><em>P</em>(<em>O</em>|<em>λ</em>) = <em>P</em>(<em>o</em><sub>1</sub>,<em>o</em><sub>2</sub>,...<em>o</em><sub><em>T</em></sub>|<em>λ</em>)</span>，后向算法递推的终点是序列的起始点，也就是<span class="math inline"><em>t</em> = 1</span>，而 <spanclass="math inline"><em>β</em><sub>1</sub>(<em>i</em>) = <em>P</em>(<em>o</em><sub>2</sub>,<em>o</em><sub>3</sub>,...,<em>o</em><sub><em>T</em></sub>|<em>S</em><sub><em>t</em></sub>=<em>q</em><sub><em>i</em></sub>,<em>λ</em>)</span>，这之间又要怎么转换？这就是后向算法比前向算法复杂的点，它并不像前向算法那样容易推导。首先，我们使用全概率公式和条件概率公式对<span class="math inline"><em>P</em>(<em>O</em>|<em>λ</em>)</span>进行变换： <span class="math display">$$\begin{aligned}P(O|\lambda)&amp;=\sum_{i=1}^{N}P(o_1,o_2,...,o_T,S_1=q_i|\lambda)\\&amp;=\sum_{i=1}^{N}P(o_1,o_2,...,o_T|S_1=q_i,\lambda)P(S_1=q_i|\lambda)\\&amp;=\sum_{i=1}^{N}P(o_1|o_2,...,o_T,S_1=q_i,\lambda)P(o_2,...,o_T|S_1=q_i,\lambda)\pi_i\\&amp;=\sum_{i=1}^Nb_i(o_1)\beta_1(i)\pi_i\end{aligned}$$</span> 经过上面的推导，我们就能发现 <spanclass="math inline"><em>P</em>(<em>O</em>|<em>λ</em>)</span> 和 <spanclass="math inline"><em>β</em><sub>1</sub>(<em>i</em>)</span>的联系。下一个要解决的就是 <spanclass="math inline"><em>β</em><sub><em>t</em></sub>(<em>i</em>)</span>的推导了，首先令 <spanclass="math inline"><em>β</em><sub><em>t</em> + 1</sub>(<em>j</em>) = <em>P</em>(<em>o</em><sub><em>t</em> + 2</sub>,<em>o</em><sub><em>t</em> + 3</sub>,...<em>o</em><sub><em>T</em></sub>|<em>S</em><sub><em>t</em> + 1</sub>=<em>q</em><sub><em>j</em></sub>,<em>λ</em>)</span>，其推导过程如下：<span class="math display">$$\begin{aligned}\beta_t(i)&amp;=P(o_{t+1},o_{t+2},...,o_T|S_t=q_i,\lambda)\\&amp;=\sum_{j=1}^{N}P(o_{t+1},o_{t+2},...,o_T,S_{t+1}=q_j|S_t=q_i,\lambda)\\&amp;=\sum_{j=1}^{N}P(o_{t+1},...,o_T|S_t=q_i,S_{t+1}=q_j,\lambda)P(S_{t+1}=q_j|S_t=q_i,\lambda)\\&amp;=\sum_{j=1}^{N}P(o_{t+1},...,o_T|S_{t+1}=q_j,\lambda)P(S_{t+1}=q_j|S_t=q_i,\lambda)\\&amp;=\sum_{j=1}^{N}P(o_{t+1}|o_{t+2},...,o_T,S_{t+1}=q_j,\lambda)P(o_{t+2},...,o_T|S_{t+1}=q_j,\lambda)P(S_{t+1}=q_j|S_t=q_i,\lambda)\\&amp;=\sum_{j=1}^N\beta_{t+1}(j)b_j(o_{t+1})a_{ij}\end{aligned}$$</span> 至此，我们就得到后向算法中的初值、递推和终止公式：</p><ol type="1"><li>初值：</li></ol><p><spanclass="math display"><em>β</em><sub><em>T</em></sub>(<em>i</em>) = 1</span></p><ol start="2" type="1"><li>递推：</li></ol><p><span class="math display">$$\beta_t(i)=\sum_{j=1}^N\beta_{t+1}(j)b_j(o_{t+1})a_{ij}$$</span></p><ol start="3" type="1"><li>终止：</li></ol><p><span class="math display">$$P(O|\lambda)=\sum_{i=1}^Nb_i(o_1)\beta_1(i)\pi_i$$</span></p><h2 id="baum-welch-算法">06 Baum-Welch 算法</h2><p>讨论完了如何求解 <spanclass="math inline"><em>P</em>(<em>O</em>|<em>λ</em>)</span>，下一步我们就要考虑最难的一个问题——如何求解HMM 的参数，即 <span class="math inline"><em>A</em></span>，<spanclass="math inline"><em>B</em></span>，<spanclass="math inline"><em>Π</em></span>。</p><p>如果是不加任何限制地考虑这个问题，那其实是很简单的。根据<strong>大数定理</strong>：在试验次数足够多的情况下，频数就等于概率。要想得到<span class="math inline"><em>A</em></span> 和 <spanclass="math inline"><em>B</em></span>，只需要对数据进行统计，计算每种状态出现的频数就行了，于是就有：<span class="math display">$$\begin{aligned}&amp;\hat{a}_{ij}=\frac{A_{ij}}{\sum_{j=1}^{N}A_{ij}}&amp;,i=1,2,...,N,j=1,2,...,N\\&amp;\hat{b}_{j}(k)=\frac{B_{jk}}{\sum_{k=1}^{M}B_{jk}}&amp;,j=1,2,...,N,k=1,2,...,M\\\end{aligned}$$</span> 解释一下取值范围：<span class="math inline"><em>A</em></span>是状态转移概率矩阵，这是隐藏状态和隐藏状态之间转移的概率，所以 <spanclass="math inline"><em>i</em></span> 和 <spanclass="math inline"><em>j</em></span> 的最大值都是隐藏状态的数量 <spanclass="math inline"><em>N</em></span>；而 <spanclass="math inline"><em>B</em></span>是生成观测状态概率矩阵，这是隐藏状态到观测状态之间转移的概率，令 <spanclass="math inline"><em>j</em></span>代表隐藏状态，其最大值就是隐藏状态的数量 <spanclass="math inline"><em>N</em></span>，<spanclass="math inline"><em>k</em></span>代表观测状态，其最大值就是观测状态的数量 <spanclass="math inline"><em>M</em></span>，我们之前讲到过，HMM中的隐藏状态和观测状态数量不一定要相同，所以 <spanclass="math inline"><em>N</em></span> 不一定等于 <spanclass="math inline"><em>M</em></span>。</p><p>至于 <span class="math inline"><em>Π</em></span>也很简单，根据往期数据计算就行了。所以如果只是像这样单纯地求解 HMM的参数，只要有数据，那就几乎是没有难度的。但我们来考虑一下求解目标中的第二个：给定观测序列<spanclass="math inline"><em>O</em> = {<em>o</em><sub>1</sub>, <em>o</em><sub>2</sub>, ..., <em>o</em><sub><em>t</em></sub>}</span>，求解参数<span class="math inline">(<em>A</em>,<em>B</em>,<em>Π</em>)</span> 使得<span class="math inline"><em>P</em>(<em>O</em>|<em>λ</em>)</span>最大。这要怎么做？</p><p>之前的讨论是在所有数据均有的情况下进行的，也就是隐藏状态序列 <spanclass="math inline"><em>I</em></span> 和观测状态序列 <spanclass="math inline"><em>O</em></span> 均已知的情况下，但现在只有观测序列<span class="math inline"><em>O</em></span>，要我们求最优的参数 <spanclass="math inline">(<em>A</em>,<em>B</em>,<em>Π</em>)</span>，使 <spanclass="math inline"><em>P</em>(<em>O</em>|<em>λ</em>)</span>最大。也就是说 <span class="math inline"><em>I</em></span>被隐藏了，这相当于是一个含隐变量的参数估计问题，需要 EM 算法来解决。</p><p>EM 算法应用到 HMM 中时通常被称为 Baum-Welch 算法，Baum-Welch 算法是EM 算法的一个特例。</p>]]></content>
      
      
      <categories>
          
          <category> machine learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> lecture notes </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>支持向量机</title>
      <link href="/2023/12/11/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/"/>
      <url>/2023/12/11/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/</url>
      
        <content type="html"><![CDATA[<p><strong>支持向量机（support vector machines,SVM）</strong>是一种二分类模型，它的基本模型是定义在特征空间上的<strong>间隔最大的线性分类器</strong>，间隔最大使它有别于感知机；SVM还包括<strong>核技巧</strong>，这使它成为实质上的非线性分类器。SVM的的学习策略就是间隔最大化，可形式化为一个求解凸二次规划的问题，也等价于正则化的合页损失函数的最小化问题。SVM的的学习算法就是求解凸二次规划的最优化算法。</p><span id="more"></span><h2 id="线性模型">01 线性模型</h2><h3 id="算法思路">1.1 算法思路</h3><p>假设训练样本在空间中的分布如下左图分布，圆圈和星星分别代表两类不同类别的数据，那么我们能找出一条直线，将两者分割开，我们就称这样的训练样本集为一个<strong>线性可分（LinearSeparable）样本集</strong>，这样的模型就被称为<strong>线性模型</strong>；同理，倘若我们找不到这样一条直线，将两者完全分离开，如下右图所示，则称这样的训练样本集为一个<strong>线性不可分（Non-LinearSeparable）样本集</strong>，这样的模型就被称为<strong>非线性模型</strong>。</p><p><img src="/img/支持向量机-01.png" /></p><p><strong>支持向量机</strong>算法的思路大致是这样的：首先讨论如何在线性可分的训练样本集上找一条直线将样本分开，然后想办法将这样的方法推广到线性不可分的训练样本集上。所以，我们首先讨论第一部分：如何找到一条直线将线性可分训练样本集分开。</p><p>对于一个训练样本集，可以证明：<u>只要存在一条直线可以将样本集分开，就肯定存在无数条直线能将该样本集分开</u>（如下图所示）。既然如此，支持向量机提出的第一个问题就是：哪条直线是最好的？</p><p><img src="/img/支持向量机-02.png" /></p><p>通过直觉来判断，我们也可以感受出上图中的红色直线应该是最好的，问题是为什么？要解答这个问题，我们就必须定义一种性能指标（PerformaceMeasure），来评估每一条直线的好坏。</p><p>为了给出这个性能指标，支持向量机做的事情是，将上面的红线向左右两边平行移动，直到这条线碰到一个或几个样本点为止（如下图中两条虚线所示）：</p><p><img src="/img/支持向量机-03.png" /></p><p>然后，支持向量机给出了这个性能指标的定义，就是上图中两条虚线的距离（Gap），用<span class="math inline"><em>d</em></span>表示。而性能最好的那条线，就是能使 <spanclass="math inline"><em>d</em></span>最大的那条线。但是这样还不完善，要知道，能使 <spanclass="math inline"><em>d</em></span>最大的线也不唯一，将上图中的实线左右移动，作一条平行线，只要平行线不越过两条虚线所界定的范围，<spanclass="math inline"><em>d</em></span>就是不变的，所以还得给出另一个限制条件：直线必须位于两根平行线的正中间，也就是使上图中的实线与左右两根平行虚线的距离分别为<span class="math inline">$\frac d2$</span>。</p><p><img src="/img/支持向量机-04.png" /></p><h3 id="数学描述">1.2 数学描述</h3><p>既然性能指标已经确定了，下一个问题就是如何描述这个优化过程了。在描述优化过程之前，我们还是先得给出一些定义，首先，我们将上面的<span class="math inline"><em>d</em></span>称为<strong>间隔（Margin）</strong>，将虚线穿过的向量称为<strong>支持向量（SupportVectors）</strong>。通过上面对支持向量机算法的简单描述，我们可以发现，支持向量机找到的最优直线，只与支持向量有关，与其他向量无关，这就是为什么支持向量机也能用在小样本的数据上。</p><p>先给出线性模型的数学描述：</p><ol type="1"><li><p>定义训练数据及标签为 <spanclass="math inline">(<em>X</em><sub>1</sub>,<em>y</em><sub>1</sub>)、(<em>X</em><sub>2</sub>,<em>y</em><sub>2</sub>)...(<em>X</em><sub><em>n</em></sub>,<em>y</em><sub><em>n</em></sub>)</span>，其中，<spanclass="math inline"><em>X</em></span>是样本的特征，在上面给出的例子里，每个样本的特征是二维的，也就是说 <spanclass="math inline">$X=\left[ \begin{array}{} x_1 \\ x_2\end{array}\right]$</span> ，分别对应 x 轴和 y 轴；而 <spanclass="math inline"><em>y</em></span>是标签，在上面这个二分类问题里，标签只有两种，所以 <spanclass="math inline"><em>y</em> ∈ { − 1, 1}</span>。</p></li><li><p>我们定义一个线性模型为 <spanclass="math inline">(<em>W</em>,<em>b</em>)</span>，其中 <spanclass="math inline"><em>W</em></span> 是一个向量，其维数与特征向量 <spanclass="math inline"><em>X</em></span> 一致，<spanclass="math inline"><em>b</em></span>是一个常数，一个线性模型确定一个<strong>超平面（Hyperplane）</strong>，所谓超平面就是指划分空间的平面，超平面在二维空间里表现为我们上面所说的那条划分样本点的直线，而在更高的维度里就是一个平面，故称之为超平面。超平面由<span class="math inline"><em>W</em></span> 和 <spanclass="math inline"><em>b</em></span> 确定，其方程为 <spanclass="math inline"><em>W</em><sup><em>T</em></sup><em>X</em> + <em>b</em> = 0</span>。机器学习的目标，就是通过所有样本的特征<span class="math inline"><em>X</em></span> 来找到一个 <spanclass="math inline"><em>W</em></span> 和 <spanclass="math inline"><em>b</em></span>，使能够确定一个超平面能划分所有的样本点。</p></li><li><p>一个训练集线性可分是指：对于 <spanclass="math inline">{(<em>X</em><sub><em>i</em></sub>,<em>y</em><sub><em>i</em></sub>)}<sub><em>i</em> = 1 ∼ <em>N</em></sub></span>，<spanclass="math inline">$\exist(W,b)$</span>，使 <spanclass="math inline">∀<em>i</em> = 1 ∼ <em>N</em></span>，有：</p><ol type="1"><li>若 <spanclass="math inline"><em>y</em><sub><em>i</em></sub> =  + 1</span>，则<spanclass="math inline"><em>W</em><sup><em>T</em></sup><em>X</em><sub><em>i</em></sub> + <em>b</em> ≥ 0</span>；</li><li>若 <spanclass="math inline"><em>y</em><sub><em>i</em></sub> =  − 1</span>，则<spanclass="math inline"><em>W</em><sup><em>T</em></sup><em>X</em><sub><em>i</em></sub> + <em>b</em> &lt; 0</span>。</li></ol><p>当然，上述线性可分的定义是不唯一的，将 <spanclass="math inline">≥</span> 和 <span class="math inline">&lt;</span>换个位置也是一样。对于上面的定义，我们可以发现，凡是线性可分问题，一定存在<spanclass="math inline"><em>y</em><sub><em>i</em></sub>[<em>W</em><sup><em>T</em></sup><em>X</em><sub><em>i</em></sub>+<em>b</em>] ≥ 0</span>。</p></li></ol><p>接下来给出支持向量机优化问题的数学描述：</p><ol type="1"><li>目标：最小化 <spanclass="math inline">∥<em>W</em>∥<sup>2</sup></span></li><li>限制条件：<spanclass="math inline"><em>y</em><sub><em>i</em></sub>[<em>W</em><sup><em>T</em></sup><em>X</em><sub><em>i</em></sub>+<em>b</em>] ≥ 1  (<em>i</em>=1∼<em>N</em>)</span></li></ol><p>对于上面这两个公式，相信很多人第一眼是懵的，因为按我们之前的描述，支持向量机算法就是去找一条使<span class="math inline"><em>d</em></span>最大且位于正中间位置的直线，怎么数学公式看起来跟这个过程完全没关系呢？</p><p>要搞清楚这两个公式，我们得先弄清楚两个事实：</p><ol type="1"><li>事实一：<spanclass="math inline"><em>W</em><sup><em>T</em></sup><em>X</em> + <em>b</em> = 0</span>与 <spanclass="math inline"><em>a</em><em>W</em><sup><em>T</em></sup><em>X</em> + <em>a</em><em>b</em> = 0  (<em>a</em>∈<em>R</em><sup>+</sup>)</span>表示的是同一个平面。</li><li>事实二：向量 <span class="math inline"><em>X</em><sub>0</sub></span>到超平面 <spanclass="math inline"><em>W</em><sup><em>T</em></sup><em>X</em> + <em>b</em> = 0</span>的距离是 <span class="math inline">$d=\frac{|W^TX_0+b|}{\lVertW\rVert}$</span>。（不要慌，这其实就是高中学的点到平面的距离公式，以一维平面<spanclass="math inline"><em>w</em><sub>1</sub><em>x</em> + <em>w</em><sub>2</sub><em>y</em> + <em>b</em> = 0</span>，也就是直线为例，点<spanclass="math inline">(<em>x</em><sub>0</sub>,<em>y</em><sub>0</sub>)</span>到这条直线的距离就是 <spanclass="math inline">$d=\frac{|w_1x_0+w_2y_0+b|}{\sqrt{w_1^2+w_2^2}}$</span>，前面的那个公式只不是这个公式在高维情况下的推广）</li></ol><p>基于事实二，我们知道，支持向量机要做的事情，就是在 <spanclass="math inline"><em>X</em><sub>0</sub></span> 是支持向量的情况下，使<span class="math inline"><em>d</em></span> 最大。</p><p>基于事实一，我们知道，我们可以找到一个正实数 <spanclass="math inline"><em>a</em></span> 来缩放 <spanclass="math inline"><em>W</em></span> 和 <spanclass="math inline"><em>b</em></span>，即 <spanclass="math inline">(<em>W</em>,<em>b</em>) → (<em>a</em><em>W</em>,<em>a</em><em>b</em>)</span>，使<span class="math inline"><em>d</em></span> 公式的分子 <spanclass="math inline">|<em>W</em><sup><em>T</em></sup><em>X</em><sub>0</sub>+<em>b</em>| = 1</span>。这样的话，<spanclass="math inline"><em>d</em></span> 的公式就变成了 <spanclass="math inline">$d=\frac{1}{\lVertW\rVert}$</span>。看到这个公式，就能明白为什么支持向量机的优化目标是最小化<span class="math inline">∥<em>W</em>∥<sup>2</sup></span> 了，因为最小化<span class="math inline">∥<em>W</em>∥<sup>2</sup></span> 就是最大化<span class="math inline"><em>d</em></span>。</p><p>现在再来看限制条件，限制条件其实就是规定了，所有样本点到超平面的距离<spanclass="math inline"><em>W</em><sup><em>T</em></sup><em>X</em><sub><em>i</em></sub> + <em>b</em></span>，要么等于<span class="math inline"><em>d</em></span>（支持向量），要么大于 <spanclass="math inline"><em>d</em></span>（非支持向量）。至于为什么要再乘上一个 <spanclass="math inline"><em>y</em><sub><em>i</em></sub></span>，其实看<u>线性可分的定义</u>就知道了，乘上<span class="math inline"><em>y</em><sub><em>i</em></sub></span>是为了与线性可分的定义相统一。</p><blockquote><p>补充：</p><ul><li><p>为什么一定要使 <spanclass="math inline">|<em>W</em><sup><em>T</em></sup><em>X</em><sub>0</sub>+<em>b</em>| = 1</span>，<spanclass="math inline">|<em>W</em><sup><em>T</em></sup><em>X</em><sub>0</sub>+<em>b</em>| = 2</span>可不可以？可以，等于 1 还是等于 2 或是其他值都没有任何关系，这只取决于<span class="math inline"><em>a</em></span> 的大小，而 <spanclass="math inline"><em>a</em></span> 并不改变超平面。</p></li><li><p>对于任何线性可分样本集，一定能找到一个超平面分割所有样本点；反之，如果是线性不可分，那么将找不到任何一个能满足要求的<span class="math inline"><em>W</em></span> 和 <spanclass="math inline"><em>b</em></span>。</p></li><li><p>某些书上会将优化目标写成最小化 <spanclass="math inline">$\frac12\lVertW\rVert^2$</span>，这其实没有任何问题，加上 <spanclass="math inline">$\frac12$</span> 只是为了求导方便。</p></li><li><p>支持向量机要解决的问题其实是一个凸优化问题，而且是一个二次规划问题，二次规划问题的特点是：</p><ul><li>目标函数（Objective Function）是二次项；</li><li>限制条件是一次项。</li></ul><p>对于凸优化问题，要么无解，要么只有一个解。凸优化问题是计算机领域研究最多的问题，因为凸优化问题要么无解，要么只要能找到一个解，那便是它唯一的解。所以只要证明一个问题是凸优化问题，那么我们只要找到一个局部极值，也便找到了它的全局极值，我们便可认定这个问题已经被解决了。</p><p>非凸问题的目标函数图像是一条包含很多局部极值的曲线，会使得机器很容易落入局部最优解的陷阱。支持向量机算法优美的地方就在于，它将求解目标化成了一个凸优化问题。</p></li></ul></blockquote><h2 id="非线性模型">02 非线性模型</h2><h3 id="优化目标">2.1 优化目标</h3><p>之前已经讨论过，非线性模型不是线性可分的，也就是说找不到一个 <spanclass="math inline"><em>W</em></span> 和 <spanclass="math inline"><em>b</em></span>，使之确定一个能完美分割所有样本点的超平面，即限制条件<spanclass="math inline"><em>y</em><sub><em>i</em></sub>[<em>W</em><sup><em>T</em></sup><em>X</em><sub><em>i</em></sub>+<em>b</em>] ≥ 1  (<em>i</em>=1∼<em>N</em>)</span>是不可满足的，原本的优化目标是无解的。SVM处理非线性模型的方法其实不难理解，就是在线性模型的基础上引入了一个<strong>松弛变量（SlackVariable）</strong>，用 <spanclass="math inline"><em>ξ</em> (<em>ξ</em>≥0)</span>表示。新的优化目标如下：</p><ol type="1"><li>目标：最小化 <span class="math inline">$\frac12\lVertW\rVert^2+C\sum_{i=1}^N\xi_i$</span></li><li>限制条件：<ol type="1"><li><spanclass="math inline"><em>y</em><sub><em>i</em></sub>[<em>W</em><sup><em>T</em></sup><em>X</em><sub><em>i</em></sub>+<em>b</em>] ≥ 1 − <em>ξ</em><sub><em>i</em></sub>  (<em>i</em>=1∼<em>N</em>)</span></li><li><spanclass="math inline"><em>ξ</em><sub><em>i</em></sub> ≥ 0</span></li></ol></li></ol><p>可以发现，新的优化目标中，限制条件变成了 <spanclass="math inline"><em>y</em><sub><em>i</em></sub>[<em>W</em><sup><em>T</em></sup><em>X</em><sub><em>i</em></sub>+<em>b</em>] ≥ 1 − <em>ξ</em><sub><em>i</em></sub>  (<em>i</em>=1∼<em>N</em>)</span>，只要这个<span class="math inline"><em>ξ</em><sub><em>i</em></sub></span>取得足够大，那么大于等于号右边就会无限小，那么限制条件就有了满足的可能；但同时，也不能允许<span class="math inline"><em>ξ</em><sub><em>i</em></sub></span>无限大，不然就没有意义了，所以新的最小化目标函数的末尾还要加上 <spanclass="math inline"><em>ξ</em><sub><em>i</em></sub></span>。</p><p>接下来要明确，在上面的式子中，哪些是已知的，哪些是要求解的参数。显然，<spanclass="math inline"><em>X</em></span>和<spanclass="math inline"><em>y</em><sub><em>i</em></sub></span>是已知的，<spanclass="math inline"><em>W</em></span>、<spanclass="math inline"><em>b</em></span>以及<spanclass="math inline"><em>ξ</em></span>是要求的，但是这里还有个<spanclass="math inline"><em>C</em></span>，这个<spanclass="math inline"><em>C</em></span>是什么？<spanclass="math inline"><em>C</em></span>是一个由人事先设定的参数，这种参数一般称为<strong>超参数</strong>（<strong>Hyperparameter</strong>），作用是平衡<spanclass="math inline">$\frac{1}{2}\lVert\ W\rVert^{2}$</span>和<spanclass="math inline">$\sum_{i=1}^{N}\xi_{i}$</span>的权重。至于<spanclass="math inline"><em>C</em></span>具体取多少是没有定论的，一般是凭经验，选定一个区间，然后一个一个尝试。SVM很方便的一点就是，它只有这一个参数需要人来设置，但是在神经网络里，要去一个一个尝试的参数可能有很多。</p><h3 id="高维映射">2.2 高维映射</h3><p>虽然通过引入松弛变量，我们将非线性问题转换为了一个线性可分问题，但是还是存在一个问题，那就是求解目标的本质没有变，最后仍然是找出一条直线，来分割样本点，也就是说，即使一个样本集用肉眼看就能看出其能被一条简单的曲线分割，SVM还是会找一条直线来分割样本点，如下图所示：</p><p><img src="/img/支持向量机-05.png" /></p><p>这显然不是我们想要的。一些算法会很符合直觉地去找非直线来分割样本集，例如决策树是用矩形来分割，但是SVM的思想很精妙，它仍然是找直线，不过它不是在当前空间里去找，而是到高维空间里去找。它定义了一个<strong>高维映射</strong><span class="math inline"><em>ϕ</em>(<em>X</em>)</span>，通过 <spanclass="math inline"><em>ϕ</em></span>，能将 <spanclass="math inline"><em>X</em></span> 这个低维向量转化成一个高维向量<spanclass="math inline"><em>ϕ</em>(<em>X</em>)</span>。也就是说，也许在低维空间中，我们不容易去找一条直线能刚刚好分割所有样本点，那么我们就去高维空间中找，或许在高维空间中，我们就能找到样一条理想的直线了。</p><p>接下来我们用异或问题的例子，来具体解释这个过程为什么有效。异或问题是二维空间下最简单的非线性问题，其在二维空间中存在如下样本点分布：</p><p><img src="/img/支持向量机-06.png" /></p><p>我们先将图中四个样本点表示为 <spanclass="math inline"><em>X</em><sub>1</sub></span>、<spanclass="math inline"><em>X</em><sub>2</sub></span>、<spanclass="math inline"><em>X</em><sub>3</sub></span> 和 <spanclass="math inline"><em>X</em><sub>4</sub></span>，并且 <spanclass="math inline"><em>X</em><sub>1</sub></span> 和 <spanclass="math inline"><em>X</em><sub>2</sub></span> 属于一个类别 <spanclass="math inline"><em>C</em><sub>1</sub></span>，<spanclass="math inline"><em>X</em><sub>3</sub></span> 和 <spanclass="math inline"><em>X</em><sub>4</sub></span> 属于一个类别 <spanclass="math inline"><em>C</em><sub>2</sub></span>，有： <spanclass="math display">$$\begin{aligned}&amp;X_1=\left[ \begin{array}{} 0 \\ 0 \end{array} \right]\quadX_2=\left[ \begin{array}{} 1 \\ 1 \end{array} \right]\quad\in C_1\\&amp;X_3=\left[ \begin{array}{} 1 \\ 0 \end{array} \right]\quadX_4=\left[ \begin{array}{} 0 \\ 1 \end{array} \right]\quad\in C_2\\\end{aligned}$$</span> 定义高维映射函数为： <span class="math display">$$\phi(X):\quad X=\left[ \begin{array}{} a \\ b \end{array}\right]\overset{\phi}{\longrightarrow}\phi(X)=\left[ \begin{array}{} a^2\\ b^2 \\ a \\ b \\ ab \end{array} \right]$$</span> 则经过映射，四个样本点将变为： <span class="math display">$$\begin{aligned}&amp;\phi(X_1)=\left[ \begin{array}{} 0 \\ 0 \\ 0 \\ 0 \end{array}\right]\quad \phi(X_2)=\left[ \begin{array}{} 1 \\ 1 \\ 1 \\ 1\end{array} \right]\quad\in C_1\\&amp;\phi(X_3)=\left[ \begin{array}{} 1 \\ 0 \\ 1 \\ 0 \\ 0 \end{array}\right]\quad \phi(X_4)=\left[ \begin{array}{} 0 \\ 1 \\ 0 \\ 1 \\ 0\end{array} \right]\quad\in C_2\\\end{aligned}$$</span> 现在，<span class="math inline"><em>X</em></span>变成了五维向量，则 <span class="math inline"><em>W</em></span>也要变成五维向量，<span class="math inline"><em>b</em></span>仍然为常量，求解的目标就变成在五维空间中找一个超平面来分割四个样本点了。能做到分割的超平面不唯一，这里举一个例子：<span class="math display">$$W=\left[ \begin{array}{} -1 \\ -1 \\ -1 \\ -1 \\ 6 \end{array}\right]\quad b=1$$</span> 将样本点代入超平面的方程： <span class="math display">$$\begin{aligned}&amp;W^T\phi(X_1)+b=1&gt;0\\&amp;W^T\phi(X_2)+b=3&gt;0\\&amp;W^T\phi(X_3)+b=-1&lt;0\\&amp;W^T\phi(X_4)+b=-1&lt;0\\\end{aligned}$$</span> 如上，该超平面刚刚好把 <spanclass="math inline"><em>X</em><sub>1</sub></span>、<spanclass="math inline"><em>X</em><sub>2</sub></span> 与 <spanclass="math inline"><em>X</em><sub>3</sub></span>、<spanclass="math inline"><em>X</em><sub>4</sub></span>分开了。也就是说，在低维空间中线性不可分的样本集，可能在高维空间中就是线性可分的，这也就是我们要去升维的原因。关于这一点也有很多人研究过，它们的结论是，对于任何线性不可分的样本集，特征空间的维数越高，其被线性分割的概率也越大；若维数趋近无穷大，那么其被线性分割的概率将达到1.</p><h3 id="核函数">2.3 核函数</h3><p>在引入了高维映射之后，优化式 1 就变成了 <spanclass="math inline"><em>y</em><sub><em>i</em></sub>[<em>W</em><sup><em>T</em></sup><em>ϕ</em>(<em>X</em><sub><em>i</em></sub>)+<em>b</em>] ≥ 1 − <em>ξ</em><sub><em>i</em></sub>  (<em>i</em>=1∼<em>N</em>)</span>，虽然看起来只有<span class="math inline"><em>X</em><sub><em>i</em></sub></span>发生了变化，但不要忘记 <span class="math inline"><em>W</em></span>也跟着一起升维了。那么现在面临的问题就是：<u>如何选取 <spanclass="math inline"><em>ϕ</em></span></u> ？SVM 的回答是：无限维。</p><p>将特征空间增长到无限维，线性不可分问题就绝对可以变成线性可分。但是问题在于，当<span class="math inline"><em>ϕ</em>(<em>X</em>)</span>变成无限维，<span class="math inline"><em>W</em></span>也要变成无限维，那这个问题就没有办法做了。这也是 SVM巧妙的另一个地方，它在将特征空间映射到无限维的同时，又采用有限维的手段。</p><p>SVM 的意思是：我们可以不知道无限维映射 <spanclass="math inline"><em>ϕ</em>(<em>X</em>)</span>的显式表达，我们只要知道一个<strong>核函数（Kernel Function）</strong>：<spanclass="math display"><em>K</em>(<em>X</em><sub>1</sub>,<em>X</em><sub>2</sub>) = <em>ϕ</em>(<em>X</em><sub>1</sub>)<sup><em>T</em></sup><em>ϕ</em>(<em>X</em><sub>2</sub>)</span>则优化式 1 仍然可解。<spanclass="math inline"><em>K</em>(<em>X</em><sub>1</sub>,<em>X</em><sub>2</sub>)</span>其实计算的是 <spanclass="math inline"><em>ϕ</em>(<em>X</em><sub>1</sub>)</span> 和 <spanclass="math inline"><em>ϕ</em>(<em>X</em><sub>2</sub>)</span>的内积，虽然 <spanclass="math inline"><em>ϕ</em>(<em>X</em><sub>1</sub>)</span> 和 <spanclass="math inline"><em>ϕ</em>(<em>X</em><sub>2</sub>)</span>是无限维的，但是两者仍然能进行内积计算，得到的结果 <spanclass="math inline"><em>K</em>(<em>X</em><sub>1</sub>,<em>X</em><sub>2</sub>)</span>是一个数。</p><p>核函数的要求是：能将函数的形式最终拆成 <spanclass="math inline"><em>ϕ</em>(<em>X</em><sub>1</sub>)<sup><em>T</em></sup><em>ϕ</em>(<em>X</em><sub>2</sub>)</span>的形式。常用的核函数有如下几个：</p><ol type="1"><li>高斯核：<span class="math inline">$K(X_1,X_2)=e^{-\frac{\lVertX_1-X_2\rVert^2}{2\sigma^2}}=\phi(X_1)^T\phi(X_2)$</span>，<spanclass="math inline"><em>σ</em><sup>2</sup></span> 是方差。</li><li>多项式核：<spanclass="math inline"><em>K</em>(<em>X</em><sub>1</sub>,<em>X</em><sub>2</sub>) = (<em>X</em><sub>1</sub><sup><em>T</em></sup><em>X</em><sub>2</sub>+1)<sup><em>d</em></sup> = <em>ϕ</em>(<em>X</em><sub>1</sub>)<sup><em>T</em></sup><em>ϕ</em>(<em>X</em><sub>2</sub>)</span>，<spanclass="math inline"><em>d</em></span> 是多项式阶数。</li></ol><p>而能将核函数 <spanclass="math inline"><em>K</em>(<em>X</em><sub>1</sub>,<em>X</em><sub>2</sub>)</span>拆成 <spanclass="math inline"><em>ϕ</em>(<em>X</em><sub>1</sub>)<sup><em>T</em></sup><em>ϕ</em>(<em>X</em><sub>2</sub>)</span>的充要条件是：</p><ol type="1"><li><spanclass="math inline"><em>K</em>(<em>X</em><sub>1</sub>,<em>X</em><sub>2</sub>) = <em>K</em>(<em>X</em><sub>2</sub>,<em>X</em><sub>1</sub>)</span>，即交换性；</li><li><spanclass="math inline">∀<em>C</em><sub><em>i</em></sub>, <em>X</em><sub><em>i</em></sub> (<em>i</em>=1∼<em>N</em>)</span>，有<spanclass="math inline">$\sum_{i=1}^{N}\sum_{j=1}^{N}C_iC_jK(X_i,X_j)\ge0$</span>，即半正定性，也就是说，我们选取的核函数，必须要对任意选定的常数<span class="math inline"><em>C</em></span> 和向量 <spanclass="math inline"><em>X</em></span> 都满足该式；</li></ol><h3 id="原问题到对偶问题">2.4 原问题到对偶问题</h3><p>现在我们有了核函数，那么我们要怎样利用核函数，来替代优化式 1 中的<span class="math inline"><em>ϕ</em>(<em>X</em>)</span>呢？在这之前，请先阅读<ahref="#7.3*%20补充：优化理论">优化理论</a>相关的内容。在稍微了解了优化理论中的原问题和对偶问题后，我们要做的，就是<u>把SVM 的优化问题从原问题转换成对偶问题</u>。</p><p>首先，我们<u>把 SVM 的优化问题转换成原问题</u>：</p><p>对于目标函数，<span class="math inline">$\frac12\lVertW\rVert^2+C\sum_{i=1}^N\xi_i$</span> 是一个<strong>凸函数</strong>。</p><p>对于限制条件，<spanclass="math inline"><em>ξ</em><sub><em>i</em></sub> ≥ 0</span>不满足原问题的限制条件形式，得先将大于等于号变成小于等于号，也就是变成<spanclass="math inline"><em>ξ</em><sub><em>i</em></sub> ≤ 0</span>，那么，目标函数就也得变换一下，变成<span class="math inline">$\frac12\lVertW\rVert^2-C\sum_{i=1}^N\xi_i$</span>；同样，另一个限制条件也得变换一下，变成<spanclass="math inline"><em>y</em><sub><em>i</em></sub>[<em>W</em><sup><em>T</em></sup><em>X</em><sub><em>i</em></sub>+<em>b</em>] ≥ 1 + <em>ξ</em><sub><em>i</em></sub>  (<em>i</em>=1∼<em>N</em>)</span>，但是这个不等式也不满足原问题的要求，必须将不等式右边变成0，所以得到 <spanclass="math inline">1 + <em>ξ</em><sub><em>i</em></sub> − <em>y</em><sub><em>i</em></sub>[<em>W</em><sup><em>T</em></sup><em>X</em><sub><em>i</em></sub>+<em>b</em>] ≤ 0  (<em>i</em>=1∼<em>N</em>)</span>。于是得到优化目标的原问题形式，新的优化目标：</p><ol type="1"><li>目标：最小化 <span class="math inline">$\frac12\lVertW\rVert^2-C\sum_{i=1}^N\xi_i$</span></li><li>限制条件：<ol type="1"><li><spanclass="math inline">1 + <em>ξ</em><sub><em>i</em></sub> − <em>y</em><sub><em>i</em></sub>[<em>W</em><sup><em>T</em></sup><em>ϕ</em>(<em>X</em><sub><em>i</em></sub>)+<em>b</em>] ≤ 0  (<em>i</em>=1∼<em>N</em>)</span></li><li><spanclass="math inline"><em>ξ</em><sub><em>i</em></sub> ≤ 0</span></li></ol></li></ol><p><u>将其转换为对偶问题</u>：</p><ol type="1"><li><p>目标：最大化 <spanclass="math inline">$\theta(\alpha,\beta)=\underset{(w,\xi_i,b)}{\inf}\{\frac12\lVertW\rVert^2-C\sum_{i=1}^N\xi_i+\sum_{i=1}^{N}\alpha_i(1+\xi_i-y_i[W^T\phi(X_i)+b])+\sum_{i=1}^N\beta_i\xi_i\}$</span></p></li><li><p>限制条件：</p><ol type="1"><li><spanclass="math inline"><em>α</em><sub><em>i</em></sub> ≥ 0  (<em>i</em>=1∼<em>N</em>)</span></li><li><spanclass="math inline"><em>β</em><sub><em>i</em></sub> ≥ 0  (<em>i</em>=1∼<em>N</em>)</span></li></ol></li></ol><p>解释一下这样变换的原因：</p><ol type="1"><li>原问题中的 <span class="math inline"><em>w</em></span>对应了原问题要求解的变量，有三个，分别是 <spanclass="math inline"><em>W</em></span>、<spanclass="math inline"><em>b</em></span> 和 <spanclass="math inline"><em>ξ</em></span>，所以对偶问题中要遍历所有的 <spanclass="math inline"><em>w</em></span>，到这里就变成了遍历所有的 <spanclass="math inline"><em>W</em></span>、<spanclass="math inline"><em>b</em></span> 和 <spanclass="math inline"><em>ξ</em></span>。</li><li>根据对偶问题的定义，<spanclass="math inline">$L(\omega,\alpha,\beta)=f(\omega)+\sum_{i=1}^{K}\alpha_ig_i(\omega)+\sum_{i=1}^M\beta_ih_i(\omega)$</span>，其中，<spanclass="math inline">$f(w)=\frac12\lVertW\rVert^2-C\sum_{i=1}^N\xi_i$</span>，这一点是没有疑问的，关键是下面，千万不要以为这里的<span class="math inline"><em>α</em></span> 和 <spanclass="math inline"><em>β</em></span> 分别对应了上面的 <spanclass="math inline"><em>α</em><sub><em>i</em></sub></span> 和 <spanclass="math inline"><em>β</em><sub><em>i</em></sub></span>，不是这样的，在对偶问题中，<spanclass="math inline"><em>α</em></span>管的是不等式条件，每个不等式条件要与 <spanclass="math inline"><em>α</em></span> 相乘，<spanclass="math inline"><em>β</em></span> 管的是等式条件，每个等式条件要与<span class="math inline"><em>β</em></span> 相乘。但是在这里，SVM原问题中的限制条件都是不等式，所以应该只有 <spanclass="math inline"><em>α</em></span>，没有 <spanclass="math inline"><em>β</em></span>，只是说为了方便表示，这里仍然沿用<span class="math inline"><em>α</em><sub><em>i</em></sub></span> 和<spanclass="math inline"><em>β</em><sub><em>i</em></sub></span>，并且，由于<span class="math inline"><em>α</em></span> 应该大于0，所以到这里就变成了 <spanclass="math inline"><em>α</em><sub><em>i</em></sub> ≥ 0  (<em>i</em>=1∼<em>N</em>)</span>，并且<spanclass="math inline"><em>β</em><sub><em>i</em></sub> ≥ 0  (<em>i</em>=1∼<em>N</em>)</span>。其他部分就是照抄的关系了。</li></ol><p>接下来我们就来求一下 <spanclass="math inline"><em>L</em>(<em>W</em>,<em>ξ</em><sub><em>i</em></sub>,<em>b</em>,<em>α</em>)</span>的最小值：</p><p>令偏导 <span class="math inline">$\frac{\partial L}{\partialW}=0$</span>，<span class="math inline">$\frac{\partial L}{\partial\xi_i}=0$</span>，<span class="math inline">$\frac{\partial L}{\partialb}=0$</span>： <span class="math display">$$\begin{aligned}&amp;\frac{\partial L}{\partial W}=0\RightarrowW=\sum_{i=1}^N\alpha_iy_i\phi(X_i)&amp;①\\&amp;\frac{\partial L}{\partial \xi_i}=0\RightarrowC=\beta_i+\alpha_i&amp;②\\&amp;\frac{\partial L}{\partial b}=0\Rightarrow\sum_{i=1}^N\alpha_iy_i=0&amp;③\end{aligned}$$</span> 接下来，我们要将上面得到的三个式子代回到 <spanclass="math inline"><em>L</em>(<em>W</em>,<em>ξ</em><sub><em>i</em></sub>,<em>b</em>,<em>α</em>)</span>中去，好消息是，将式 1 和式 3代入之后，式子中的大部分项就能被消掉了，得到 <spanclass="math inline">$\theta(\alpha,\beta)=\frac12\lVertW\rVert^2+\sum_{i=1}^N\alpha_i-\sum_{i=1}^N\alpha_iy_iW^T\phi(X_i)$</span>，先来计算<span class="math inline">$\frac12\lVert W\rVert^2$</span>： <spanclass="math display">$$\begin{aligned}\frac12\lVert W\rVert^2&amp;=\frac12W^TW\\&amp;=\frac12(\sum_{i=1}^N\alpha_iy_i\phi(X_i))^T(\sum_{j=1}^N\alpha_jy_j\phi(X_j))\\&amp;=\frac12\sum_{i=1}^N\sum_{j=1}^N\alpha_i\alpha_jy_iy_j\phi(X_i)^T\phi(X_j)\\&amp;=\frac12\sum_{i=1}^N\sum_{j=1}^N\alpha_i\alpha_jy_iy_jK(X_i,X_j)\end{aligned}$$</span> 一件惊喜的事情：上式的最终化简结果里出现了核函数！接下来化简<spanclass="math inline">$-\sum_{i=1}^N\alpha_iy_iW^T\phi(X_i)$</span>：<span class="math display">$$\begin{aligned}-\sum_{i=1}^N\alpha_iy_iW^T\phi(X_i)&amp;=-\sum_{i=1}^N\alpha_iy_i(\sum_{j=1}^N\alpha_jy_j\phi(X_j))^T\phi(X_i)\\&amp;=-\sum_{i=1}^N\sum_{j=1}^N\alpha_i\alpha_jy_iy_j\phi(X_j)^T\phi(X_i)\\&amp;=-\sum_{i=1}^N\sum_{j=1}^N\alpha_i\alpha_jy_iy_jK(X_i,X_j)\end{aligned}$$</span> 所以，最后会得到： <span class="math display">$$\theta(\alpha,\beta)=\sum_{i=1}^{N}\alpha_i-\frac12\sum_{i=1}^N\sum_{j=1}^N\alpha_i\alpha_jy_iy_jK(X_i,X_j)$$</span> 经过这样一系列的推导，最终问题的形式会变成：</p><ol type="1"><li><p>目标：最大化 <spanclass="math inline">$\theta(\alpha)=\sum_{i=1}^{N}\alpha_i-\frac12\sum_{i=1}^N\sum_{j=1}^N\alpha_i\alpha_jy_iy_jK(X_i,X_j)$</span></p></li><li><p>限制条件：</p><ol type="1"><li><spanclass="math inline">0 ≤ <em>α</em><sub><em>i</em></sub> ≤ <em>C</em>  (<em>i</em>=1∼<em>N</em>)</span></li><li><span class="math inline">$\sum_{i=1}^N\alpha_iy_i=0$</span></li></ol></li></ol><p>解释一下限制条件：根据之前求的偏导我们得到了 <spanclass="math inline"><em>β</em><sub><em>i</em></sub> + <em>α</em><sub><em>i</em></sub> = <em>C</em></span>，由于之前的限制条件规定了<span class="math inline"><em>β</em><sub><em>i</em></sub> ≥ 0</span>以及 <spanclass="math inline"><em>α</em><sub><em>i</em></sub> ≥ 0</span>，所以我们可以直接把这两个条件合并成一个条件，即<spanclass="math inline">0 ≤ <em>α</em><sub><em>i</em></sub> ≤ <em>C</em>  (<em>i</em>=1∼<em>N</em>)</span>，那么为什么要合并呢？因为我们现在的目标函数中只剩下了<span class="math inline"><em>α</em><sub><em>i</em></sub></span> 和<spanclass="math inline"><em>α</em><sub><em>j</em></sub></span>，已经不存在<span class="math inline"><em>β</em></span>了；而第二个限制条件则是直接照抄的令 <spanclass="math inline">$\frac{\partial L}{\partial b}=0$</span>得到的结果。</p><p>在这个对偶问题中，目标函数仍然是一个<strong>凸函数</strong>。并且，其中未知的参数只有<span class="math inline"><em>α</em><sub><em>i</em></sub></span> 和<spanclass="math inline"><em>α</em><sub><em>j</em></sub></span>，核函数是已经确定的了。由于是一个凸优化问题，所以它应该是很容易求解的。有一种凸优化问题求解算法叫做<strong>SMO算法</strong>，在这里不再展开叙述，感兴趣的同学自行了解。总之，我们只需要知道，这个问题是有解的。</p><p>但是到这里还没结束，我们现在已经将 SVM的优化问题从原问题转换成了对偶问题，将 <spanclass="math inline"><em>ϕ</em>(<em>X</em><sub><em>i</em></sub>)</span>用核函数进行了替换，但是还有一个问题：<u>对偶问题是求解 <spanclass="math inline"><em>α</em><sub><em>i</em></sub></span> 和 <spanclass="math inline"><em>α</em><sub><em>j</em></sub></span>，而我们要的是<span class="math inline"><em>W</em></span> 和 <spanclass="math inline"><em>b</em></span>，如何在这两者之间进行转换？</u></p><p>这里又体现了 SVM 的精妙之处，我们并不需要知道 <spanclass="math inline"><em>W</em></span>具体长什么样，根据我们之前求偏导的结果，我们知道 <spanclass="math inline">$W=\sum_{i=1}^N\alpha_iy_i\phi(X_i)$</span>，同时我们也知道，最后分类的方法是，对于测试样本<span class="math inline"><em>X</em></span>，若：</p><ol type="1"><li><spanclass="math inline"><em>W</em><sup><em>T</em></sup><em>ϕ</em>(<em>X</em>) + <em>b</em> ≥ 0</span>，则<span class="math inline"><em>y</em> =  + 1</span></li><li><spanclass="math inline"><em>W</em><sup><em>T</em></sup><em>ϕ</em>(<em>X</em>) + <em>b</em> &lt; 0</span>，则<span class="math inline"><em>y</em> =  − 1</span></li></ol><p>我们将 <spanclass="math inline">$W=\sum_{i=1}^N\alpha_iy_i\phi(X_i)$</span>代入到不等式左边的式子中，就会得到： <span class="math display">$$\begin{aligned}W^T\phi(X)+b&amp;=\sum_{i=1}^N[\alpha_iy_i\phi(X_i)]^T\phi(X)+b\\&amp;=\sum_{i=1}^N\alpha_iy_i\phi(X_i)^T\phi(X)+b\\&amp;=\sum_{i=1}^N\alpha_iy_iK(X_i,X)+b\end{aligned}$$</span> 所以说，我们并不需要知道 <spanclass="math inline"><em>W</em></span>的具体值，我们只需要有核函数，就能对样本进行分类。现在的关键问题是：<u><spanclass="math inline"><em>b</em></span> 是多少</u>？<spanclass="math inline"><em>b</em></span>的求解并不简单，需要用到优化理论中的 <strong>KKT 条件</strong>。</p><p>根据 KKT 条件，当原问题和对偶问题满足强对偶定理时，<spanclass="math inline">∀<em>i</em> = 1 ∼ <em>K</em></span>，要么 <spanclass="math inline"><em>β</em><sub><em>i</em></sub><sup>*</sup> = 0</span>，要么<spanclass="math inline"><em>h</em><sub><em>i</em></sub>(<em>ω</em><sup>*</sup>) = 0</span>；要么<spanclass="math inline"><em>α</em><sub><em>i</em></sub><sup>*</sup> = 0</span>，要么<spanclass="math inline"><em>g</em><sub><em>i</em></sub>(<em>ω</em><sup>*</sup>) = 0</span>，而在这个问题中，<spanclass="math inline"><em>g</em>(<em>W</em>) = 1 + <em>ξ</em><sub><em>i</em></sub> − <em>y</em><sub><em>i</em></sub>[<em>W</em><sup><em>T</em></sup><em>ϕ</em>(<em>X</em><sub><em>i</em></sub>)+<em>b</em>]</span>，所以，要么<spanclass="math inline"><em>α</em><sub><em>i</em></sub> = 0</span>，要么<spanclass="math inline"><em>g</em>(<em>W</em>) = 1 + <em>ξ</em><sub><em>i</em></sub> − <em>y</em><sub><em>i</em></sub>[<em>W</em><sup><em>T</em></sup><em>ϕ</em>(<em>X</em><sub><em>i</em></sub>)+<em>b</em>] = 0</span>.现在，我们取一个 <spanclass="math inline"><em>α</em><sub><em>i</em></sub></span>，使之 <spanclass="math inline">0 &lt; <em>α</em><sub><em>i</em></sub> &lt; <em>C</em></span>（这是肯定能满足的，原因见限制条件），则根据KKT 条件，肯定有 <spanclass="math inline">1 + <em>ξ</em><sub><em>i</em></sub> − <em>y</em><sub><em>i</em></sub>[<em>W</em><sup><em>T</em></sup><em>ϕ</em>(<em>X</em><sub><em>i</em></sub>)+<em>b</em>] = 0</span>。又因为<spanclass="math inline"><em>β</em><sub><em>i</em></sub> + <em>α</em><sub><em>i</em></sub> = <em>C</em></span>，根据KKT 条件，所以 <spanclass="math inline"><em>β</em><sub><em>i</em></sub> ≠ 0</span>，<spanclass="math inline"><em>h</em>(<em>W</em>) = <em>ξ</em><sub><em>i</em></sub> = 0</span>.将 <span class="math inline"><em>ξ</em><sub><em>i</em></sub> = 0</span>代入前式，就有： <span class="math display">$$\begin{aligned}&amp;1-y_i[W^T\phi(X_i)+b]=0\\&amp;\Downarrow\text{to rearrange the terms}\\&amp;b=\frac{1-y_iW^T\phi(X_i)}{y_i}\\&amp;\Downarrow\text{to substitute}W^T\phi(X)=\sum_{i=1}^N\alpha_iy_iK(X_i,X)\text{ into it}\\&amp;b=\frac{1-y_i\sum_{i=1}^N\alpha_iy_iK(X_i,X)}{y_i}\end{aligned}$$</span> 于是，就连 <span class="math inline"><em>b</em></span>我们也知道了。在现实中，我们一般会遍历所有 <spanclass="math inline"><em>α</em><sub><em>i</em></sub> ∉ {0, <em>C</em>}</span>（在上面的讨论中我们只取了一个<span class="math inline"><em>α</em></span>），然后计算 <spanclass="math inline"><em>b</em></span>，最后取 <spanclass="math inline"><em>b</em></span>的平均值，这样能使结果更加精确。</p><h3 id="算法流程总结">2.5 算法流程总结</h3><h4 id="训练流程">训练流程</h4><ol type="1"><li>输入：<spanclass="math inline">{(<em>X</em><sub><em>i</em></sub>,<em>y</em><sub><em>i</em></sub>)}  <em>i</em> = 1 ∼ <em>N</em></span></li><li>求解优化问题（SMO 算法）：<ol type="1"><li>最大化 <spanclass="math inline">$\theta(\alpha)=\sum_{i=1}^{N}\alpha_i-\frac12\sum_{i=1}^N\sum_{j=1}^N\alpha_i\alpha_jy_iy_jK(X_i,X_j)$</span></li><li>限制条件：<ol type="1"><li><spanclass="math inline">0 ≤ <em>α</em><sub><em>i</em></sub> ≤ <em>C</em>  (<em>i</em>=1∼<em>N</em>)</span></li><li><span class="math inline">$\sum_{i=1}^N\alpha_iy_i=0$</span></li></ol></li></ol></li><li>通过上一步计算出来的 <spanclass="math inline"><em>α</em><sub><em>i</em></sub></span> 来计算 <spanclass="math inline"><em>b</em></span>：<spanclass="math inline">$b=\frac{1-y_i\sum_{i=1}^N\alpha_iy_iK(X_i,X)}{y_i}$</span></li></ol><h4 id="测试流程">测试流程</h4><ol type="1"><li>输入测试样本 <span class="math inline"><em>X</em></span></li><li>分类：<ol type="1"><li>若 <spanclass="math inline">$\sum_{i=1}^N\alpha_iy_iK(X_i,X)+b\ge0$</span>，则<span class="math inline"><em>y</em> =  + 1</span></li><li>若 <spanclass="math inline">$\sum_{i=1}^N\alpha_iy_iK(X_i,X)+b\lt0$</span>，则<span class="math inline"><em>y</em> =  − 1</span></li></ol></li></ol><blockquote><p>可以发现，最终训练流程和测试流程中完全不需要用到无限维的 <spanclass="math inline"><em>ϕ</em>(<em>X</em>)</span>，只需要使用核函数就行了。这也就是SVM 用有限维手段来处理无限维问题的方法。</p></blockquote><h2 id="补充优化理论">03* 补充：优化理论</h2><p>在优化领域中，在优化理论中，<strong>原问题（PrimeProblem）</strong>和<strong>对偶问题（DualProblem）</strong>是一对相关的数学问题。</p><p>原问题的定义如下：</p><ol type="1"><li>目标：最小化 <spanclass="math inline"><em>f</em>(<em>ω</em>)</span></li><li>限制条件：<ol type="1"><li><spanclass="math inline"><em>g</em><sub><em>i</em></sub>(<em>ω</em>) ≤ 0  (<em>i</em>=1∼<em>K</em>)</span></li><li><spanclass="math inline"><em>h</em><sub><em>i</em></sub>(<em>ω</em>) = 0  (<em>i</em>=1∼<em>M</em>)</span></li></ol></li></ol><p>原问题是非常普适化的，虽然上面展示的是最小化问题，但只需要在 <spanclass="math inline"><em>f</em>(<em>ω</em>)</span>前加一个负号，立马就变成了最大化问题；同样地，在 <spanclass="math inline"><em>g</em><sub><em>i</em></sub>(<em>ω</em>) ≤ 0</span>中加一个负号，也就变成了 <spanclass="math inline"> − <em>g</em><sub><em>i</em></sub>(<em>ω</em>) ≥ 0</span>；而在式2 的左边减去一个常数 <spanclass="math inline"><em>C</em></span>，就立马变成了 <spanclass="math inline"><em>h</em><sub><em>i</em></sub>(<em>ω</em>) − <em>C</em> = 0</span>，这样就可以把等式右边的0 变成任意常数 <span class="math inline"><em>C</em></span>。</p><p>对偶问题是从原问题派生出来的一个新问题，对偶问题首先定义了一个函数：<span class="math display">$$\begin{aligned}L(\omega,\alpha,\beta)&amp;=f(\omega)+\sum_{i=1}^{K}\alpha_ig_i(\omega)+\sum_{i=1}^M\beta_ih_i(\omega)\quad&amp;①\\&amp;=f(\omega)+\alpha^Tg(\omega)+\beta^Th(\omega)\quad&amp;②\end{aligned}$$</span> 上式中，<span class="math inline"><em>α</em></span> 和 <spanclass="math inline"><em>β</em></span> 是两个和 <spanclass="math inline"><em>ω</em></span>维数一样的向量，并且分别乘上了不等式的限制条件和等式的限制条件。式 ①是该式的代数形式，式 ②是该式的矩阵形式。有了这个函数，我们就可以给出对偶问题的定义了：</p><ol type="1"><li>目标：最大化 <spanclass="math inline">$\theta(\alpha,\beta)=\underset{\omega}{\inf}\{L(\omega,\alpha,\beta)\}$</span></li><li>限制条件：<spanclass="math inline"><em>α</em><sub><em>i</em></sub> ≥ 0  (<em>i</em>=1∼<em>K</em>)</span></li></ol><p>解释一下这里的目标函数，<span class="math inline">inf </span>就是求最小值的意思，下面的 <span class="math inline"><em>ω</em></span>是指，遍历所有每个 <span class="math inline"><em>ω</em></span> 对应的<spanclass="math inline"><em>L</em>(<em>ω</em>,<em>α</em>,<em>β</em>)</span>，所以 <spanclass="math inline">$\underset{\omega}{\inf}\{L(\omega,\alpha,\beta)\}$</span>就是指，求所有 <span class="math inline"><em>ω</em></span> 对应的 <spanclass="math inline"><em>L</em>(<em>ω</em>,<em>α</em>,<em>β</em>)</span>中，<spanclass="math inline"><em>L</em>(<em>ω</em>,<em>α</em>,<em>β</em>)</span>最小的取值。而通过 <spanclass="math inline"><em>θ</em>(<em>α</em>,<em>β</em>)</span>可以看出，<span class="math inline"><em>α</em></span> 和 <spanclass="math inline"><em>β</em></span> 是固定的，也就是说，我们每确定一组<span class="math inline"><em>α</em></span> 和 <spanclass="math inline"><em>β</em></span>，就去求一次 <spanclass="math inline"><em>L</em>(<em>ω</em>,<em>α</em>,<em>β</em>)</span>的最小值，所以 <span class="math inline"><em>θ</em></span> 是只和 <spanclass="math inline"><em>α</em></span> 和 <spanclass="math inline"><em>β</em></span> 有关的函数。而我们的目标又是最大化<spanclass="math inline"><em>θ</em>(<em>α</em>,<em>β</em>)</span>，所以，实质上我们就是要使<spanclass="math inline"><em>L</em>(<em>ω</em>,<em>α</em>,<em>β</em>)</span>的最小值最大化。而对偶问题的限制条件很简单，只要求每个 <spanclass="math inline"><em>α</em><sub><em>i</em></sub></span> 大于 0即可。</p><p>接下来我们就来介绍一下原问题和对偶问题的关系，有一条定理是这样的：</p><blockquote><p>如果 <span class="math inline"><em>ω</em><sup>*</sup></span>是原问题的解，而 <span class="math inline"><em>α</em><sup>*</sup></span>和 <span class="math inline"><em>β</em><sup>*</sup></span>是对偶问题的解，则有 <spanclass="math inline"><em>f</em>(<em>ω</em><sup>*</sup>) ≥ <em>θ</em>(<em>α</em><sup>*</sup>,<em>β</em><sup>*</sup>)</span>。</p></blockquote><p>这条定理的证明如下：</p><p>由于 <span class="math inline"><em>α</em><sup>*</sup></span> 和 <spanclass="math inline"><em>β</em><sup>*</sup></span>是对偶问题的解，则下式肯定成立： <span class="math display">$$\theta(\alpha^*,\beta^*)=\underset{\omega}{\inf}\{L(\omega,\alpha^*,\beta^*\}\leL(\omega^*,\alpha^*,\beta^*)$$</span> 这里的 <span class="math inline"><em>ω</em><sup>*</sup></span>是指一个具体的 <span class="math inline"><em>ω</em></span> 的值。根据<spanclass="math inline"><em>L</em>(<em>ω</em>,<em>α</em>,<em>β</em>)</span>的定义，展开不等式右边的式子有： <span class="math display">$$L(\omega^*,\alpha^*,\beta^*)=f(\omega^*)+\sum_{i=1}^{K}\alpha_i^*g_i(\omega^*)+\sum_{i=1}^M\beta_i^*h_i(\omega^*)$$</span> 既然 <span class="math inline"><em>ω</em><sup>*</sup></span>是原问题的解，那么 <spanclass="math inline"><em>ω</em><sup>*</sup></span>必然满足原问题的两个限制条件，也就是说 <spanclass="math inline"><em>g</em><sub><em>i</em></sub>(<em>ω</em><sup>*</sup>) ≤ 0</span>，<spanclass="math inline"><em>h</em><sub><em>i</em></sub>(<em>ω</em><sup>*</sup>) = 0</span>；另外，既然<span class="math inline"><em>α</em><sup>*</sup></span>是对偶问题的解，那么 <spanclass="math inline"><em>α</em><sup>*</sup></span> 也必然满足 <spanclass="math inline"><em>α</em><sup>*</sup> ≥ 0</span>。进一步，既然<spanclass="math inline"><em>h</em><sub><em>i</em></sub>(<em>ω</em><sup>*</sup>) = 0</span>，那么上式中<spanclass="math inline">$\sum_{i=1}^M\beta_i^*h_i(\omega^*)=0$</span>；既然<spanclass="math inline"><em>g</em><sub><em>i</em></sub>(<em>ω</em><sup>*</sup>) ≤ 0</span>，<spanclass="math inline"><em>α</em><sup>*</sup> ≥ 0</span>，那么上式中 <spanclass="math inline">$\sum_{i=1}^{K}\alpha_i^*g_i(\omega^*)\le0$</span>，所以存在：<span class="math display">$$\theta(\alpha^*,\beta^*)=\underset{\omega}{\inf}\{L(\omega,\alpha^*,\beta^*\}\leL(\omega^*,\alpha^*,\beta^*)\le f(\omega^*)$$</span> 证毕。</p><p>遂定义： <spanclass="math display"><em>G</em> = <em>f</em>(<em>ω</em><sup>*</sup>) − <em>θ</em>(<em>α</em><sup>*</sup>,<em>β</em><sup>*</sup>) ≥ 0</span><span class="math inline"><em>G</em></span>叫做原问题与对偶问题的<strong>间距（DualityGap）</strong>。对应某些特定的优化问题，可以证明 <spanclass="math inline"><em>G</em> = 0</span>.这里不再证明，直接给出问题的结论——<strong>强对偶定理</strong>：</p><blockquote><p>若 <span class="math inline"><em>f</em>(<em>ω</em>)</span>为凸函数，且 <spanclass="math inline"><em>g</em>(<em>ω</em>) = <em>A</em><em>ω</em> + <em>b</em></span>（线性函数），<spanclass="math inline"><em>h</em>(<em>ω</em>) = <em>C</em><em>W</em> + <em>d</em></span>（一组线性函数），则此优化问题的原问题和对偶问题的间距是0，即 <spanclass="math inline"><em>f</em>(<em>ω</em><sup>*</sup>) = <em>θ</em>(<em>α</em><sup>*</sup>,<em>β</em><sup>*</sup>)</span>。</p></blockquote><p>问题是，强对偶定理的前提成立意味着什么？假设现在原问题和对偶问题满足强对偶定理，即<spanclass="math inline"><em>f</em>(<em>ω</em><sup>*</sup>) = <em>θ</em>(<em>α</em><sup>*</sup>,<em>β</em><sup>*</sup>)</span>成立，那么就有 <spanclass="math inline">$f(\omega^*)=\theta(\alpha^*,\beta^*)=\underset{\omega}{\inf}\{L(\omega,\alpha^*,\beta^*\}$</span>，也就是说，<u>原问题的解<spanclass="math inline"><em>ω</em><sup>*</sup></span>，刚刚就是对偶问题在<span class="math inline"><em>α</em><sup>*</sup></span> 和 <spanclass="math inline"><em>β</em><sup>*</sup></span>确定时，取到最小值的那个点</u>。</p><p>更加精妙的是，当 <span class="math inline"><em>G</em> = 0</span>成立时，<spanclass="math inline">$\sum_{i=1}^{K}\alpha_i^*g_i(\omega^*)+\sum_{i=1}^M\beta_i^*h_i(\omega^*)=0$</span>，其中，<spanclass="math inline">$\sum_{i=1}^M\beta_i^*h_i(\omega^*)$</span> 等于 0不用再说了，但 <spanclass="math inline">$\sum_{i=1}^{K}\alpha_i^*g_i(\omega^*)=0$</span>意味着，<u><spanclass="math inline">∀<em>i</em> = 1 ∼ <em>K</em></span>，要么 <spanclass="math inline"><em>α</em><sub><em>i</em></sub><sup>*</sup> = 0</span>，要么<spanclass="math inline"><em>g</em><sub><em>i</em></sub>(<em>ω</em><sup>*</sup>) = 0</span></u>。这个条件叫做<strong>KKT 条件</strong>。</p>]]></content>
      
      
      <categories>
          
          <category> machine learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> lecture notes </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>A-star搜索</title>
      <link href="/2023/12/06/A-star%E6%90%9C%E7%B4%A2/"/>
      <url>/2023/12/06/A-star%E6%90%9C%E7%B4%A2/</url>
      
        <content type="html"><![CDATA[<h2 id="启发式搜索">01 启发式搜索</h2><p>能有助于简化搜索过程的信息称为启发信息，利用启发信息的搜索过程称为启发式搜索。</p><p>求解问题中能利用的大多是非完备的启发信息，所谓非完备就是指，信息也许对搜索有正面影响的，但是我们无法得知它是否总能提供正面影响，不知道它是否会造成负面影响。就例如极值点导数为0，这是一条完备的信息，因为它可被证明总是成立。造成这种结果的原因如下：</p><ol type="1"><li>求解问题系统不可能知道与实际问题有关的全部信息，因而无法知道该问题的全部状态空间，也不可能用一套算法来求解所以问题；</li><li>有些问题在理论上虽然存在着求解算法，但是在工程实践中，这些算法不是效率太低，就是根本无法实现(就比如宽度优先搜索，它总能找到最优解，但是无法实现)。</li></ol><p>启发式搜索在搜索过程中根据启发信息评估各个节点的重要性，优先搜索重要的节点。<strong>估价函数</strong>的任务就是估计待搜索节点“有希望”的程度。估价函数<span class="math inline"><em>f</em><sub><em>n</em></sub></span>表示从初始节点经过 <span class="math inline"><em>n</em></span>节点到达目的节点的路径的最小代价估计值，其一般形式为： <spanclass="math display"><em>f</em>(<em>n</em>) = <em>g</em>(<em>n</em>) + <em>h</em>(<em>n</em>)</span><span id="more"></span></p><p>其中，<span class="math inline"><em>g</em>(<em>n</em>)</span>是从初始节点到结点 <span class="math inline"><em>n</em></span>的<strong>实际代价</strong>，<spanclass="math inline"><em>h</em>(<em>n</em>)</span> 是从节点 <spanclass="math inline"><em>n</em></span>到目的节点的最佳路径的<strong>估计代价</strong>。一般地，在 <u><spanclass="math inline"><em>f</em>(<em>n</em>)</span> 中，<spanclass="math inline"><em>g</em>(<em>n</em>)</span>的比重越大，越倾向于宽度优先搜索方式，而 <spanclass="math inline"><em>h</em>(<em>n</em>)</span>的比重越大，表示启发性能更强</u>。如果 <spanclass="math inline"><em>h</em>(<em>n</em>)</span> 的比重降为0，则搜索过程将变为盲目搜索，因为不再考虑启发信息。</p><p>估价函数的设计方法有很多种，并且不同的估价函数对问题有不同的影响。以八数码问题为例，最简单的估价函数可以取一格局与目的格局相比，其位置不同的数码数目；这种估价函数是最简单实现的，但是效果未必好，一种比较好的估价函数的设计是取各数码移到目的位置所需移动的距离的总和，这是最理想的，但是不可能实现；还可以将每一对逆转数码<ahref="#fn1" class="footnote-ref" id="fnref1"role="doc-noteref"><sup>1</sup></a>乘以一个倍数3；但是这种做法的局限性太大，所以还可以在此基础上再加上位置不符的数码的个数。</p><h2 id="a-搜索算法">02 A 搜索算法</h2><p>启发式图搜索法的基本特点：寻找并设计一个与问题有关的 <spanclass="math inline"><em>h</em>(<em>n</em>)</span> 以构造 <spanclass="math inline"><em>f</em>(<em>n</em>) = <em>g</em>(<em>n</em>) + <em>h</em>(<em>n</em>)</span>，然后以<span class="math inline"><em>f</em>(<em>n</em>)</span>的大小来排列待扩展状态的次序，每次选择 <spanclass="math inline"><em>f</em>(<em>n</em>)</span>值<strong>最小者</strong>进行扩展。这也就是 A 搜索算法的执行流程。</p><p>利用 A 搜索算法求解八数码问题，估价函数定义为： <spanclass="math display"><em>f</em>(<em>n</em>) = <em>d</em>(<em>n</em>) + <em>w</em>(<em>n</em>)</span>其中，<span class="math inline"><em>d</em>(<em>n</em>)</span>代表状态的深度，每步为单位代价；<spanclass="math inline"><em>w</em>(<em>n</em>)</span>以与目标格局不符的数码数量作为启发信息的度量。例如：</p><p><img src="/img/A-star搜索-01.png" /></p><p>初始格局处于第 0 层，因此 <spanclass="math inline"><em>d</em>(<em>S</em>) = 0</span>，其中，<spanclass="math inline">*</span> 代表空格，计算 <spanclass="math inline"><em>w</em>(<em>n</em>)</span> 的时候，既可以算上<spanclass="math inline">*</span>，也可以不算，反正不影响节点扩展，如果不算入，那么<span class="math inline"><em>w</em>(<em>S</em>) = 4</span>(算入的话结果为 5)。由初始格局可得到 3种状态，即分别把空格往上、左、右移动，上图中只展示了 3 种情况。其中，A格局的 <spanclass="math inline"><em>d</em>(<em>A</em>) = 1</span>，<spanclass="math inline"><em>w</em>(<em>A</em>) = 5</span>，所以 <spanclass="math inline"><em>f</em>(<em>A</em>) = 6</span>；B 格局的 <spanclass="math inline"><em>d</em>(<em>B</em>) = 1</span>，<spanclass="math inline"><em>w</em>(<em>B</em>) = 3</span>，所以 <spanclass="math inline"><em>f</em>(<em>B</em>) = 4</span>；C 格局的 <spanclass="math inline"><em>d</em>(<em>C</em>) = 1</span>，<spanclass="math inline"><em>w</em>(<em>C</em>) = 5</span>，所以 <spanclass="math inline"><em>f</em>(<em>C</em>) = 6</span>。根据 A搜索算法的原则，估价函数值最小的是 B格局，因此应该扩展该节点，接下来的过程也是一样。最终经过 5次搜索，也就是扩展 5层节点，最终能到达目标格局，该过程的完整搜索树在课本 P143页，这里就不再展开了。</p><p>那么 A搜索算法能否保证找到最优解？其实仔细想想就知道，我们构造的估价函数中，<spanclass="math inline"><em>h</em>(<em>n</em>)</span>是对待扩展节点到目标节点的代价估计，不一定准确，所以 A搜索算法不一定能保证找到最优解。</p><blockquote><p>如果有代价一样的结点怎么办：可以随机。</p></blockquote><h2 id="a-搜索算法-1">03 A* 搜索算法</h2><p>A* 搜索算法是对 A 搜索算法的改进。我们说 A搜索算法无法保证找到最优解，而 A*搜索算法则保证了一定能搜索到解，并且一定能搜索到最优解。A* 算法给出了 A算法能得到最优解的条件，我们令 <spanclass="math inline"><em>h</em><sup>*</sup>(<em>n</em>)</span> 为状态<span class="math inline"><em>n</em></span>到目标状态的实际最小代价，<spanclass="math inline"><em>h</em>(<em>n</em>)</span>是我们定义的估计代价，则当 <spanclass="math inline">∀<em>n</em></span>，<spanclass="math inline"><em>h</em>(<em>n</em>) ≤ <em>h</em><sup>*</sup>(<em>n</em>)</span>时，我们就称该搜索算法为 A* 搜索算法。</p><p>这就好比是有人托我们帮他买衣服，这种品牌的衣服的最低价格是 1000元(相当于 <spanclass="math inline"><em>h</em><sup>*</sup></span>)，如果他希望以不高于1000 (相当于 <spanclass="math inline"><em>h</em></span>)的价格买到这件衣服(此时 <spanclass="math inline"><em>h</em> ≤ <em>h</em><sup>*</sup></span>)，那我们就要搜索很多家店(前提是这家店要存在)，这种情况下，虽然搜索的店较多，但是我们必然能找到一家店能满足要求；但是倘若他希望以1500 元以内(相当于 <spanclass="math inline"><em>h</em></span>)的价格买到这件衣服(此时 <spanclass="math inline"><em>h</em> &gt; <em>h</em><sup>*</sup></span>)，那搜索的范围就大大减少了，因为最低价是1000，那么价格高于 1000的店找起来肯定没那么费力气，但缺点就是找到的店铺未必是最便宜的。</p><p>在上面我们讨论的八数码问题中，我们选取一格局与目标格局不符的数码数量作为启发信息的度量<spanclass="math inline"><em>w</em>(<em>n</em>)</span>，而八数码问题中的<span class="math inline"><em>h</em><sup>*</sup>(<em>n</em>)</span>应该是各数码移到目的位置所需移动的距离的总和，显然 <spanclass="math inline"><em>w</em>(<em>n</em>) ≤ <em>h</em><sup>*</sup>(<em>n</em>)</span>，满足了<spanclass="math inline"><em>h</em>(<em>n</em>) ≤ <em>h</em><sup>*</sup>(<em>n</em>)</span>的条件，所以也算一种 A* 搜索算法。</p><section id="footnotes" class="footnotes footnotes-end-of-document"role="doc-endnotes"><hr /><ol><li id="fn1"><p>逆转数码的概念涉及到逆序数。例如 1 4 23，在这个序列中，它并未按照从小到大的顺序排序，2 和 3 比 4小，但是却排在 4 后面，所以该序列的逆序数就是 2.逆序数可以用来判断一个八数码问题是否有解，至于原因就不在此赘述了。<ahref="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li></ol></section>]]></content>
      
      
      <categories>
          
          <category> AI </category>
          
      </categories>
      
      
        <tags>
            
            <tag> lecture notes </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>鲁滨逊归结原理</title>
      <link href="/2023/12/06/%E9%B2%81%E6%BB%A8%E9%80%8A%E5%BD%92%E7%BB%93%E5%8E%9F%E7%90%86/"/>
      <url>/2023/12/06/%E9%B2%81%E6%BB%A8%E9%80%8A%E5%BD%92%E7%BB%93%E5%8E%9F%E7%90%86/</url>
      
        <content type="html"><![CDATA[<h2 id="归结推理">1 归结推理</h2><p>反证法：<spanclass="math inline"><em>P</em> ⇒ <em>Q</em></span>，当且仅当 <spanclass="math inline"><em>P</em> ∧ ¬<em>Q</em> ⇔ <em>F</em></span>，即<span class="math inline"><em>Q</em></span> 为 <spanclass="math inline"><em>P</em></span> 的逻辑结论，当且仅当 <spanclass="math inline"><em>P</em> ∧ ¬<em>Q</em></span> 是不可满足的。</p><p>定理：<span class="math inline"><em>Q</em></span> 为 <spanclass="math inline"><em>P</em><sub>1</sub></span>，<spanclass="math inline"><em>P</em><sub>2</sub></span>，……，<spanclass="math inline"><em>P</em><sub><em>n</em></sub></span>的逻辑结论，当且仅当 <spanclass="math inline">(<em>P</em><sub>1</sub>∧<em>P</em><sub>2</sub>∧…∧<em>P</em><sub><em>n</em></sub>) ∧ ¬<em>Q</em></span>是不可满足的。</p><p>归结推理就是基于上面两条定理，将原命题转换成反命题，然后证明其反命题是不可满足的，即可得证原命题是真命题。归结推理的整体思路是：</p><ol type="1"><li>欲证明 <span class="math inline"><em>P</em> ⇒ <em>Q</em></span></li><li>化为反命题 <spanclass="math inline"><em>P</em> ∧ ¬<em>Q</em></span></li><li>化成子句集</li><li>证明子句集不可满足(鲁滨逊归结原理)</li></ol><span id="more"></span><h2 id="子句集">2 子句集</h2><p>什么是子句？如何将谓词公式化为子句集？</p><p>我们称一个不能再分割的命题为<strong>原子谓词公式</strong>，将原子谓词公式及其否定形式称为<strong>文字</strong>，而<strong>子句</strong>就是任何文字的<u>析取式</u>，任何文字本身也是子句。<strong>空子句</strong>是一个不包含任何文字的子句，它永远为假，不可满足，通常表示为<spanclass="math inline"><em>N</em><em>I</em><em>L</em></span>，虽然听上去没什么用，但它却是归结推理中最重要的子句，之后你会知道为什么。以上就是子句的概念，而子句集就是由子句构成的集合。</p><p>以下面这道题为例讲解如何将一个谓词公式化为子句集： <spanclass="math display">(∀<em>x</em>)((∀<em>y</em>)<em>P</em>(<em>x</em>,<em>y</em>)→¬(∀<em>y</em>)(<em>Q</em>(<em>x</em>,<em>y</em>)→<em>R</em>(<em>x</em>,<em>y</em>)))</span>第一步：消去谓词公式中的 <span class="math inline">→</span> 和 <spanclass="math inline">↔︎</span>，得到： <spanclass="math display">(∀<em>x</em>)(¬(∀<em>y</em>)<em>P</em>(<em>x</em>,<em>y</em>)∨¬(∀<em>y</em>)(¬<em>Q</em>(<em>x</em>,<em>y</em>)∨<em>R</em>(<em>x</em>,<em>y</em>)))</span>第二步：将否定符号 <span class="math inline">¬</span>移到紧靠谓词的位置上： <spanclass="math display">(∀<em>x</em>)((∃<em>y</em>)¬<em>P</em>(<em>x</em>,<em>y</em>)∨(∃<em>y</em>)(<em>Q</em>(<em>x</em>,<em>y</em>)∧¬<em>R</em>(<em>x</em>,<em>y</em>)))</span>第三步：变量标准化，将重复的变量名换掉： <spanclass="math display">(∀<em>x</em>)((∃<em>y</em>)¬<em>P</em>(<em>x</em>,<em>y</em>)∨(∃<em>z</em>)(<em>Q</em>(<em>x</em>,<em>z</em>)∧¬<em>R</em>(<em>x</em>,<em>z</em>)))</span>第四步：消去存在量词，要用到 Skolem 函数，令 <spanclass="math inline"><em>y</em> = <em>f</em>(<em>x</em>)</span>，<spanclass="math inline"><em>z</em> = <em>g</em>(<em>x</em>)</span>： <spanclass="math display">(∀<em>x</em>)(¬<em>P</em>(<em>x</em>,<em>f</em>(<em>x</em>))∨(<em>Q</em>(<em>x</em>,<em>g</em>(<em>x</em>))∧¬<em>R</em>(<em>x</em>,<em>g</em>(<em>x</em>))))</span>第五步：化为前束形，即将所有的全称谓词提到公式最前面，使母式中不存在任何量词，上式已满足前束形。</p><p>第六步：化为 Skolem 标准形，即将母式化为合取式： <spanclass="math display">(∀<em>x</em>)((¬<em>P</em>(<em>x</em>,<em>f</em>(<em>x</em>))∨<em>Q</em>(<em>x</em>,<em>g</em>(<em>x</em>))∧(¬<em>P</em>(<em>x</em>,<em>f</em>(<em>x</em>))∨¬<em>R</em>(<em>x</em>,<em>g</em>(<em>x</em>))))</span>第七步：略去全称量词： <spanclass="math display">(¬<em>P</em>(<em>x</em>,<em>f</em>(<em>x</em>)) ∨ <em>Q</em>(<em>x</em>,<em>g</em>(<em>x</em>)) ∧ (¬<em>P</em>(<em>x</em>,<em>f</em>(<em>x</em>))∨¬<em>R</em>(<em>x</em>,<em>g</em>(<em>x</em>)))</span>第八步：把和取词看作分隔符，把整体化为集合： <spanclass="math display">{¬<em>P</em>(<em>x</em>,<em>f</em>(<em>x</em>)) ∨ <em>Q</em>(<em>x</em>,<em>g</em>(<em>x</em>)), ¬<em>P</em>(<em>x</em>,<em>f</em>(<em>x</em>)) ∨ ¬<em>R</em>(<em>x</em>,<em>g</em>(<em>x</em>))}</span></p><p>第九步：子句变量标准化，即将不同的子句中的变量名字区分开，用不同的符号表示：<spanclass="math display">{¬<em>P</em>(<em>x</em>,<em>f</em>(<em>x</em>)) ∨ <em>Q</em>(<em>x</em>,<em>g</em>(<em>x</em>)), ¬<em>P</em>(<em>y</em>,<em>f</em>(<em>y</em>)) ∨ ¬<em>R</em>(<em>y</em>,<em>g</em>(<em>y</em>))}</span>以上，就得到了谓词公式的子句集。</p><h2 id="鲁滨逊归结原理">3 鲁滨逊归结原理</h2><p>子句集中的各子句是合取关系，所以只要有一个不可满足，则整个子句集不可满足。所以，我们需要去找一个空子句，假如子句集中存在空子句，那就肯定不可满足。但是子句集中直接出现空子句的情况是很少的，那么如何找到空子句？这就是鲁滨逊归结原理要解决的问题，根据鲁滨逊归结原理对子句集进行归结，如果最终归结出一个空子句，则说明该子句集不可满足，进一步说明原命题不可满足。</p><p>鲁滨逊归结原理（也称消解原理）的基本思路是：检查子句集 <spanclass="math inline">S</span> 中是否包含空子句，若包含，则 <spanclass="math inline">S</span> 不可满足；若不包含，在 <spanclass="math inline">S</span>中选择合适的子句进行归结，一旦归结出空子句，就说明 <spanclass="math inline">S</span> 是不可满足的。</p><p>归结的定义：设 <spanclass="math inline"><em>C</em><sub>1</sub></span> 和 <spanclass="math inline"><em>C</em><sub>2</sub></span>是子句集中的任意两个子句，如果 <spanclass="math inline"><em>C</em><sub>1</sub></span> 中的文字 <spanclass="math inline"><em>L</em><sub>1</sub></span> 与 <spanclass="math inline"><em>C</em><sub>2</sub></span> 中的文字 <spanclass="math inline"><em>L</em><sub>2</sub></span> 互补，那么从 <spanclass="math inline"><em>C</em><sub>1</sub></span> 和 <spanclass="math inline"><em>C</em><sub>2</sub></span> 中分别消去 <spanclass="math inline"><em>L</em><sub>1</sub></span> 和 <spanclass="math inline"><em>L</em><sub>2</sub></span>，并将两个子句中余下的部分<strong>析取</strong>，构成一个新子句<span class="math inline"><em>C</em><sub>12</sub></span>。</p><hr /><p>例题：设 <spanclass="math inline"><em>C</em><sub>1</sub> = ¬<em>P</em> ∨ <em>Q</em></span>，<spanclass="math inline"><em>C</em><sub>2</sub> = ¬<em>Q</em> ∨ <em>R</em></span>，<spanclass="math inline"><em>C</em><sub>3</sub> = <em>P</em></span>，请对<spanclass="math inline">{<em>C</em><sub>1</sub>, <em>C</em><sub>2</sub>, <em>C</em><sub>3</sub>}</span>进行归结。</p><p><span class="math inline"><em>C</em><sub>1</sub></span> 和 <spanclass="math inline"><em>C</em><sub>2</sub></span> 中存在互补子句 <spanclass="math inline"><em>Q</em></span> 和 <spanclass="math inline">¬<em>Q</em></span>，所以消去这两个子句集，并将余下子句析取，得到<spanclass="math inline"><em>C</em><sub>12</sub> = ¬<em>P</em> ∨ <em>R</em></span>；<spanclass="math inline"><em>C</em><sub>12</sub></span> 和 <spanclass="math inline"><em>C</em><sub>3</sub></span> 中存在互补子句 <spanclass="math inline">¬<em>P</em></span> 和 <spanclass="math inline"><em>P</em></span>，所以消去这两个子句集，并将余下子句析取，得到<spanclass="math inline"><em>C</em><sub>123</sub> = <em>R</em></span>。所以<span class="math inline"><em>C</em><sub>123</sub></span>就是该子句集归结的结果。</p><hr /><p>定理：归结式 <span class="math inline"><em>C</em><sub>12</sub></span>是其亲本子句 <span class="math inline"><em>C</em><sub>1</sub></span> 和<span class="math inline"><em>C</em><sub>2</sub></span>的逻辑结论，即，如果 <spanclass="math inline"><em>C</em><sub>1</sub></span> 和 <spanclass="math inline"><em>C</em><sub>2</sub></span> 为真，则 <spanclass="math inline"><em>C</em><sub>12</sub></span> 也为真。</p><p>上述定理有一条推论：设 <spanclass="math inline"><em>C</em><sub>1</sub></span> 和 <spanclass="math inline"><em>C</em><sub>2</sub></span> 是子句集 <spanclass="math inline">$\text S$</span> 中的两个子句集，<spanclass="math inline"><em>C</em><sub>12</sub></span> 是它们的归结式，若用<span class="math inline"><em>C</em><sub>12</sub></span> 代替 <spanclass="math inline"><em>C</em><sub>1</sub></span> 和 <spanclass="math inline"><em>C</em><sub>2</sub></span> 后得到新子句集 <spanclass="math inline">S<sub>1</sub></span>，则由 <spanclass="math inline">S<sub>1</sub></span> 不可满足性可推出 <spanclass="math inline">S</span> 的不可满足性。但是注意，这条推论不能证明若<span class="math inline">S</span> 是不可满足的，则 <spanclass="math inline">S<sub>1</sub></span>也不可满足，所以还有另一条推论：设 <spanclass="math inline"><em>C</em><sub>1</sub></span> 和 <spanclass="math inline"><em>C</em><sub>2</sub></span> 是子句集 <spanclass="math inline">$\text S$</span> 中的两个子句集，<spanclass="math inline"><em>C</em><sub>12</sub></span> 是它们的归结式，若<span class="math inline"><em>C</em><sub>12</sub></span> 加入原子句集<span class="math inline">S</span>，得到新子句集 <spanclass="math inline">S<sub>2</sub></span>，则 <spanclass="math inline">$\text S$</span> 和 <span class="math inline">$\textS_2$</span> 在不可满足性上是<u>等价</u>的，即若 <spanclass="math inline">S</span> 是不可满足的，则 <spanclass="math inline">S<sub>2</sub></span>也不可满足，反之亦然。不过我们的目的只是为了证明原子句集不可满足，也就是归结出一个空子句，所以上述两个推论均可用。</p><h2 id="归结反演">4 归结反演</h2><p>应用鲁滨逊归结原理证明定理的过程称为<strong>归结反演</strong>。它总共分为以下四个步骤：</p><ol type="1"><li>将已知前提表示为谓词公式 <spanclass="math inline"><em>F</em></span>；</li><li>将待证明的结论表示为谓词公式 <spanclass="math inline"><em>Q</em></span>，并否定得到 <spanclass="math inline">¬<em>Q</em></span>；</li><li>把谓词公式集 <spanclass="math inline">{<em>F</em>, ¬<em>Q</em>}</span> 化为子句集 <spanclass="math inline">$\text S$</span>；</li><li>应用归结原理对子句集 <span class="math inline">$\text S$</span>中的子句进行归结，并把每次归结得到的归结式都并入到 <spanclass="math inline">$\text S$</span>中。如此反复进行，若出现了空子句，则停止归结，此时就证明了 <spanclass="math inline"><em>Q</em></span> 为真。</li></ol><hr /><p>例题：某公司招聘工作人员，A，B，C三人面试，经面试后公司表示如下想法：</p><ul><li>三人中至少录取一人；</li><li>如果录取 A 而不录取 B，则一定录取 C；</li><li>如果录取 B，则一定录取 C。</li></ul><p>求证：公司一定录取 C。</p><p>解：第一步，将已知前提表示为谓词公式，先定义谓词：设 <spanclass="math inline"><em>P</em>(<em>x</em>)</span> 表示录取 <spanclass="math inline"><em>x</em></span>。于是可得如下前提：</p><ul><li><spanclass="math inline"><em>P</em>(<em>A</em>) ∨ <em>P</em>(<em>B</em>) ∨ <em>P</em>(<em>C</em>)</span></li><li><spanclass="math inline"><em>P</em>(<em>A</em>) ∧ ¬<em>P</em>(<em>B</em>) → <em>P</em>(<em>C</em>)</span></li><li><spanclass="math inline"><em>P</em>(<em>B</em>) → <em>P</em>(<em>C</em>)</span></li></ul><p>第二步，将待证明的结论表示为谓词公式，并将其否定：<spanclass="math inline">¬<em>P</em>(<em>C</em>)</span>。</p><p>第三步，将上述谓词公式化为子句集：</p><ol type="1"><li><spanclass="math inline"><em>P</em>(<em>A</em>) ∨ <em>P</em>(<em>B</em>) ∨ <em>P</em>(<em>C</em>)</span></li><li><spanclass="math inline">¬<em>P</em>(<em>A</em>) ∨ <em>P</em>(<em>B</em>) ∨ <em>P</em>(<em>C</em>)</span></li><li><spanclass="math inline">¬<em>P</em>(<em>B</em>) ∨ <em>P</em>(<em>C</em>)</span></li><li><span class="math inline">¬<em>P</em>(<em>C</em>)</span></li></ol><p>第四步，应用归结原理进行归结：</p><ol start="5" type="1"><li><spanclass="math inline"><em>P</em>(<em>B</em>) ∨ <em>P</em>(<em>C</em>)  <em>归</em><em>结</em>(1)<em>和</em>(2)</span></li><li><spanclass="math inline"><em>P</em>(<em>C</em>)          <em>归</em><em>结</em>(3)<em>和</em>(5)</span></li><li><spanclass="math inline"><em>N</em><em>I</em><em>L</em>           <em>归</em><em>结</em>(4)<em>和</em>(6)</span></li></ol><p>由于归结出了空子句，所以成功证明了 <spanclass="math inline">¬<em>P</em>(<em>C</em>)</span> 为假，因此原命题<span class="math inline"><em>P</em>(<em>C</em>)</span>为真，公司一定录取 C。</p><hr /><p>例题：已知：</p><ul><li>任何人的兄弟不是女性；</li><li>任何人的姐妹必是女性；</li><li>Mary 是 Bill 的姐妹。</li></ul><p>求证：Mary 不是 Tom 的兄弟。</p><p>解：第一步，将已知前提表示为谓词公式，先定义谓词：设 <spanclass="math inline"><em>b</em><em>r</em><em>o</em><em>t</em><em>h</em><em>e</em><em>r</em>(<em>x</em>,<em>y</em>)</span>表示录取 <span class="math inline"><em>x</em></span> 是 <spanclass="math inline"><em>y</em></span> 的兄弟，设 <spanclass="math inline"><em>s</em><em>i</em><em>s</em><em>t</em><em>e</em><em>r</em>(<em>x</em>,<em>y</em>)</span>表示录取 <span class="math inline"><em>x</em></span> 是 <spanclass="math inline"><em>y</em></span> 的姐妹，设 <spanclass="math inline"><em>w</em><em>o</em><em>m</em><em>a</em><em>n</em>(<em>x</em>)</span>表示录取 <span class="math inline"><em>x</em></span>是女性。于是可得如下前提：</p><ul><li><spanclass="math inline">(∀<em>x</em>)(∀<em>y</em>)(<em>b</em><em>r</em><em>o</em><em>t</em><em>h</em><em>e</em><em>r</em>(<em>x</em>,<em>y</em>)→¬<em>w</em><em>o</em><em>m</em><em>a</em><em>n</em>(<em>x</em>))</span></li><li><spanclass="math inline">(∀<em>x</em>)(∀<em>y</em>)(<em>s</em><em>i</em><em>s</em><em>t</em><em>e</em><em>r</em>(<em>x</em>,<em>y</em>)→<em>w</em><em>o</em><em>m</em><em>a</em><em>n</em>(<em>x</em>))</span></li><li><spanclass="math inline"><em>s</em><em>i</em><em>s</em><em>t</em><em>e</em><em>r</em>(<em>M</em><em>a</em><em>r</em><em>y</em>,<em>B</em><em>i</em><em>l</em><em>l</em>)</span></li></ul><p>第二步，将待证明的结论表示为谓词公式，并将其否定：<spanclass="math inline"><em>b</em><em>r</em><em>o</em><em>t</em><em>h</em><em>e</em><em>r</em>(<em>M</em><em>a</em><em>r</em><em>y</em>,<em>T</em><em>o</em><em>m</em>)</span>。</p><p>第三步，将上述谓词公式化为子句集：</p><ol type="1"><li><spanclass="math inline"><em>C</em><sub>1</sub> = ¬<em>b</em><em>r</em><em>o</em><em>t</em><em>h</em><em>e</em><em>r</em>(<em>x</em>,<em>y</em>) ∨ ¬<em>w</em><em>o</em><em>m</em><em>a</em><em>n</em>(<em>x</em>)</span></li><li><spanclass="math inline"><em>C</em><sub>2</sub> = ¬<em>s</em><em>i</em><em>s</em><em>t</em><em>e</em><em>r</em>(<em>x</em>,<em>y</em>) ∨ <em>w</em><em>o</em><em>m</em><em>a</em><em>n</em>(<em>x</em>)</span></li><li><spanclass="math inline"><em>C</em><sub>3</sub> = <em>s</em><em>i</em><em>s</em><em>t</em><em>e</em><em>r</em>(<em>M</em><em>a</em><em>r</em><em>y</em>,<em>B</em><em>i</em><em>l</em><em>l</em>)</span></li><li><spanclass="math inline"><em>C</em><sub>4</sub> = <em>b</em><em>r</em><em>o</em><em>t</em><em>h</em><em>e</em><em>r</em>(<em>M</em><em>a</em><em>r</em><em>y</em>,<em>T</em><em>o</em><em>m</em>)</span></li></ol><p>第四步，应用归结原理进行归结：</p><ol type="1"><li><spanclass="math inline"><em>C</em><sub>23</sub> = <em>w</em><em>o</em><em>m</em><em>a</em><em>n</em>(<em>M</em><em>a</em><em>r</em><em>y</em>)</span></li><li><spanclass="math inline"><em>C</em><sub>123</sub> = ¬<em>b</em><em>o</em><em>r</em><em>t</em><em>h</em><em>e</em><em>r</em>(<em>M</em><em>a</em><em>r</em><em>y</em>,<em>y</em>)</span></li><li><spanclass="math inline"><em>C</em><sub>1234</sub> = <em>N</em><em>I</em><em>L</em></span></li></ol><p>由于归结出了空子句，所以成功证明了 <spanclass="math inline"><em>b</em><em>r</em><em>o</em><em>t</em><em>h</em><em>e</em><em>r</em>(<em>M</em><em>a</em><em>r</em><em>y</em>,<em>T</em><em>o</em><em>m</em>)</span>为假，因此原命题 <spanclass="math inline">¬<em>b</em><em>r</em><em>o</em><em>t</em><em>h</em><em>e</em><em>r</em>(<em>M</em><em>a</em><em>r</em><em>y</em>,<em>T</em><em>o</em><em>m</em>)</span>为真，Mary 不是 Tom 的兄弟。</p>]]></content>
      
      
      <categories>
          
          <category> AI </category>
          
      </categories>
      
      
        <tags>
            
            <tag> lecture notes </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>初识Transformer</title>
      <link href="/2023/10/24/%E5%88%9D%E8%AF%86Transformer/"/>
      <url>/2023/10/24/%E5%88%9D%E8%AF%86Transformer/</url>
      
        <content type="html"><![CDATA[<h2 id="seq2seq">01 Seq2seq</h2><p>Transformer 是一个 sequence-to-sequence model，我们一般简写作seq2seq。seq2seq 的意思是指，输入是一个 sequence，输出也是一个sequence，并且两个 sequence 的长度不一定，这里的长度不一定的意思是指：1.输入的长度不一定；2. 输出的长度由机器自己决定；3.输入和输出的长度并不存在必然关系。</p><p>比较典型的 seq2seq 的例子有语音识别、文字翻译、语音翻译等，还有例如text-to-speech (文字转语音) 的 model 也是一种 seq2seq，甚至是聊天机器人chatbot，也是 seq2seq。</p><p>其实，seq2seq 在 NLP 方面的应用是很广泛的，很多你认为可能跟 seq2seq无关的任务也可以转换成 seq2seq。NLP 的本质其实是 question answering(QA)，而只要能想象成 QA，就基本上都能用 seq2seq 解决。</p><p>Seq2seq 还可以用于解决 multi-label classification，multi-label是将一个东西分到多个类别里的任务，并且一个东西可能属于不止一个类别，而seq2seq的输出长度是机器自己决定的，也就是机器觉得有几个输出就几个输出，也就是机器觉得这个东西属于哪几个类别那就是哪几个类别，所以seq2seq 也可以硬解 multi-label classification。</p><p>除此以外，就连图像识别问题也可以用seq2seq，但是这里就不展开了。至少至此，我们已经知道了 seq2seq是一个强大的model，那么它究竟是怎么做到的，接下来我们就要开始研究了。</p><span id="more"></span><h2 id="mechanism">02 Mechanism</h2><p>Seq2seq 主要由两大部分组成 —— encoder 和 decoder。</p><p><img src="/img/Transformer-01.png" /></p><p>Encoder 负责接收和处理数据，然后将处理过的数据交给 decoder，decoder则根据数据决定输出结果。接下来，我们就来具体来看看 encoder 和 decoder的结构。</p><h3 id="encoder">2.1 Encoder</h3><p>简单来说，encoder要做的事情就是将接收的一排向量，转换成另一排向量，这一个过程可以用很多种方法来完成，例如RNN 和 CNN，而 transformer 采用的是[[Self-attention|self-attention]]，大名鼎鼎的注意力机制也就是从transformer 中诞生的。</p><p>Encoder 内部其实是一个一个 block (块)，每个 block都接收一排向量，输出一排向量，而且每个 block的工作也大致都是相同的，其实都是对 input 做 self-attention，然后将output1 丢进一个全连接网络，得到 output2，这个就是一个 block的输出。所以 encoder 的结构大致如下：</p><p><img src="/img/Transformer-02.png" /></p><p>实际上，transformer 中的 self-attention 是比我们之前讲的self-attention 更复杂的。在 transformer 中，self-attention 的 output还要再加上最开始的 input，才算得到最终的 output，这种架构被称为<strong>residual connection(残差连接)</strong>，这个技术旨在解决深度神经网络训练过程中的梯度消失和梯度爆炸等问题。然后，还要对得到的residule 做 layer normalization，方法是：求 residule 整个序列的均值<span class="math inline"><em>m</em></span> 和标准差 <spanclass="math inline"><em>σ</em></span>，然后做标准化：</p><p><span class="math display">$$x_i^\prime=\frac{x_i-m}{\sigma}$$</span></p><p>这样得到的序列才是全连接网络的输入，但是还没完，全连接网络的输出仍要再进行一次residule connection，即将输入和输出相加，然后同样要对 residule 进行layer normalization，这样才能得到 block 的输出。</p><p>现在我们来看一下 <em>Attention is all you need</em> 这篇论文中所画的encoder 的结构：</p><p><img src="/img/Transformer-03.png" /></p><p>首先，input 进行 embedding 之后，作为输入进行 multi-headattention，考虑到有些时候序列可能是有序的，所以图中还画出了 positionalencoding 的环节，这项技术在 self-attention 的笔记中有提到。Attention之后，得到的输出要先于最初的输入进行相加 (add)，然后进行 layernormalization，这就是上图中淡黄色框的含义。再网上要进行 feedforward，其实就是把上一步得到的结果喂给全连接网络，然后对得到的结果再进行一次Add &amp; Norm。这就是上图的含义。</p><h3 id="decoder">2.2 Decoder</h3><p>Decoder 的架构其实分为两种 —— autoregressive (AT) 和non-autoregressive (NAT)，接下来要将的 autoprogressive是比较常见的架构，我们将以语音辨识为例进行讲解。</p><h4 id="autoprogressive">2.2.1 Autoprogressive</h4><h5 id="decoder-的工作流程">decoder 的工作流程</h5><p>假设我们在处理语音辨识的问题，现在，语音已经通过 encoder转换成了一个序列，decoder要接收这个序列，然后输出对应的文字。我们暂且不提 decoder 是如何接收encoder 的输出的，我们假设 decoder 能接收到 encoder的输出，然后来解释一下 decoder 的工作流程。</p><p>首先，我们得给 decoder 一个特殊的 token，我们称之为 BOS (begin ofsentence)，接下来简称 BEGIN。当 decoder 接收到这个 token时，它就开始输出，它的输出应该是一个 one-hotvector，也就是一个独热编码的向量，其大小等同于词汇的大小，以中文为例，可能就是所有中文字的数量。当然你可能会说这有点太大了，那实际上，我们可能只取常见的几千个中文字，生僻字我们不去管。</p><p>这个 vector 中的数字就是每个对应位置上的中文字的可能性，它们的总和是1，由对 decoder 的输出做 softmax 之后得到，最终 decoder生成汉字就是取其中可能性最大的那个。</p><p>总而言之，我们现在得到了第一个汉字，假设这个汉字是“机”，那么下一步，我们要把这个decoder 生成的汉字加入到 decoder 的输入中，也就是说，现在的 decoder的输入不只有 BEGIN了，还有了“机”，于是重复上面的步骤，得到第二个输出“器”，周而复始……最终得到完整的输出“机器学习”。在这个过程中，decoder当然也有读入 encoder的输出，但是这一部分我们先不讲。总结上面的过程，我们可以说，其实 decoder就是将自己前一刻的输出当作输入，进一步得到下一时刻的输出。这里就诞生了一个问题：要是decoder 自己预测的内容出错了怎么办？会不会造成 errorpropagation，也就是一步错步步错？当然是有可能的，但是这个问题我们之后再谈，我们先暂且当作没这回事。</p><h5 id="decoder-和-encoder-的结构对比">decoder 和 encoder的结构对比</h5><p>现在我们来看看论文中画的 decoder 的结构，decoder看上去很复杂，但如果我们将其与 encoder的结构进行对比，似乎能发现一些相似之处。</p><p><img src="/img/Transformer-04.png" /></p><p>你可能会发现，如果我们把 decoder 中间那一块盖起来，encoder的结构好像就跟 decoder 差不了多少了，无非是 decoder的输出最后还要经过一个线性层，再经过 softmax激活，来输出可能性罢了。唯一的不同可能就是 decoder 中，一开始的attention 是 masked multi-head attention，那这个 masked 是什么意思？</p><h5 id="masked-self-attention">masked self-attention</h5><p>在我们原来所讲的 self-attention 中，输出的 <spanclass="math inline"><em>b</em><sub>1</sub></span> 是考虑了 <spanclass="math inline"><em>a</em><sub>1</sub> ∼ <em>a</em><sub><em>n</em></sub></span>所有的资讯之后得到的 <spanclass="math inline"><em>a</em><sub>1</sub></span> 的资讯，<spanclass="math inline"><em>b</em><sub>2</sub> ∼ <em>b</em><sub><em>n</em></sub></span>皆是如此；而在 masked self-attention中，输出不再能考虑后面的资讯，意思是，<spanclass="math inline"><em>b</em><sub>1</sub></span> 只是考虑了 <spanclass="math inline"><em>a</em><sub>1</sub></span> 后，<spanclass="math inline"><em>a</em><sub>1</sub></span> 的资讯；<spanclass="math inline"><em>b</em><sub>2</sub></span> 是考虑了 <spanclass="math inline"><em>a</em><sub>1</sub>, <em>a</em><sub>2</sub></span>之后，<span class="math inline"><em>a</em><sub>2</sub></span>的资讯；<span class="math inline"><em>b</em><sub>3</sub></span> 是考虑了<spanclass="math inline"><em>a</em><sub>1</sub>, <em>a</em><sub>2</sub>, <em>a</em><sub>3</sub></span>之后，<span class="math inline"><em>a</em><sub>3</sub></span>的资讯……只有 <spanclass="math inline"><em>b</em><sub><em>n</em></sub></span>是考虑了整个输入后，输出的资讯。这就是 masked self-attention。</p><p>至于为什么要用 masked，其实原因很简单。我们之前也解释了，decoder会拿自己的输出作为输入，输出是从左到右依次产生的，输入肯定也只能从左到右依次输入，所以就出现了这样只能读左边，而不能读右边的masked self-attention 机制。</p><h5 id="何时终止输出">何时终止输出</h5><p>之前说过，transformer 的输出长度是机器自己决定的，也就是由 decoder决定的，但到目前为止，我们都还没有讨论过，decoder到底是如何决定输出长度的，它是怎么知道什么时候该停止输出的。就像前面举的例子，在decoder 输出完“机器学习”之后，万一它又把BEGIN+“机器学习”作为输入，输出了个“惯”字怎么办？</p><p>对这一点的处理是很巧妙的，要解决这个问题，我们还得再准备一个特殊的token，叫作 END。就像 BEGIN 是开始的标志，END 就是终止的标志，并且，END存储在 encoder 的输出 one-hot vector 里，也就是说，one-hot vector的长度是 vocabulary 的长度再加上 END。如果根据 encoder的计算，发现输出的 one-hot vector 中，END的概率是最高的，那就意味着输出该结束了。通过这种方式，encoder就能自行决定何时终止输出。</p><h4 id="non-autoregressive">2.2.2 Non-autoregressive</h4><p>接下来我们简短地介绍一下另一种 decoder 的架构 ——non-autoregressive，简称 NAT。</p><h5 id="at-vs-nat">AT vs NAT</h5><p>AT 的运作方式是，先将开始标志 BEGIN作为输入传入，得到一个输出，随后将自己的输出作为输入再传进来，再得到下一个输出，然后再将下一个输出继续传进来，得到下下个输出，循环往复，直到输出END；而 NAT 则不一样，它一开始就接收多个BEGIN，于是得到多个输出，也就是一个完整的句子，然后就结束了。</p><h5 id="何时终止输出-1">何时终止输出</h5><p>那么问题就来了，既然我们一开始都不知道输出的长度，那我们要怎么知道该给decoder 多少个 BEGIN？</p><p>解决这个问题的方法有很多，一种方法就是准备一个 classifier，将 encoder的输出先丢给 classifier，由 classifier决定输出的长度，于是就丢对应长度的 BEGIN。</p><p>另一种可能的处理方法是，不管输出的长度，直接丢给 decoder 尽可能大的BEGIN 数目，然后看看 decoder 的输出中，哪里出现了 END 标志，我们只取 END前面的内容，END 后面的输出就不管了。</p><h5 id="nat-的优势">NAT 的优势</h5><p>NAT 的优势在于，它的运算是平行的，而不像 AT那样，要预测下一个输出，就必须等前一个输出完成，所以 NAT 的速度应该是比AT要快的；另外，它的输出长度是可控的，这就允许我们人为地去控制输出。</p><p>NAT 是一个热门的研究话题，它的性能比 AT 要好，也比 AT的可控性高，但一个严重的问题是，NAT 的准确率是远不及 AT 的。NAT 要想赶上AT 的准确率，往往需要很多的秘诀才能做到，而 AT可能只要随便跑跑就能达到比较高的准确率。所以，如何让 NAT 赶上 AT的准确率，是目前一个大热门。不过这里就不细讲了，毕竟这是一个大坑，有兴趣的话可以自己去了解。</p><h3 id="encoder-decoder">2.3 Encoder-Decoder</h3><p>接下来，我们就要来看看之前说先暂且遮起来的那一块了。</p><p><img src="/img/Transformer-05.png" /></p><h5 id="cross-attention">cross attention</h5><p>图中被框起来的那一部分叫作 cross attention，它是连接 encoder 和decoder 的桥梁。你可以看到，从 encoder 引出了两个箭头连接到了 multi-headattention，除此以外还有一个箭头是从 decoder的上一层引出的，那这个模组到底是怎么运作的？我们继续以语音辨识的例子进行阐述。</p><p>首先，encoder 读进一个向量 <spanclass="math inline"><em>a</em><sup>1</sup>, <em>a</em><sup>2</sup>, <em>a</em><sup>3</sup></span>，并且输出一个等长的向量<spanclass="math inline"><em>b</em><sup>1</sup>, <em>b</em><sup>2</sup>, <em>b</em><sup>3</sup></span>，用同样的方法，得到对应的<spanclass="math inline"><em>k</em><sup>1</sup>, <em>k</em><sup>2</sup>, <em>k</em><sup>3</sup></span>和 <spanclass="math inline"><em>v</em><sup>1</sup>, <em>v</em><sup>2</sup>, <em>v</em><sup>3</sup></span>；同时，decoder读进一个 BEGIN，进行 masked self-attention，由于是attention，所以输出也肯定是一个和 BEGIN等长的向量，接下来，将这个输出乘上一个矩阵，进行 transform，得到 <spanclass="math inline"><em>q</em></span>；然后用 <spanclass="math inline"><em>q</em></span> 与 <spanclass="math inline"><em>k</em><sup>1</sup>, <em>k</em><sup>2</sup>, <em>k</em><sup>3</sup></span>去分别计算，得到 <spanclass="math inline"><em>α</em><sup>1</sup>, <em>α</em><sup>2</sup>, <em>α</em><sup>3</sup></span>，然后再分别乘上<spanclass="math inline"><em>v</em><sup>1</sup>, <em>v</em><sup>2</sup>, <em>v</em><sup>3</sup></span>，将加权的结果加起来，得到<span class="math inline"><em>v</em></span>，这个 <spanclass="math inline"><em>v</em></span>就是接下来的全连接网络的输入。在这个过程中，<spanclass="math inline"><em>α</em>, <em>k</em>, <em>v</em></span> 都来自encoder，而 <span class="math inline"><em>q</em></span> 来自decoder，所以这个过程就叫做 <strong>cross attention</strong>。</p><p><img src="/img/Transformer-06.png" /></p><p>如果说现在 decoder已经产生了一个输出“机”，那么接下来的操作也是一样的，将“机”作为输入传进去，进行masked self-attention，然后得到 <spanclass="math inline"><em>q</em><sup>′</sup></span>，去做相同的运算得到<spanclass="math inline"><em>v</em><sup>′</sup></span>，再丢进全连接网络得到下一个输出，如此往复……</p><h3 id="conclusion">2.4 Conclusion</h3><p>Transformer的工作流程已经讲完了，为了深入理解这些过程，而不是仅仅停留于表面的数学运算，我们还得从实例中剖析这个过程。Ecoder所做的工作其实是对 input 进行提炼，方法就是 self-attention；而 decoder则是将 encoder 提炼后的数据，以及一个开始标志 BEGIN作为输入，开始生成结果，并且每得到一个 output，就将这个结果加入到自己的input 中，做 masked self-attention，以此不断得到新的 output，直到decoder 输出 END 为止。</p><p>下面我们以机器翻译为例，假设我们需要机器翻译“我喜欢你”这句话，那么transfomer 做的第一件事是，将这句话输入到 encoder 当中，通过self-attention 获取语义编码，这里以 <spanclass="math inline"><em>c</em></span> 标识，这里的 <spanclass="math inline"><em>c</em></span> 是一个向量，其中包括了 <spanclass="math inline"><em>c</em><sub>1</sub>, <em>c</em><sub>2</sub>, ...</span>等等，每一个汉字就对应一个 <spanclass="math inline"><em>c</em></span>。</p><p><spanclass="math display"><em>c</em> = Encoder ("我喜欢你")</span></p><p>然后，decoder 将该语义编码与一个 token，也就是上面说的 BEGIN作为输入，得到第一个单词的输出：</p><p><spanclass="math display">I = Decoder (<em>c</em><sub>1</sub>,<em>B</em><em>E</em><em>G</em><em>I</em><em>N</em>)</span></p><p>然后将这个单词作为输入，再进行一次输出：</p><p><spanclass="math display">love  = Decoder (<em>c</em><sub>2</sub>,<em>B</em><em>E</em><em>G</em><em>I</em><em>N</em>,<em>I</em>)</span></p><p>重复上面的过程，直到输出 END：</p><p><span class="math display">$$\begin{aligned}&amp;\text{you} = \operatorname{Decoder}(c_3,BEGIN,I,love)\\\\&amp;\text{END}=\operatorname{Decoder}(c_4,BEGIN,I,love,you)\end{aligned}$$</span></p><p>于是最后得到的翻译结果：<span class="math inline">I loveyou.</span></p><h2 id="training">03 Training</h2><p>现在我们已经把 transformer 的内部运作方式给讲完了，下一步，就是讲讲transformer 的训练。</p><p>我们知道，机器学习的目标就是不断降低 loss。而 seq2seq这件事，似乎就是在做分类，最后的输出是从所有汉字中选择一个最可能的，所以衡量seq2seq 的 loss，可以使用和 classifier 类似的方法。在 transformer中，我们继续以语音辨识为例，decoder输出的每一个汉字都与正确答案之间有一个 crossentropy，模型优化的目标就是，使所有输出与正确答案之间的 cross entropy的总和最小。</p><h3 id="copy-mechanism">3.1 Copy mechanism</h3><p>有些时候，输出的序列会和输入的序列有重合的部分，例如下面这个 chatbot的例子：</p><p><img src="/img/Transformer-07.png" /></p><p>再比如说文献摘要，让机器来给一篇文献写摘要，摘要中肯定会有与正文内容重复的部分。我们当然是希望机器能够从输入的内容中，直接把这些重合的部分提取出来，而不是自己合成，那这就需要借助copy mechanism，让机器能够直接从输入中把部分内容 copy过来。这样的模型当然是存在的，最早的具有复制能力的模型是 copynetwork，后来还有论文 <ahref="https://arxiv.org/abs/1603.06393">Incorporating Copy Mechanism inSequence-to-Sequence Learning</a>也做过这个，如果感兴趣可以自己去了解。</p><h3 id="guided-attention">3.2 Guided attention</h3><p>机器观察输入，对输入做 attention的顺序是不固定的，这是机器自己学习的结果，自己学习就会导致问题。例如在TTS (Text to Speech) 的任务中，有些时候，机器居然会漏字。如果是一般的chatbot 或者 summarization 问题，那么漏一两个字可能也没什么关系，但在TTS 中，漏字是非常严重的问题。这个时候，我们可以用 guided attention来教给机器一个固定的顺序来处理输入。</p><p>例如说，在 TTS 中，读入一段文字之后，机器应该从左到右依次做attention，这样才是正确解决问题的方法。但如果机器颠三倒四，先看后面，再看前面，最后看中间，那显然有些事情就做错了，需要我们人工纠正。</p><p><img src="/img/Transformer-08.png" /></p><p>所以 guided attention 就是要求机器的 attention遵循一个固定的法则，这个法则肯定是我们事先就知道了这个问题的处理方式才得出的，guidedattention 也一般只适用于那些有固定解法的问题。</p><p>一些关于 guided attention 的关键词汇：<u>monotonicattention</u>、<u>location-awareattention</u>。如果感兴趣可以自行搜索。</p><h3 id="beam-search">3.3 Beam search</h3><p>继续以语音辨识为例，假设现在的 vocabulary 中只有 A 和 B两个字，那么我们可以构建出一棵树，每次 decoder 都只面对 A 和 B两种选择。那么根据原则，在每一个节点处，decoder会选择分数最高的那个，一直到叶子节点。这种搜索方法叫作 greedydecoding，因为它每次都挑分数最高的。</p><p>但是有时候，选择当前分数最高的，未必能选出一条总分数最高的path，如下图所示：</p><p><img src="/img/Transformer-09.png" /></p><p>虽然一开始 B 的分数比 A低，但是我们可以看到，之后路径上的节点的分数明显比 A之后路径上节点分数要高，所以总体而言，一开始选分数较低的那个，反而能得到一条更好的path。但是我们要怎么做才能选出这条 path 呢？一种方法是dfs，即全部走一遍，但显然这种方法不显示，毕竟中文里的汉字有几千个，不可能用dfs 来搜索。</p><p>那么还有一种方法就是 beamsearch，它可以找出一条相对好的，但也不是很精准的path。那么这个算法到底有没有用呢？有趣的是，它有时候有效，有时候就没什么作用。那什么时候有用呢？根据研究，当一个问题有一个比较明确的解时，beamsearch就会比较有用；但当一个问题需要发挥机器自己的想象力来完成的时候，beamsearch 就比较没用。如果感兴趣，可以自行了解。</p>]]></content>
      
      
      <categories>
          
          <category> deep learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> lecture notes </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
